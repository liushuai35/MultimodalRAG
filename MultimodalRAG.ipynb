{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------------------\n",
    "# 导入标准库模块\n",
    "# 这是系统稳定运行的基础，我必须确保每一个模块都正确导入和使用。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "import sqlite3 # 导入 SQLite 数据库模块。它是我们存储和管理文档元数据的关键，比如文档的内部ID、原始ID、文本描述和图像路径。\n",
    "import os      # 导入操作系统模块。它提供了与操作系统交互的必要功能，例如处理文件路径、检查文件或目录是否存在、以及创建目录等。这些操作对于管理索引和数据文件至关重要。\n",
    "import numpy as np # 导入 NumPy 库。它提供了高效的数值计算能力，特别是在处理向量（如CLIP模型生成的特征向量）时不可或缺，能极大地提升性能。\n",
    "from typing import List, Dict, Union, Optional, Tuple, Any # 导入类型提示模块。使用类型提示能让代码更清晰、更易于理解和维护，也能帮助静态分析工具发现潜在错误，这是高质量代码的重要保障。(增加了 Any 类型，以适应某些字典中可能包含的更广泛的数据类型)\n",
    "import json    # 导入 JSON 库。用于处理 JSON (JavaScript Object Notation) 格式的数据，常用于配置文件读写、API数据交换等。我们的数据源就是JSON格式，正确处理它非常重要。\n",
    "import time    # 导入时间库。提供时间相关的函数，比如获取当前时间、程序暂停（sleep）等，可以在需要时用于控制程序流程或添加延时。\n",
    "import random  # 导入随机库。用于生成伪随机数，例如在示例查询中随机选择文档，以展示系统的多种能力。\n",
    "import logging # 导入日志模块。这是追踪程序运行状态、诊断问题、记录信息、警告和错误的核心工具。详细的日志是确保系统可维护性的基石。\n",
    "import sys     # 导入系统模块。提供了访问由 Python 解释器使用或维护的变量和函数的接口，此处用于配置日志输出到标准输出，方便实时监控。\n",
    "import datetime # 导入日期时间模块。用于处理日期和时间，如此处用于生成带有时间戳的目录名，确保每次运行的输出结果能够唯一且易于组织。\n",
    "import re      # 导入正则表达式模块。用于进行强大的文本模式匹配和字符串操作，如此处用于清理文件名中的非法字符，确保文件路径的有效性。\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 导入第三方库模块 (这些是实现多模态功能的核心，需要预先安装。我已确认其必要性。)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "import faiss   # 导入 Faiss 库。这是一个由 Facebook AI Research 开发的、用于高效相似度搜索和聚类的向量库。它将是我们的向量检索引擎。\n",
    "               # 安装提示: pip install faiss-cpu (如果您使用CPU) 或 pip install faiss-gpu (如果您有CUDA环境的GPU)。\n",
    "from transformers import CLIPProcessor, CLIPModel # 从 Hugging Face Transformers 库导入 CLIP 模型的处理器和模型本身。\n",
    "                                                 # CLIP (Contrastive Language–Image Pre-training) 是一个强大的多模态模型，能够将文本和图像编码到同一个向量空间，这是我们实现多模态检索的关键技术。\n",
    "                                                 # 安装提示: pip install transformers torch pillow。\n",
    "from PIL import Image, UnidentifiedImageError # 导入 Pillow 库 (PIL 的一个分支)。它是Python中事实上的图像处理标准库，用于图像文件的加载、处理和保存。 (增加了 UnidentifiedImageError 用于捕获特定的图像加载错误)\n",
    "import torch   # 导入 PyTorch 库。这是一个广泛使用的开源机器学习框架，Transformers 库基于它构建。我们使用它来加载和运行CLIP模型，并利用其GPU加速能力（如果可用）。\n",
    "import zhipuai # 导入 ZhipuAI 客户端库。用于与智谱 AI 开发的大语言模型 (LLM) API 进行交互。它将负责根据检索到的信息生成最终答案。\n",
    "               # 安装提示: pip install zhipuai。\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 全局日志记录器设置 (在 `if __name__ == \"__main__\":` 中会进一步精细配置，这里只是初始化)\n",
    "# 这是一个重要的工具，我必须确保它随时可用，以便记录系统的每一个动作和潜在问题。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "logger = logging.getLogger(__name__) # 初始化一个模块级别的日志记录器实例。`__name__` 会被设置成当前模块的名称，便于区分日志来源。\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 工具函数定义\n",
    "# 这些是辅助性的功能，但同样重要，必须确保它们可靠无误。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def setup_logging(log_file_path: str):\n",
    "    \"\"\"\n",
    "    配置全局日志记录器 (logger)。\n",
    "    此函数设定日志记录的最低级别、输出格式，并将日志信息同时发送到控制台和指定的日志文件。\n",
    "    这是确保系统运行可追踪性的重要步骤。\n",
    "\n",
    "    Args:\n",
    "        log_file_path (str): 日志文件的完整路径。程序运行的所有日志信息将被精确地记录到此文件。\n",
    "    \"\"\"\n",
    "    global logger # 声明我们要修改的是全局变量 `logger`。\n",
    "    logger.setLevel(logging.INFO) # 设置日志记录的最低级别为 INFO。这意味着只有 INFO 及以上级别（如 WARNING, ERROR, CRITICAL）的日志才会被处理和输出。\n",
    "\n",
    "    # 在添加新的处理器之前，清理可能已存在的旧处理器。这样做是为了避免在重复调用此函数时（例如在交互式环境或测试中）导致日志被多次记录。这是保证日志行为一致性的重要细节。\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # 创建一个文件处理器 (FileHandler)，用于将日志信息写入到指定的日志文件。\n",
    "    # `encoding='utf-8'` 确保日志文件能正确处理包括中文在内的多语言字符。\n",
    "    # `mode='w'` 表示每次运行程序时会覆盖（overwrite）之前的日志文件，确保日志只记录当次运行的情况。如果需要保留历史日志，可以将模式改为 'a' (追加)。\n",
    "    file_handler = logging.FileHandler(log_file_path, encoding='utf-8', mode='w')\n",
    "    file_handler.setLevel(logging.INFO) # 文件处理器也只处理 INFO 及以上级别的日志。\n",
    "\n",
    "    # 创建一个控制台处理器 (StreamHandler)，用于将日志信息输出到标准输出（通常是终端控制台）。\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO) # 控制台处理器同样只处理 INFO 及以上级别的日志。\n",
    "\n",
    "    # 定义日志格式器 (Formatter)。它决定了每条日志记录的显示格式。一个清晰的格式有助于快速定位问题。\n",
    "    # 格式字符串包含:\n",
    "    #   %(asctime)s: 日志记录的创建时间。\n",
    "    #   %(levelname)s: 日志级别 (例如 INFO, WARNING, ERROR)。\n",
    "    #   [%(filename)s.%(funcName)s:%(lineno)d]: 日志发出的文件名、函数名和行号。这是快速定位代码位置的关键信息。\n",
    "    #   %(message)s: 实际的日志消息内容。\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(filename)s.%(funcName)s:%(lineno)d] - %(message)s')\n",
    "\n",
    "    # 将定义好的格式器应用到文件处理器和控制台处理器。\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # 将配置好的文件处理器和控制台处理器添加到全局日志记录器中。\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.info(\"全局日志记录器配置完成。日志将同时输出到控制台，并写入文件: %s\", log_file_path) # 记录一条日志，明确表明配置已成功。\n",
    "\n",
    "def sanitize_filename(filename: str, max_length: int = 100, is_dir_component: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    清理输入的字符串，使其成为一个有效的文件名或目录名组件。\n",
    "    这个函数会替换或移除文件名中可能导致问题的特殊字符，并将文件名截断到指定的最大长度，以确保跨操作系统的兼容性和文件系统的稳定性。\n",
    "\n",
    "    Args:\n",
    "        filename (str): 需要被清理的原始字符串。\n",
    "        max_length (int): 清理后文件名的最大允许长度。默认为 100 个字符。\n",
    "        is_dir_component (bool): (此参数在此当前的实现中未产生不同行为，保留以备未来扩展) 指示该字符串是否用作目录路径的一部分。\n",
    "                                 为了简单和安全起见，此函数对文件名和目录名组件采用相同的严格清理规则。\n",
    "\n",
    "    Returns:\n",
    "        str: 清理和截断后的、可以用作文件系统名称的字符串。这个结果是可靠且安全的。\n",
    "    \"\"\"\n",
    "    # 如果输入的文件名为空或 None，返回一个默认的占位符名称。这是为了避免生成空名称的文件或目录，增加系统的鲁棒性。\n",
    "    if not filename:\n",
    "        return \"unnamed_component\" # 未命名组件\n",
    "\n",
    "    # 使用正则表达式替换掉文件名中常见的非法字符。\n",
    "    # 这些字符 (\\ / * ? : \" < > |) 在大多数主流文件系统中都是不允许的。\n",
    "    # 将这些非法字符统一替换为下划线 \"_\"。\n",
    "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "    # 将字符串两端的空白字符（空格、制表符、换行符等）去除。\n",
    "    # 然后，将字符串内部的一个或多个连续空白字符替换为单个下划线 \"_\"。这能使文件名更紧凑和规范。\n",
    "    sanitized = re.sub(r'\\s+', '_', sanitized.strip())\n",
    "\n",
    "    # 移除文件名开头可能存在的点和下划线。以点开头的在某些系统上是隐藏文件，以下划线开头的可能导致不规范。\n",
    "    sanitized = re.sub(r'^[\\._]+', '', sanitized)\n",
    "\n",
    "    # 将清理后的字符串截断到 `max_length` 指定的最大长度。\n",
    "    # 注意：简单的切片可能在多字节字符（如某些中文）的中间截断，导致乱码。对于本例主要处理ASCII文件名，这通常不是问题。若需完美处理，需要更复杂的截断逻辑，但当前实现已足够满足大多数常见文件名场景。\n",
    "    sanitized = sanitized[:max_length]\n",
    "\n",
    "    # 再次检查，如果清理和截断后字符串变为空，或者只包含点号 \".\" (可能导致隐藏文件或路径问题，尤其是在Unix-like系统中)，则返回一个特定的占位符名称。这是最后一层安全检查。\n",
    "    if not sanitized or all(c == '.' for c in sanitized):\n",
    "        return \"sanitized_empty_name\" # 清理后为空的名称\n",
    "\n",
    "    # 避免使用 Windows 系统中的保留设备名作为文件名（不区分大小写）。\n",
    "    # 例如: CON, PRN, AUX, NUL, COM1-COM9, LPT1-LPT9。这些名称在某些操作下可能导致问题。\n",
    "    # 如果清理后的名称（转换为大写后）匹配这些保留名，则在其前后添加下划线以作区分。这是一个简化的检查，完整的跨平台文件名验证会更复杂，但这个检查覆盖了常见的高风险情况。\n",
    "    reserved_names_check = sanitized.upper()\n",
    "    if reserved_names_check in [\"CON\", \"PRN\", \"AUX\", \"NUL\"] or \\\n",
    "       re.match(r\"COM[1-9]$\", reserved_names_check) or \\\n",
    "       re.match(r\"LPT[1-9]$\", reserved_names_check):\n",
    "        sanitized = f\"_{sanitized}_\" # 在保留名称前后加下划线\n",
    "\n",
    "    return sanitized # 返回最终清理后的、安全且符合文件系统规范的文件名字符串。\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 数据加载与预处理模块\n",
    "# 这是系统获取原始知识的基础步骤，必须确保数据的准确性和完整性。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def load_data_from_json_and_associate_images(json_path: str, image_dir: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    从指定的 JSON 文件加载文档的元数据 (如 ID 和描述文本)，\n",
    "    并根据文档 ID (JSON中的 'name' 字段) 在指定的图像目录中查找并关联对应的图像文件。\n",
    "    函数假设图像文件名是文档 ID 加上常见的图片扩展名 (如 .png, .jpg)。\n",
    "\n",
    "    Args:\n",
    "        json_path (str): 包含文档元数据的 JSON 文件路径。\n",
    "                         JSON 文件应为一个列表，其中每个对象至少包含 'name' 和 'description' 字段。\n",
    "        image_dir (str): 存放与 JSON 数据对应的图片文件的目录路径。\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: 一个包含处理后文档信息的字典列表。\n",
    "                    每个字典包含以下键：\n",
    "                    - 'id': 文档的唯一标识符 (来自 JSON 'name' 字段，确保为字符串)。\n",
    "                    - 'text': 文档的文本描述 (来自 JSON 'description' 字段，确保为字符串或 None)。\n",
    "                    - 'image_path': 找到的对应图像文件的完整路径 (str)。如果未找到图像或 image_dir 无效，则为 None。\n",
    "                    如果 JSON 文件不存在、无法解析或读取失败，则返回空列表 ([]).\n",
    "    \"\"\"\n",
    "    # 获取当前模块的日志记录器实例，用于记录此函数的执行信息。详细的日志有助于追踪数据加载过程。\n",
    "    func_logger = logging.getLogger(__name__) # 使用模块级 logger\n",
    "    func_logger.info(f\"开始从 JSON 文件 '{json_path}' 加载数据，并在目录 '{image_dir}' 中关联图像...\")\n",
    "\n",
    "    # 步骤 1: 检查 JSON 文件是否存在。如果文件不存在，这是个严重问题，必须立即报告并停止。\n",
    "    if not os.path.exists(json_path):\n",
    "        func_logger.error(f\"错误：JSON 文件 '{json_path}' 未找到。请检查文件路径是否正确。\")\n",
    "        return [] # 文件不存在，无法继续，返回空列表。\n",
    "\n",
    "    # 初始化用于存储最终处理后文档信息的列表。\n",
    "    documents: List[Dict[str, Any]] = []\n",
    "    # 定义一个包含常见图像文件扩展名的列表。我们必须考虑多种可能的图像格式。\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp'] # 增加了 .webp 格式以支持更多现代格式\n",
    "\n",
    "    # 步骤 2: 尝试打开并解析 JSON 文件。文件读取和解析是容易出错的地方，必须小心处理异常。\n",
    "    try:\n",
    "        # 使用 'with' 语句确保文件在使用后自动关闭，即使发生错误。这是良好的资源管理习惯。\n",
    "        # 'r' 表示以只读模式打开文件。\n",
    "        # 'encoding='utf-8'' 指定使用 UTF-8 编码读取文件，以正确处理包括中文在内的各种字符。\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f) # 解析 JSON 数据，将其转换为 Python 的列表或字典。\n",
    "            # 验证 JSON 数据的顶层结构必须是列表。不符合预期的格式是数据源常见问题。\n",
    "            if not isinstance(json_data, list):\n",
    "                func_logger.error(f\"错误: JSON 文件 '{json_path}' 的顶层结构不是一个列表。请确保JSON文件格式正确。\")\n",
    "                return []\n",
    "    except json.JSONDecodeError as e:\n",
    "        # 如果 JSON 文件内容格式不正确，json.load() 会抛出 JSONDecodeError。这是 JSON 数据格式问题的明确指示。\n",
    "        func_logger.error(f\"错误：JSON 文件 '{json_path}' 解析失败。错误详情: {e}\")\n",
    "        func_logger.error(f\"        请确保文件内容是有效的 JSON 格式 (一个包含对象的列表)。\")\n",
    "        return [] # JSON 格式错误，返回空列表。\n",
    "    except Exception as e:\n",
    "        # 捕获其他可能的读取文件错误，例如文件权限问题或磁盘问题。必须考虑到所有可能性。\n",
    "        func_logger.error(f\"错误：读取 JSON 文件 '{json_path}' 时发生未知错误。错误详情: {e}\")\n",
    "        return [] # 其他读取错误，返回空列表。\n",
    "\n",
    "    func_logger.info(f\"已成功从 '{json_path}' 加载 {len(json_data)} 条原始记录。\")\n",
    "\n",
    "    # 初始化计数器，用于统计数据处理过程中的情况。清晰的统计数据有助于了解数据质量。\n",
    "    found_images_count = 0    # 成功关联到图像的文档数量。\n",
    "    missing_key_count = 0     # 因缺少必要字段 ('name' 或 'description') 或内容无效而被跳过的记录数量。\n",
    "    image_dir_warning_issued = False # 用于控制图像目录无效警告只输出一次。\n",
    "\n",
    "    # 步骤 3: 遍历从 JSON 文件加载的每一条原始记录。这是核心的处理循环。\n",
    "    for item_index, item in enumerate(json_data): # 使用 enumerate 获取索引，方便日志记录和问题定位。\n",
    "        # 确保每一项都是字典。非字典项是无效数据。\n",
    "        if not isinstance(item, dict):\n",
    "            func_logger.warning(f\"警告：跳过第 {item_index + 1} 条记录（JSON索引 {item_index}），因其不是一个有效的字典对象。记录内容: {item}\")\n",
    "            missing_key_count += 1\n",
    "            continue\n",
    "\n",
    "        doc_id = item.get('name')         # 尝试获取 'name' 字段作为文档 ID。使用 .get() 是安全的，即使键不存在也不会引发错误。\n",
    "        text_content = item.get('description') # 尝试获取 'description' 字段作为文本内容。\n",
    "\n",
    "        # 检查关键字段 'name' 和 'description' 是否存在且有值。这两个字段是文档的基本信息，缺失则无法索引。\n",
    "        # 如果任一字段缺失或为空字符串（去除空白后），则跳过该条记录。\n",
    "        valid_doc_id = doc_id is not None and str(doc_id).strip()\n",
    "        valid_text = text_content is not None and str(text_content).strip()\n",
    "\n",
    "        if not valid_doc_id or not valid_text:\n",
    "            missing_key_count += 1\n",
    "            reason = []\n",
    "            if not valid_doc_id: reason.append(\"'name'字段缺失或为空\")\n",
    "            if not valid_text: reason.append(\"'description'字段缺失或为空\")\n",
    "            # 记录详细警告，包括原始索引和部分内容，便于用户排查数据源。\n",
    "            func_logger.warning(f\"警告：跳过第 {item_index + 1} 条记录（原始JSON索引 {item_index}），原因: {', '.join(reason)}。记录内容: {item}\")\n",
    "            continue # 继续处理下一条记录。\n",
    "\n",
    "        # 初始化图像路径为 None。如果在指定目录中找不到匹配的图像，它将保持为 None。\n",
    "        image_path: Optional[str] = None\n",
    "        # 检查图像目录路径是否有效（已提供且存在于文件系统中）。只有目录有效时才尝试查找图像。\n",
    "        if image_dir and os.path.isdir(image_dir): # 确保 image_dir 是一个存在的目录\n",
    "            # 遍历预定义的图像扩展名列表，尝试构建并查找图像文件。\n",
    "            for ext in image_extensions:\n",
    "                # 构建潜在的图像文件名：文档ID（来自 'name' 字段）+ 当前扩展名。\n",
    "                # 使用 str(doc_id) 确保即使 doc_id 是数字也能正确拼接。\n",
    "                potential_image_filename = str(doc_id) + ext\n",
    "                # 使用 os.path.join 安全地构建跨平台的完整图像文件路径。\n",
    "                potential_image_path = os.path.join(image_dir, potential_image_filename)\n",
    "\n",
    "                # 检查构建的图像文件路径是否存在于文件系统中，并且是一个文件。\n",
    "                if os.path.exists(potential_image_path) and os.path.isfile(potential_image_path): # 确保是文件\n",
    "                    image_path = potential_image_path # 找到图像，记录其完整路径。\n",
    "                    found_images_count += 1           # 增加找到图像的计数。\n",
    "                    break # 找到一个匹配的图像后，无需再检查其他扩展名，跳出内层循环。\n",
    "        elif image_dir and not os.path.isdir(image_dir) and not image_dir_warning_issued:\n",
    "            # 如果提供了 image_dir 但它不是一个有效的目录，记录一次警告。使用标志避免重复警告。\n",
    "            func_logger.warning(f\"提供的图像目录 '{image_dir}' 不是一个有效的目录，将无法关联图像。\")\n",
    "            image_dir_warning_issued = True # 设置标志，表示警告已发出。\n",
    "        elif not image_dir:\n",
    "             # 如果未提供图像目录，在 DEBUG 级别记录，避免在正常运行时输出过多信息。\n",
    "             func_logger.debug(f\"未提供图像目录 (image_dir 为 None 或空)，将不尝试关联图像。\")\n",
    "\n",
    "\n",
    "        # 将处理后的文档信息（包括 ID、文本和可能的图像路径）添加到 `documents` 列表中。\n",
    "        documents.append({\n",
    "            'id': str(doc_id), # 确保文档 ID 是字符串类型，以便后续一致处理。\n",
    "            'text': str(text_content) if text_content is not None else None, # 确保文本是字符串；如果原始为 None，则保持 None。\n",
    "            'image_path': image_path # 存储找到的图像路径，如果未找到则为 None。\n",
    "        })\n",
    "\n",
    "    # 步骤 4: 打印数据加载和关联过程的总结信息。清晰的总结是流程结束的标志。\n",
    "    func_logger.info(f\"成功准备了 {len(documents)} 个文档用于后续处理。\")\n",
    "    if missing_key_count > 0:\n",
    "        func_logger.warning(f\"在原始 JSON 数据中，共有 {missing_key_count} 条记录因格式无效或缺少有效 'name'/'description' 字段而被跳过。\")\n",
    "    func_logger.info(f\"在有效文档中，共有 {found_images_count} 个文档成功关联了图像文件。\")\n",
    "\n",
    "    # 如果指定了图像目录，但没有找到任何图像文件（并且至少有一个文档被处理了），则给出提示，帮助用户排查图片关联问题。\n",
    "    if len(documents) > 0 and found_images_count == 0 and image_dir and os.path.isdir(image_dir):\n",
    "         func_logger.info(f\"提示: 未在目录 '{image_dir}' 中找到任何与文档 ID 匹配的图像文件。\")\n",
    "         func_logger.info(f\"        请检查图像文件名是否严格遵循 '文档ID.扩展名' 的格式 (例如，如果文档 'name' 是 'item01'，则图像应为 'item01.png')。\")\n",
    "\n",
    "    func_logger.info(f\"--- 数据加载与图像关联流程结束 ---\")\n",
    "    return documents # 返回包含所有已处理文档信息的列表。\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 多模态编码器类 (MultimodalEncoder)\n",
    "# 这是一个核心组件，负责将原始数据转化为机器可理解的向量表示。它的准确性直接影响检索效果。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "class MultimodalEncoder:\n",
    "    \"\"\"\n",
    "    使用 Hugging Face Transformers 库中的 CLIP (Contrastive Language–Image Pre-training) 模型\n",
    "    来对文本和/或图像进行编码，将它们转换为高维向量表示 (特征向量)。\n",
    "    CLIP 模型能够将文本和图像映射到同一个语义向量空间，使得它们的向量表示具有可比性，\n",
    "    这是多模态检索和理解的基础。\n",
    "\n",
    "    核心功能:\n",
    "    - 在初始化时加载预训练的 CLIP 模型和对应的处理器 (processor)。\n",
    "    - 提供 `encode` 方法，该方法可以接受文本字符串、图像文件路径，或两者都接受。\n",
    "    - `encode` 方法对输入进行预处理、通过 CLIP 模型进行编码，然后对输出的向量进行 L2 归一化。\n",
    "    - 返回一个字典，包含文本向量、图像向量以及（如果两者都提供了）两者的平均向量。\n",
    "    - 自动检测并优先使用 GPU (如果 CUDA 可用) 进行计算加速，否则回退到 CPU。\n",
    "    - L2 归一化对于后续使用 Faiss 进行基于内积 (Inner Product) 的相似度搜索至关重要，\n",
    "      因为归一化向量的内积等价于它们之间的余弦相似度。\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"\n",
    "        初始化 MultimodalEncoder 类。我需要确保模型和处理器能够被正确加载，这是编码器能工作的先决条件。\n",
    "\n",
    "        Args:\n",
    "            model_name (str): 指定要加载的 Hugging Face Hub 上的 CLIP 模型名称。\n",
    "                              例如 \"openai/clip-vit-base-patch32\"。\n",
    "                              不同的 CLIP 模型变体具有不同的性能、速度和输出向量维度。\n",
    "                              选择合适的模型取决于具体的应用需求和可用资源。\n",
    "                              请注意: \"openai/clip-vit-base-patch32\" 是一个性能和资源消耗均衡的基准模型。\n",
    "                              若资源极度受限，可研究更轻量模型，但可能影响编码质量。\n",
    "\n",
    "        Raises:\n",
    "            Exception: 如果在加载 CLIP 模型或处理器时发生任何错误（例如，网络问题导致无法下载模型文件、\n",
    "                       指定的模型名称无效、或者相关的依赖库未正确安装），则会抛出异常。\n",
    "                       由于模型是编码器的核心，加载失败意味着编码器无法工作，这需要立即报告为严重错误。\n",
    "        \"\"\"\n",
    "        # 获取一个特定于此类实例的日志记录器，方便追踪和调试。\n",
    "        self.logger = logging.getLogger(__name__ + \".\" + self.__class__.__name__)\n",
    "        self.logger.info(f\"开始初始化 MultimodalEncoder，尝试加载 CLIP 模型: {model_name}\")\n",
    "\n",
    "        try:\n",
    "            # 步骤 1: 加载与指定 CLIP 模型相关联的处理器 (CLIPProcessor)。\n",
    "            # 处理器负责将原始的文本和图像数据转换为 CLIP 模型期望的输入格式。\n",
    "            # 对于文本，这通常包括分词 (tokenization)、添加特殊标记、转换为 token ID。\n",
    "            # 对于图像，这通常包括调整大小 (resizing)、归一化 (normalization) 像素值。\n",
    "            # 这是数据进入模型的必经之路，不能出错。\n",
    "            self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "            self.logger.info(f\"CLIP Processor for '{model_name}' 加载成功。\")\n",
    "\n",
    "            # 步骤 2: 加载预训练的 CLIP 模型本身 (CLIPModel)。模型是编码功能的核心。\n",
    "            self.model = CLIPModel.from_pretrained(model_name)\n",
    "            self.logger.info(f\"CLIP Model '{model_name}' 加载成功。\")\n",
    "\n",
    "            # 步骤 3: 获取模型的输出向量维度。这个维度信息对于构建 Faiss 索引是必需的。\n",
    "            # 对于 CLIP 模型，文本编码器和图像编码器的输出向量维度通常是相同的。\n",
    "            # text_model.config.hidden_size 通常存储了这个维度值。\n",
    "            self.vector_dimension = self.model.text_model.config.hidden_size\n",
    "            self.logger.info(f\"CLIP 模型的特征向量维度为: {self.vector_dimension}\")\n",
    "\n",
    "            # 步骤 4: 将模型设置为评估模式 (evaluation mode)。\n",
    "            # 调用 .eval() 会关闭模型中的 Dropout 层和 Batch Normalization 层的更新行为。\n",
    "            # 这对于推理（编码）阶段非常重要，以确保结果的一致性和确定性。这是标准做法。\n",
    "            self.model.eval()\n",
    "\n",
    "            # 步骤 5: 检测可用的计算设备 (GPU 或 CPU)，并将模型迁移到该设备。\n",
    "            # 优先使用 GPU 以提高编码速度，这对于处理大量数据时非常重要。\n",
    "            if torch.cuda.is_available(): # 检查系统中是否有可用的 CUDA GPU。\n",
    "                self.device = torch.device(\"cuda\") # 如果有，则选择使用 GPU。\n",
    "                self.logger.info(\"检测到 CUDA 支持，模型将运行在 GPU 上以获得更快的编码速度。\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")  # 如果没有 GPU，则使用 CPU。\n",
    "                self.logger.info(\"未检测到 CUDA 支持，模型将运行在 CPU 上 (编码速度可能较慢)。\")\n",
    "\n",
    "            self.model.to(self.device) # 将模型的所有参数和缓冲区移动到选定的设备。\n",
    "            self.logger.info(f\"模型已成功移动到设备: {self.device}\")\n",
    "            self.logger.info(\"MultimodalEncoder 初始化成功完成。我已经准备好进行编码工作了。\")\n",
    "\n",
    "        except Exception as e:\n",
    "             # 如果在上述任何步骤中发生错误，记录详细的错误信息并重新抛出异常。加载模型的失败是致命的，必须报告。\n",
    "             self.logger.critical(f\"初始化 MultimodalEncoder 失败：加载 CLIP 模型 '{model_name}' 时发生严重错误。\")\n",
    "             self.logger.error(f\"错误详情: {e}\", exc_info=True) # exc_info=True 会记录完整的堆栈跟踪，便于诊断。\n",
    "             self.logger.error(\"请检查以下几点：\")\n",
    "             self.logger.error(f\"  1. 确保指定的模型名称 '{model_name}' 正确且在 Hugging Face Hub 上可用。\")\n",
    "             self.logger.error(\"  2. 确保已正确安装必要的 Python 库: 'transformers', 'torch', 'pillow'。\")\n",
    "             self.logger.error(\"     (例如，通过命令: pip install transformers torch pillow)\")\n",
    "             self.logger.error(\"  3. 确保网络连接正常，以便能够从 Hugging Face Hub 下载模型文件 (首次加载时需要)。\")\n",
    "             raise RuntimeError(f\"MultimodalEncoder 初始化失败: {e}\") from e # 重新抛出异常，表明初始化失败。\n",
    "\n",
    "    def _normalize_vector(self, vector: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        对输入的 NumPy 向量进行 L2 范数归一化 (L2 Normalization)。\n",
    "        L2 归一化将向量缩放，使其 L2 范数（欧几里得长度）为 1。\n",
    "        这对于计算余弦相似度非常重要：两个 L2 归一化向量的点积（内积）等于它们之间的余弦相似度。\n",
    "        保持向量归一化是使用内积进行相似度搜索的必要前处理。\n",
    "\n",
    "        Args:\n",
    "            vector (np.ndarray): 需要进行 L2 归一化的 NumPy 浮点数向量。\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 经过 L2 归一化后的向量。如果输入向量的范数非常接近于零（即零向量），\n",
    "                        则直接返回一个相同形状的零向量，以避免除以零的错误。这是为了处理特殊情况，确保程序的健壮性。\n",
    "        \"\"\"\n",
    "        # 计算向量的 L2 范数 (向量的欧几里得长度)。\n",
    "        norm = np.linalg.norm(vector)\n",
    "\n",
    "        # 检查范数是否大于一个很小的阈值 (epsilon)，以避免除以零或因浮点数精度问题导致的数值不稳定。\n",
    "        # 1e-9 是一个常用的小正数，用于判断一个浮点数是否“接近于零”。\n",
    "        if norm > 1e-9:\n",
    "            # 如果范数足够大，则将向量的每个元素除以该范数，得到归一化向量。\n",
    "            return vector / norm\n",
    "        else:\n",
    "            # 如果范数非常小（向量接近零向量），直接返回一个与输入向量形状相同但所有元素为零的向量。\n",
    "            # 这是对数值稳定性的考虑。\n",
    "            self.logger.debug(\"尝试归一化一个范数接近零的向量。返回零向量。\")\n",
    "            return np.zeros_like(vector)\n",
    "\n",
    "    def encode(self, text: Optional[str] = None, image_path: Optional[str] = None) -> Dict[str, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        对输入的文本字符串和/或图像文件路径进行编码，生成它们对应的归一化特征向量。\n",
    "        这是将原始数据转化为向量的核心操作。\n",
    "\n",
    "        Args:\n",
    "            text (Optional[str]): 需要编码的文本字符串。如果为 None 或空字符串，则不进行文本编码。\n",
    "            image_path (Optional[str]): 需要编码的图像文件的完整路径。如果为 None 或路径无效，则不进行图像编码。\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Optional[np.ndarray]]: 一个字典，包含以下可能的键值对：\n",
    "                - 'text_vector': 如果提供了有效的文本且编码成功，则为该文本的 L2 归一化 NumPy 向量 (float32)。否则为 None。\n",
    "                - 'image_vector': 如果提供了有效的图像路径、图像文件可读且编码成功，则为该图像的 L2 归一化 NumPy 向量 (float32)。否则为 None。\n",
    "                - 'mean_vector': 如果文本和图像都提供了，并且两者都成功编码，则为两者特征向量的 L2 归一化平均向量 (float32)。\n",
    "                                 这个平均向量可以作为文本和图像结合的多模态表示。如果任一编码失败或未提供，则为 None。\n",
    "            如果 `text` 和 `image_path` 都为 None，将记录错误并返回所有值为 None 的字典。\n",
    "        \"\"\"\n",
    "        # 输入有效性检查：必须至少提供文本或图像路径之一。如果什么都没提供，就无法编码。\n",
    "        is_text_valid = text is not None and text.strip()\n",
    "        is_image_path_valid = image_path is not None and image_path.strip()\n",
    "\n",
    "        if not is_text_valid and not is_image_path_valid:\n",
    "            self.logger.error(\"编码错误：必须至少提供有效的非空文本或有效的图像路径才能进行编码。\")\n",
    "            return {'text_vector': None, 'image_vector': None, 'mean_vector': None}\n",
    "\n",
    "        # 初始化各个向量为 None，它们将在编码成功后被赋值。\n",
    "        text_vector: Optional[np.ndarray] = None\n",
    "        image_vector: Optional[np.ndarray] = None\n",
    "        mean_vector: Optional[np.ndarray] = None\n",
    "\n",
    "        # 使用 torch.no_grad() 上下文管理器进行推理。\n",
    "        # 这会禁用 PyTorch 的梯度计算，从而减少内存消耗并加速计算，因为在编码（推理）阶段不需要进行反向传播。这是提高效率的标准做法。\n",
    "        with torch.no_grad():\n",
    "            # --- 步骤 A: 编码文本 (如果提供了文本) ---\n",
    "            if is_text_valid: # 确保文本非None且非空（去除两端空白后）\n",
    "                self.logger.debug(f\"开始编码文本: '{text[:50]}{'...' if len(text)>50 else ''}'\")\n",
    "                try:\n",
    "                    # 1. 预处理文本: 使用 CLIP Processor 将文本字符串转换为模型所需的输入格式。\n",
    "                    #    `return_tensors=\"pt\"`: 返回 PyTorch 张量 (tensors)。\n",
    "                    #    `padding=True`: 将批次内的文本填充到相同长度 (批次中最长文本的长度)。\n",
    "                    #    `truncation=True`: 如果文本超过模型的最大输入长度，则进行截断。\n",
    "                    #    `.to(self.device)`: 将生成的输入张量移动到之前确定的计算设备 (CPU 或 GPU)。\n",
    "                    text_inputs = self.processor(text=text, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "                    # 2. 获取文本特征: 调用 CLIP 模型的 `get_text_features` 方法，传入预处理后的输入。\n",
    "                    #    使用 `**text_inputs` 将字典解包为关键字参数。\n",
    "                    text_features_tensor = self.model.get_text_features(**text_inputs)\n",
    "\n",
    "                    # 3. 后处理特征张量:\n",
    "                    #    `.squeeze()`: 如果批次大小为1，移除批次维度，得到一个一维张量 (向量)。\n",
    "                    #    `.cpu()`: 将结果张量从 GPU (如果在使用) 移回 CPU，因为 NumPy 操作通常在 CPU 上进行。\n",
    "                    #    `.numpy()`: 将 PyTorch 张量转换为 NumPy 数组。\n",
    "                    #    `.astype('float32')`: 确保数据类型为 float32，这是 Faiss 常用的数值类型，也节省内存。\n",
    "                    text_vector_raw = text_features_tensor.squeeze().cpu().numpy().astype('float32')\n",
    "\n",
    "                    # 4. L2 归一化: 对原始的文本特征向量进行 L2 范数归一化。\n",
    "                    text_vector = self._normalize_vector(text_vector_raw)\n",
    "                    self.logger.debug(\"文本编码成功并已归一化。\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # 如果文本编码过程中发生任何错误，记录错误信息。这可能是由于模型输入处理问题。\n",
    "                    self.logger.error(f\"编码文本时发生错误。文本: '{text[:50]}...'. 错误详情: {e}\", exc_info=False) # exc_info=False 避免在每次文本编码失败时都打印完整堆栈\n",
    "                    text_vector = None # 确保在失败时 text_vector 为 None。\n",
    "\n",
    "            # --- 步骤 B: 编码图像 (如果提供了图像路径) ---\n",
    "            if is_image_path_valid: # 确保图像路径非None且非空\n",
    "                self.logger.debug(f\"开始编码图像: '{image_path}'\")\n",
    "                try:\n",
    "                    # 1. 加载图像: 使用 Pillow (PIL) 库的 Image.open() 方法打开图像文件。\n",
    "                    #    `.convert(\"RGB\")`: 确保图像转换为 RGB 格式。CLIP 模型通常期望 RGB 图像作为输入。\n",
    "                    #                       即使原始图像是 RGBA 或灰度图，也会被转换为 RGB。这是模型输入的要求。\n",
    "                    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "                    # 2. 预处理图像: 使用 CLIP Processor 将 PIL.Image 对象转换为模型所需的输入格式。\n",
    "                    #    `return_tensors=\"pt\"`: 返回 PyTorch 张量。\n",
    "                    #    `.to(self.device)`: 将输入张量移动到计算设备。\n",
    "                    image_inputs = self.processor(images=image_pil, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "                    # 3. 获取图像特征: 调用 CLIP 模型的 `get_image_features` 方法。\n",
    "                    image_features_tensor = self.model.get_image_features(**image_inputs)\n",
    "\n",
    "                    # 4. 后处理特征张量 (与文本编码类似): 转换为归一化的 NumPy float32 数组。\n",
    "                    image_vector_raw = image_features_tensor.squeeze().cpu().numpy().astype('float32')\n",
    "                    image_vector = self._normalize_vector(image_vector_raw)\n",
    "                    self.logger.debug(f\"图像 '{os.path.basename(image_path)}' 编码成功并已归一化。\")\n",
    "\n",
    "                except FileNotFoundError:\n",
    "                    # 如果指定的图像文件路径不存在。这是文件系统问题。\n",
    "                    self.logger.warning(f\"图像编码警告: 图像文件未找到于路径 '{image_path}'。将跳过此图像的编码。\")\n",
    "                    image_vector = None\n",
    "                except UnidentifiedImageError: # Pillow 无法识别图像格式\n",
    "                    self.logger.error(f\"图像编码错误: 无法识别或打开图像文件 '{image_path}'。文件可能已损坏或格式不受支持。\")\n",
    "                    image_vector = None\n",
    "                except Exception as e:\n",
    "                    # 如果在加载或处理图像时发生其他错误 (例如，图像文件损坏、权限问题)。\n",
    "                    self.logger.error(f\"编码图像 '{image_path}' 时发生错误。错误详情: {e}\", exc_info=False)\n",
    "                    image_vector = None\n",
    "\n",
    "        # --- 步骤 C: 计算平均向量 (仅当文本和图像都成功编码时) ---\n",
    "        # 检查 text_vector 和 image_vector 是否都成功生成 (即它们都不是 None)。只有同时有文本和图像时，计算平均向量才有意义。\n",
    "        if text_vector is not None and image_vector is not None:\n",
    "            self.logger.debug(\"文本和图像均成功编码，开始计算它们的平均向量...\")\n",
    "            try:\n",
    "                # 1. 计算平均: 使用 NumPy 的 `mean` 函数计算两个向量的逐元素平均值。\n",
    "                #    `axis=0` 表示沿着第一个轴（即向量本身）计算平均值。\n",
    "                #    确保结果的数据类型为 float32。\n",
    "                mean_vector_raw = np.mean(np.array([text_vector, image_vector]), axis=0).astype('float32')\n",
    "\n",
    "                # 2. L2 归一化: 对计算出的原始平均向量再次进行 L2 归一化。\n",
    "                #    这很重要，因为两个单位向量的平均向量长度通常不为 1。\n",
    "                mean_vector = self._normalize_vector(mean_vector_raw)\n",
    "                self.logger.debug(\"平均向量计算并归一化成功。\")\n",
    "            except Exception as e:\n",
    "                # 如果计算平均向量时出错。\n",
    "                self.logger.error(f\"计算文本和图像的平均向量时发生错误。错误详情: {e}\", exc_info=False)\n",
    "                mean_vector = None\n",
    "        elif (text_vector is not None or image_vector is not None):\n",
    "             # 如果只编码了文本或图像之一，则不需要计算平均向量。\n",
    "             self.logger.debug(\"仅文本或图像之一被成功编码，因此不计算平均向量。\")\n",
    "\n",
    "\n",
    "        # 总结编码结果，用于日志记录。这提供了每条编码操作的清晰反馈。\n",
    "        results_summary = []\n",
    "        if text_vector is not None: results_summary.append(\"文本向量\")\n",
    "        if image_vector is not None: results_summary.append(\"图像向量\")\n",
    "        if mean_vector is not None: results_summary.append(\"平均向量\")\n",
    "\n",
    "        input_summary_parts = []\n",
    "        if is_text_valid: input_summary_parts.append(f\"文本='{text[:30]}{'...' if len(text)>30 else ''}'\")\n",
    "        if is_image_path_valid: input_summary_parts.append(f\"图像='{os.path.basename(image_path)}'\")\n",
    "        input_desc = \", \".join(input_summary_parts) if input_summary_parts else \"无有效输入\"\n",
    "\n",
    "        # 根据编码结果是否有向量生成，记录不同级别的日志。\n",
    "        if not results_summary and (is_text_valid or is_image_path_valid):\n",
    "             self.logger.warning(f\"编码完成，但对于输入 ({input_desc})，未能生成任何有效向量。\")\n",
    "        elif results_summary:\n",
    "             self.logger.info(f\"编码完成对于 ({input_desc})。成功生成的向量: {', '.join(results_summary)}。\")\n",
    "\n",
    "        # 返回包含所有结果向量的字典。\n",
    "        return {\n",
    "            'text_vector': text_vector,\n",
    "            'image_vector': image_vector,\n",
    "            'mean_vector': mean_vector\n",
    "        }\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 索引器类 (Indexer)\n",
    "# Indexer 是整个系统的知识库构建者和管理者。它的稳定性和数据一致性至关重要。\n",
    "# 我必须确保数据库和向量索引的正确初始化、填充和持久化。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "class Indexer:\n",
    "    \"\"\"\n",
    "    Indexer 类是多模态 RAG (Retrieval Augmented Generation) 系统的数据管理核心。它负责：\n",
    "    1.  **接收文档数据**: 从外部（例如，`load_data_from_json_and_associate_images` 函数）获取包含文本和图像路径的文档列表。\n",
    "    2.  **调用编码器**: 使用内部的 `MultimodalEncoder` 实例对每个文档的文本内容和/或关联图像进行向量化，生成特征向量。\n",
    "    3.  **存储元数据**: 将文档的原始信息（如原始ID、文本内容、图像文件路径）存储在 SQLite 数据库中。\n",
    "        数据库中会为每个文档生成一个自增的整数主键 `internal_id`，这个ID将用作 Faiss 索引中对应向量的唯一标识符。\n",
    "    4.  **构建和管理向量索引**:\n",
    "        -   创建并维护 **三个独立** 的 Faiss 索引：一个用于存储纯文本向量，一个用于存储纯图像向量，一个用于存储文本和图像结合的平均向量。\n",
    "        -   每个 Faiss 索引都使用 `IndexIDMap2` 类型，这允许我们将向量与我们自定义的 `internal_id` (来自SQLite数据库) 关联起来，方便后续检索和数据回溯。\n",
    "        -   索引使用内积 (`IndexFlatIP`) 作为相似度度量方法。由于所有向量都经过了L2归一化，内积等价于余弦相似度，值越大表示越相似。\n",
    "    5.  **持久化**: 能够从指定文件路径加载先前已保存的索引文件和数据库，或者在首次运行时创建它们。在关闭时，会将当前的索引状态保存到文件，以便下次使用。\n",
    "\n",
    "    这种分离索引的设计（文本、图像、平均）允许在检索阶段根据用户查询的类型（纯文本、纯图像、或图文多模态）灵活地选择最合适的索引进行搜索，从而提高检索的准确性和效率。\n",
    "    我的目标是构建一个可靠的、易于管理的知识库。\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 db_path: str,\n",
    "                 faiss_text_index_path: str,\n",
    "                 faiss_image_index_path: str,\n",
    "                 faiss_mean_index_path: str,\n",
    "                 clip_model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        \"\"\"\n",
    "        初始化 Indexer 实例。我需要一丝不苟地设置好数据库和所有索引文件路径，并准备好编码器。\n",
    "\n",
    "        Args:\n",
    "            db_path (str): 指定 SQLite 数据库文件的保存路径。\n",
    "            faiss_text_index_path (str): 指定文本向量 Faiss 索引文件的保存路径。\n",
    "            faiss_image_index_path (str): 指定图像向量 Faiss 索引文件的保存路径。\n",
    "            faiss_mean_index_path (str): 指定平均向量 Faiss 索引文件的保存路径。\n",
    "            clip_model_name (str): 传递给内部 `MultimodalEncoder` 的 CLIP 模型名称。\n",
    "                                   此模型名称必须与后续用于查询编码的模型保持一致，以确保向量空间的一致性。\n",
    "\n",
    "        Raises:\n",
    "            Exception: 如果在初始化内部编码器、数据库或 Faiss 索引时发生任何错误，则抛出异常。\n",
    "                       这些都是 Indexer 工作的核心依赖，任何失败都是严重问题。\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__ + \".\" + self.__class__.__name__)\n",
    "        self.logger.info(\"开始初始化 Indexer...\")\n",
    "\n",
    "        # 保存传入的路径和模型名称配置。清晰的路径信息是管理文件的重要前提。\n",
    "        self.db_path = db_path\n",
    "        self.faiss_text_index_path = faiss_text_index_path\n",
    "        self.faiss_image_index_path = faiss_image_index_path\n",
    "        self.faiss_mean_index_path = faiss_mean_index_path\n",
    "        self.logger.info(f\"  数据库路径: {self.db_path}\")\n",
    "        self.logger.info(f\"  文本索引路径: {self.faiss_text_index_path}\")\n",
    "        self.logger.info(f\"  图像索引路径: {self.faiss_image_index_path}\")\n",
    "        self.logger.info(f\"  平均向量索引路径: {self.faiss_mean_index_path}\")\n",
    "\n",
    "        # 步骤 1: 初始化多模态编码器 (MultimodalEncoder)。\n",
    "        # Indexer 内部拥有一个 Encoder 实例，专门用于对其接收的文档进行编码。这是将数据转换为向量的基础。\n",
    "        self.logger.info(f\"  - 正在初始化内部 MultimodalEncoder，使用 CLIP 模型: {clip_model_name}...\")\n",
    "        try:\n",
    "            self.encoder = MultimodalEncoder(clip_model_name) # 创建编码器实例。\n",
    "            self.vector_dimension = self.encoder.vector_dimension # 从编码器获取产生的向量维度。确保所有索引都使用正确的维度。\n",
    "            self.logger.info(f\"  - MultimodalEncoder 初始化完成。特征向量维度为: {self.vector_dimension}。\")\n",
    "        except Exception as e_encoder:\n",
    "            self.logger.critical(f\"Indexer 初始化严重失败：内部 MultimodalEncoder 创建失败。错误: {e_encoder}\", exc_info=True)\n",
    "            raise RuntimeError(f\"Indexer 无法初始化 Encoder: {e_encoder}\") from e_encoder\n",
    "\n",
    "        # 步骤 2: 初始化 SQLite 数据库 (用于存储文档元数据)。\n",
    "        # 调用私有方法 `_init_db` 来确保数据库文件存在，并创建所需的表结构（如果尚不存在）。数据库是元数据的可靠来源。\n",
    "        self.logger.info(f\"  - 正在初始化 SQLite 数据库，路径: '{self.db_path}'...\")\n",
    "        try:\n",
    "            self._init_db() # 此方法会处理数据库目录的创建。\n",
    "            self.logger.info(f\"  - SQLite 数据库初始化完成。\")\n",
    "        except Exception as e_db_init:\n",
    "            self.logger.critical(f\"Indexer 初始化严重失败：SQLite 数据库初始化失败。错误: {e_db_init}\", exc_info=True)\n",
    "            raise RuntimeError(f\"Indexer 无法初始化数据库: {e_db_init}\") from e_db_init\n",
    "\n",
    "\n",
    "        # 步骤 3: 加载或创建三个独立的 Faiss 向量索引。\n",
    "        # 分别为文本向量、图像向量和平均向量（文本+图像组合）加载或创建 Faiss 索引。\n",
    "        # `_load_or_create_faiss_index` 方法会处理文件存在性检查、维度匹配和新索引创建的逻辑。索引是快速检索的基础，必须准备好。\n",
    "        self.logger.info(f\"  - 正在加载或创建 Faiss 向量索引...\")\n",
    "        try:\n",
    "            self.text_index = self._load_or_create_faiss_index(self.faiss_text_index_path, \"文本(Text)\")\n",
    "            self.image_index = self._load_or_create_faiss_index(self.faiss_image_index_path, \"图像(Image)\")\n",
    "            self.mean_index = self._load_or_create_faiss_index(self.faiss_mean_index_path, \"平均(Mean)\")\n",
    "            self.logger.info(f\"  - 所有 Faiss 索引均已准备就绪。\")\n",
    "        except Exception as e_faiss_init:\n",
    "            self.logger.critical(f\"Indexer 初始化严重失败：一个或多个 Faiss 索引加载/创建失败。错误: {e_faiss_init}\", exc_info=True)\n",
    "            raise RuntimeError(f\"Indexer 无法初始化 Faiss 索引: {e_faiss_init}\") from e_faiss_init\n",
    "\n",
    "\n",
    "        self.logger.info(\"Indexer 初始化成功完成。我将竭尽全力确保索引的准确和高效。\")\n",
    "\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"\n",
    "        初始化 SQLite 数据库连接并创建所需的 'documents' 表（如果它还不存在）。\n",
    "        这个表用于存储文档的元数据，并将原始文档 ID (doc_id) 映射到数据库生成的\n",
    "        自增主键 `internal_id`。这个 `internal_id` 将作为 Faiss 索引中对应向量的 ID。\n",
    "        此方法还会确保数据库文件所在的目录存在。这是构建可靠知识库的基石。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"正在连接并初始化数据库表结构于路径: '{self.db_path}'...\")\n",
    "\n",
    "        # 确保数据库文件所在的目录存在，如果不存在则创建它。这是文件系统操作的标准防御性编程。\n",
    "        db_directory = os.path.dirname(self.db_path)\n",
    "        if db_directory and not os.path.exists(db_directory):\n",
    "            try:\n",
    "                os.makedirs(db_directory, exist_ok=True) # exist_ok=True 表示如果目录已存在则不抛出错误。\n",
    "                self.logger.debug(f\"已确保数据库目录 '{db_directory}' 存在 (或已创建)。\")\n",
    "            except OSError as e:\n",
    "                self.logger.error(f\"创建数据库目录 '{db_directory}' 失败: {e}\", exc_info=True)\n",
    "                raise # Re-raise the exception as this is critical\n",
    "\n",
    "        try:\n",
    "            # 使用 'with' 语句确保数据库连接在使用后自动关闭，并能自动处理事务（默认提交，出错回滚）。这是安全的数据库操作模式。\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor() # 获取数据库游标，用于执行 SQL 命令。\n",
    "\n",
    "                # SQL 语句，用于创建 'documents' 表。\n",
    "                # `IF NOT EXISTS` 确保如果表已经存在，则不会尝试重新创建它，从而避免错误。\n",
    "                # 表结构定义必须精确：\n",
    "                #   - internal_id: 整数类型，主键，自动增长。这是数据库内部ID，也将用作Faiss索引的ID。\n",
    "                #   - doc_id: 文本类型，唯一约束，不能为空。这是原始文档的唯一标识符 (例如来自JSON的'name'字段)，必须保证唯一性。\n",
    "                #   - text: 文本类型，存储文档的文本内容，允许为空 (NULL)。\n",
    "                #   - image_path: 文本类型，存储关联图像文件的路径，允许为空。\n",
    "                cursor.execute('''\n",
    "                    CREATE TABLE IF NOT EXISTS documents (\n",
    "                        internal_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        doc_id TEXT UNIQUE NOT NULL,\n",
    "                        text TEXT,\n",
    "                        image_path TEXT\n",
    "                    )\n",
    "                ''')\n",
    "\n",
    "                # 可选：在 'doc_id' 列上创建一个索引。\n",
    "                # 这可以加快通过原始 `doc_id` 查找记录的速度，例如在 `index_documents` 方法中检查重复文档时。索引能提升性能。\n",
    "                cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_doc_id ON documents (doc_id)\")\n",
    "\n",
    "                conn.commit() # 提交事务，使表结构更改和索引创建生效。\n",
    "                self.logger.info(f\"数据库表 'documents' (及索引 'idx_doc_id') 初始化成功，或已存在。\")\n",
    "        except sqlite3.Error as e: # Catch specific SQLite errors\n",
    "             self.logger.critical(f\"严重错误：初始化 SQLite 数据库 '{self.db_path}' 失败。错误详情: {e}\", exc_info=True)\n",
    "             raise RuntimeError(f\"SQLite数据库操作失败: {e}\") from e # Re-raise as a more generic runtime error\n",
    "        except Exception as e_general:\n",
    "            self.logger.critical(f\"初始化 SQLite 数据库 '{self.db_path}' 时发生未知错误。错误详情: {e_general}\", exc_info=True)\n",
    "            raise RuntimeError(f\"SQLite数据库初始化未知错误: {e_general}\") from e_general\n",
    "\n",
    "\n",
    "    def _load_or_create_faiss_index(self, index_path: str, index_type_description: str) -> faiss.Index:\n",
    "        \"\"\"\n",
    "        尝试从指定路径加载一个 Faiss 索引文件。\n",
    "        - 如果文件存在且其内部存储的向量维度与当前编码器 (`self.encoder`) 的输出维度匹配，则加载该索引。\n",
    "        - 如果文件不存在，或者文件存在但维度不匹配（表明该索引可能是用不同模型创建的，不能兼容），则创建一个新的、空的 Faiss 索引。\n",
    "        - 使用 `faiss.IndexIDMap2` 类型的索引，它允许我们将自定义的 64 位整数 ID (数据库的 internal_id) 与每个向量关联起来。\n",
    "        此方法还会确保索引文件所在的目录存在。我的职责是确保索引可用，无论是加载旧的还是创建一个新的。\n",
    "\n",
    "        Args:\n",
    "            index_path (str): Faiss 索引文件的期望路径。\n",
    "            index_type_description (str): 索引类型的描述性名称 (例如 \"文本\", \"图像\", \"平均\")，主要用于日志记录。\n",
    "\n",
    "        Returns:\n",
    "            faiss.Index: 加载的或新创建的 Faiss 索引对象 (具体类型为 `faiss.IndexIDMap2`)。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"正在为 '{index_type_description}' 索引加载或创建 Faiss 文件于路径: '{index_path}'...\")\n",
    "\n",
    "        # 确保 Faiss 索引文件所在的目录存在，如果不存在则创建它。\n",
    "        index_directory = os.path.dirname(index_path)\n",
    "        if index_directory and not os.path.exists(index_directory):\n",
    "            try:\n",
    "                os.makedirs(index_directory, exist_ok=True)\n",
    "                self.logger.debug(f\"已确保 '{index_type_description}' 索引的目录 '{index_directory}' 存在 (或已创建)。\")\n",
    "            except OSError as e:\n",
    "                self.logger.critical(f\"创建Faiss索引目录 '{index_directory}' 失败: {e}\", exc_info=True)\n",
    "                raise # Re-raise as this is critical\n",
    "\n",
    "\n",
    "        try:\n",
    "            # 检查指定的索引文件是否已经存在于文件系统中。\n",
    "            if os.path.exists(index_path) and os.path.isfile(index_path): # 确保是文件\n",
    "                self.logger.info(f\"发现已存在的 '{index_type_description}' Faiss 索引文件，尝试加载: {index_path}\")\n",
    "                # 使用 faiss.read_index 函数读取磁盘上的索引文件。\n",
    "                index = faiss.read_index(index_path)\n",
    "                self.logger.info(f\"文件 '{index_path}' 读取成功，包含 {index.ntotal} 个向量，维度为 {index.d}。\")\n",
    "\n",
    "                # **重要**: 检查加载的索引的维度 (`index.d`) 是否与当前编码器模型产生的向量维度 (`self.vector_dimension`) 一致。\n",
    "                # 维度不一致意味着索引与当前模型不兼容，强行使用会导致错误或无效结果。必须进行此项检查。\n",
    "                if index.d != self.vector_dimension:\n",
    "                    # 如果维度不匹配，这意味着已加载的索引是用不同的（或不同配置的）CLIP 模型创建的，因此不能直接使用。\n",
    "                    self.logger.warning(f\"维度不匹配警告! 加载的 '{index_type_description}' 索引维度 ({index.d}) 与当前编码器配置的维度 ({self.vector_dimension}) 不一致。\")\n",
    "                    self.logger.warning(f\"这通常意味着之前的索引是用不同的模型创建的。将忽略已加载的旧索引，并创建一个新的空 '{index_type_description}' 索引。\")\n",
    "                    # 创建一个新的、空的 Faiss 索引来替换掉加载的不兼容的旧索引。\n",
    "                    index = self._create_new_faiss_index(index_type_description)\n",
    "                else:\n",
    "                    # 维度匹配，加载成功。太好了，我们可以复用之前的索引了。\n",
    "                    self.logger.info(f\"成功加载 '{index_type_description}' Faiss 索引，维度 ({index.d}) 与当前模型匹配。索引中包含 {index.ntotal} 个向量。\")\n",
    "            else:\n",
    "                # 如果索引文件不存在。\n",
    "                self.logger.info(f\"未找到 '{index_type_description}' Faiss 索引文件: '{index_path}'。将创建一个新的空索引。\")\n",
    "                # 调用内部方法创建新的空索引。\n",
    "                index = self._create_new_faiss_index(index_type_description)\n",
    "        except Exception as e:\n",
    "            # 处理在加载或读取索引文件过程中可能发生的任何其他错误。即使加载失败，也要保证能创建一个新索引。\n",
    "            self.logger.error(f\"错误：加载或处理 '{index_type_description}' Faiss 索引 '{index_path}' 失败。错误详情: {e}\", exc_info=True)\n",
    "            self.logger.info(f\"作为安全回退机制，将创建一个新的空 '{index_type_description}' 索引。\")\n",
    "            # 即使加载失败，也创建一个新的空索引，以保证程序能够继续运行（尽管可能没有历史数据）。\n",
    "            index = self._create_new_faiss_index(index_type_description)\n",
    "        return index\n",
    "\n",
    "    def _create_new_faiss_index(self, index_type_description: str) -> faiss.Index:\n",
    "         \"\"\"\n",
    "         创建一个新的、空的 Faiss 索引。\n",
    "         该索引被配置为使用内积 (`IndexFlatIP`) 进行相似度搜索，并使用 `IndexIDMap2` 包装器\n",
    "         来支持为每个向量存储自定义的 64 位整数 ID。\n",
    "         `IndexFlatIP` 适用于存储原始（未压缩）向量并进行精确的、暴力的内积搜索。\n",
    "         对于已经 L2 归一化的向量，内积得分等价于余弦相似度。我选择这个类型是因为它简单且对于归一化向量表现良好。\n",
    "\n",
    "         Args:\n",
    "             index_type_description (str): 索引类型的描述 (例如 \"文本\", \"图像\")，用于日志记录。\n",
    "\n",
    "         Returns:\n",
    "             faiss.Index: 新创建的、空的 `faiss.IndexIDMap2` 索引对象。\n",
    "         \"\"\"\n",
    "         self.logger.info(f\"开始为 '{index_type_description}' 创建一个新的空 Faiss 索引...\")\n",
    "         # 步骤 1: 创建基础索引 (也称为 quantizer，在更复杂的索引类型中作用更明显)。\n",
    "         # 这里使用 `faiss.IndexFlatIP`:\n",
    "         #   - `IndexFlat`: 表示 Faiss 将存储完整的、未经压缩或量化的原始向量。这提供了最精确的搜索结果，但需要更多内存。\n",
    "         #   - `IP` (Inner Product): 表示该索引将使用内积作为向量间的距离/相似度度量。\n",
    "         #   当存储的向量都经过 L2 归一化时，它们之间的内积值等于它们之间的余弦相似度。\n",
    "         #   `self.vector_dimension` 是从 CLIP 模型获取的特征向量的维度，这是索引创建的基础。\n",
    "         quantizer = faiss.IndexFlatIP(self.vector_dimension)\n",
    "         self.logger.debug(f\"  为 '{index_type_description}' 创建了 IndexFlatIP 基础索引，维度: {self.vector_dimension}。\")\n",
    "\n",
    "         # 步骤 2: 创建 ID 映射包装器 `faiss.IndexIDMap2`。\n",
    "         #   - `IndexIDMap2` 包装了一个基础索引 (此处是 `quantizer`)。\n",
    "         #   - 它允许我们在向索引添加向量时，为每个向量指定一个我们自己定义的 64 位整数 ID。\n",
    "         #   - 在搜索时，它会返回这些我们指定的 ID，而不是 Faiss 内部的连续行号。\n",
    "         #   - '2' 在名称中通常表示它使用了更现代或更灵活的内部ID重映射机制。\n",
    "         #   - 我们将使用从 SQLite 数据库生成的 `internal_id` 作为这个自定义 ID。这保证了向量与元数据的关联。\n",
    "         index = faiss.IndexIDMap2(quantizer)\n",
    "         self.logger.debug(f\"  将 IndexFlatIP 包装在 IndexIDMap2 中，以支持自定义向量 ID。\")\n",
    "\n",
    "         self.logger.info(f\"已成功为 '{index_type_description}' 创建一个新的、空的 Faiss 索引 (类型: IndexIDMap2 包裹 IndexFlatIP)。\")\n",
    "         self.logger.info(f\"    索引维度: {self.vector_dimension}。\")\n",
    "         self.logger.info(f\"    相似度度量: 内积 (Inner Product) - 对于归一化向量，这等同于余弦相似度。\")\n",
    "         return index\n",
    "\n",
    "    def index_documents(self, documents: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        核心的文档索引流程。\n",
    "        该方法接收一个文档列表，对每个文档进行多模态编码（文本和/或图像），\n",
    "        然后将文档的元数据存储到 SQLite 数据库中，并将生成的特征向量（文本向量、图像向量、平均向量）\n",
    "        及其对应的数据库内部ID (`internal_id`) 添加到各自的 Faiss 索引中。\n",
    "        此方法会处理基于 `doc_id` 的重复文档（即，如果一个具有相同 `doc_id` 的文档已存在于数据库中，则跳过它）。\n",
    "        为了提高效率，向量会分批收集，然后一次性批量添加到 Faiss 索引中。这是确保效率和可靠性的关键。\n",
    "\n",
    "        Args:\n",
    "            documents (List[Dict[str, Any]]): 一个字典列表，其中每个字典代表一个待索引的文档。\n",
    "                                    每个字典应至少包含 'id' (原始文档ID), 'text' (文本内容),\n",
    "                                    和 'image_path' (关联图像的路径，可能为None) 这几个键。\n",
    "                                    这通常是 `load_data_from_json_and_associate_images` 函数的输出格式。\n",
    "        \"\"\"\n",
    "        # 检查输入的文档列表是否为空。如果没有任何文档，就没有必要继续。\n",
    "        if not documents:\n",
    "            self.logger.info(\"未提供任何文档进行索引操作。流程结束。\")\n",
    "            return\n",
    "\n",
    "        self.logger.info(f\"开始执行文档索引流程，准备处理 {len(documents)} 个文档...\")\n",
    "\n",
    "        # 初始化列表，用于批量收集需要添加到 Faiss 索引的向量和它们对应的 ID。\n",
    "        # 分别为文本、图像和平均向量准备独立的批处理列表。批量操作比逐个添加更高效。\n",
    "        text_vectors_batch: List[np.ndarray] = []   # 存储文本特征向量 (NumPy 数组)。\n",
    "        text_ids_batch: List[int] = []              # 存储与文本向量对应的 `internal_id` (整数)。\n",
    "        image_vectors_batch: List[np.ndarray] = []  # 存储图像特征向量。\n",
    "        image_ids_batch: List[int] = []             # 存储与图像向量对应的 `internal_id`。\n",
    "        mean_vectors_batch: List[np.ndarray] = []   # 存储平均（文本+图像）特征向量。\n",
    "        mean_ids_batch: List[int] = []              # 存储与平均向量对应的 `internal_id`。\n",
    "\n",
    "        # 初始化计数器，用于跟踪索引过程的各种统计数据。清晰的统计数据帮助了解索引的实际情况。\n",
    "        processed_count = 0          # 成功处理并至少为其生成了一个向量的文档数量。\n",
    "        skipped_duplicate_count = 0  # 因 `doc_id` 已存在于数据库中而被跳过的文档数量。\n",
    "        skipped_invalid_input_count = 0 # 因输入数据无效（缺少ID或内容）而被跳过的文档数量。\n",
    "        encoding_failure_count = 0   # 因编码阶段（文本或图像）出错而未能为其生成向量的文档数量 (即使元数据可能已插入)。\n",
    "        db_check_error_count = 0     # 因检查重复项时数据库出错而被跳过的文档数量。\n",
    "        db_insert_error_count = 0    # 因数据库插入操作出错而被跳过的文档数量。\n",
    "\n",
    "        conn: Optional[sqlite3.Connection] = None # 初始化数据库连接变量，确保在 try...finally 块中可见，用于最终的连接关闭。\n",
    "        try:\n",
    "            # 步骤 1: 建立与 SQLite 数据库的连接。\n",
    "            self.logger.debug(f\"正在连接到数据库: {self.db_path}\")\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor() # 获取数据库游标。\n",
    "            # sqlite3 默认在执行 DML 语句 (如 INSERT) 时会自动开始一个事务。\n",
    "            # 我们将在所有文档处理完毕后，在循环外部统一提交 (commit) 或回滚 (rollback) 事务，\n",
    "            # 以确保数据库操作的原子性（相对于整个批次而言）。这是保证数据一致性的重要手段。\n",
    "\n",
    "            # 步骤 2: 遍历每个待索引的文档。\n",
    "            self.logger.info(f\"开始遍历 {len(documents)} 个文档进行处理和编码...\")\n",
    "            for i, doc_data in enumerate(documents): # 使用 enumerate 获取索引和文档数据，便于日志记录和问题定位。\n",
    "                doc_id = doc_data.get('id')          # 获取原始文档 ID。\n",
    "                text_content = doc_data.get('text')  # 获取文本内容。\n",
    "                image_file_path = doc_data.get('image_path') # 获取图像路径。\n",
    "\n",
    "                self.logger.debug(f\"处理文档 {i+1}/{len(documents)}: ID='{doc_id}'\")\n",
    "\n",
    "                # 基本有效性验证：`doc_id` 必须存在且非空。这是文档的唯一标识。\n",
    "                doc_id_str = str(doc_id).strip() if doc_id is not None else None\n",
    "                if not doc_id_str:\n",
    "                    self.logger.warning(f\"跳过列表中的第 {i+1} 条记录（原始索引 {i}），因其缺少有效 'id' 字段。记录: {doc_data}\")\n",
    "                    skipped_invalid_input_count += 1 # 计入无效输入\n",
    "                    continue # 跳到下一个文档。\n",
    "\n",
    "                # 至少需要文本或图像路径之一才能进行有意义的编码。\n",
    "                has_valid_text = text_content is not None and str(text_content).strip()\n",
    "                has_valid_image = image_file_path is not None and str(image_file_path).strip()\n",
    "                if not has_valid_text and not has_valid_image:\n",
    "                     self.logger.warning(f\"跳过文档 ID '{doc_id_str}'，因为它既没有有效的文本内容，也没有有效的关联图像路径。无法为其生成任何向量。\")\n",
    "                     skipped_invalid_input_count += 1 # 计入无效输入\n",
    "                     continue # 跳到下一个文档。\n",
    "\n",
    "                # --- 2a. 检查文档是否已在数据库中存在 (基于 `doc_id`) ---\n",
    "                # 这是防止重复索引的关键步骤。\n",
    "                try:\n",
    "                    cursor.execute(\"SELECT internal_id FROM documents WHERE doc_id = ?\", (doc_id_str,))\n",
    "                    existing_record = cursor.fetchone() # 获取查询结果（如果存在的话）。\n",
    "                    if existing_record:\n",
    "                         self.logger.debug(f\"文档 ID '{doc_id_str}' 已存在于数据库中 (其 internal_id 为: {existing_record[0]})。将跳过此重复文档的索引。\")\n",
    "                         skipped_duplicate_count += 1\n",
    "                         continue # 跳到下一个文档。\n",
    "                except sqlite3.Error as e_check:\n",
    "                    self.logger.error(f\"检查文档 ID '{doc_id_str}' 是否存在时发生数据库错误: {e_check}。将跳过此文档以防意外行为。\")\n",
    "                    db_check_error_count += 1 # 计入数据库检查错误\n",
    "                    continue # 跳到下一个文档以确保安全。\n",
    "\n",
    "\n",
    "                # --- 2b. 使用内部 Encoder 对文档进行多模态向量化 ---\n",
    "                # 在插入数据库之前进行编码，如果编码失败，则不插入元数据，保持一致性。\n",
    "                encoded_data: Optional[Dict[str, Optional[np.ndarray]]] = None # 初始化编码结果。\n",
    "                encoding_succeeded = False\n",
    "                try:\n",
    "                    self.logger.debug(f\"开始为文档 '{doc_id_str}' 进行多模态编码...\")\n",
    "                    # Pass original text_content and image_file_path, encode() handles validation internally\n",
    "                    encoded_data = self.encoder.encode(text=text_content, image_path=image_file_path)\n",
    "                    # 检查编码是否至少生成了一个向量\n",
    "                    if encoded_data and (encoded_data.get('text_vector') is not None or\n",
    "                                          encoded_data.get('image_vector') is not None or\n",
    "                                          encoded_data.get('mean_vector') is not None):\n",
    "                        encoding_succeeded = True\n",
    "                        self.logger.debug(f\"文档 '{doc_id_str}' 编码成功，至少生成了一个向量。\")\n",
    "                    else:\n",
    "                         self.logger.warning(f\"文档 '{doc_id_str}' 编码完成，但未能生成任何有效向量 (即使输入有效)。\")\n",
    "                         encoding_failure_count += 1\n",
    "                         # 不继续插入数据库，因为没有向量可以关联\n",
    "\n",
    "                except Exception as encode_e:\n",
    "                    self.logger.error(f\"严重错误：在编码文档 '{doc_id_str}' 时发生意外错误: {encode_e}\", exc_info=True)\n",
    "                    encoding_failure_count += 1\n",
    "                    # 不继续插入数据库，因为编码失败\n",
    "\n",
    "                # 如果编码失败（没有生成任何向量或发生异常），则跳过此文档的后续处理（数据库插入和Faiss添加）。\n",
    "                if not encoding_succeeded:\n",
    "                    self.logger.warning(f\"由于文档 '{doc_id_str}' 的编码未能生成有效向量，将跳过此文档的数据库插入和 Faiss 索引添加。\")\n",
    "                    continue\n",
    "\n",
    "                # --- 2c. 将文档元数据插入到数据库，并获取生成的 `internal_id` ---\n",
    "                # 只有在编码成功后才执行此步骤。\n",
    "                internal_id: Optional[int] = None # 初始化 internal_id。\n",
    "                try:\n",
    "                    # 执行 INSERT 语句将新文档的元数据插入到 'documents' 表。\n",
    "                    # 使用参数化查询 (问号占位符) 来防止 SQL 注入攻击。\n",
    "                    cursor.execute(\n",
    "                        \"INSERT INTO documents (doc_id, text, image_path) VALUES (?, ?, ?)\",\n",
    "                        (doc_id_str, text_content, image_file_path) # 使用清理过的 doc_id_str\n",
    "                    )\n",
    "                    # 获取刚刚插入行的自增主键 (`internal_id`)。\n",
    "                    # `cursor.lastrowid` 返回最后插入行的 ROWID。\n",
    "                    internal_id = cursor.lastrowid\n",
    "                    if internal_id is None:\n",
    "                        # 这是一个理论上不太可能发生但在极端情况下需要考虑的问题。\n",
    "                        self.logger.error(f\"严重数据库错误：为文档 '{doc_id_str}' 插入元数据后，未能获取有效的 internal_id (lastrowid is None)。这将导致向量无法与元数据关联！\")\n",
    "                        # 尝试回滚当前文档的操作？或者记录错误并继续？这里选择记录并尝试继续，但标记错误。\n",
    "                        db_insert_error_count += 1\n",
    "                        continue # 跳过这个文档的 Faiss 添加，因为没有有效的 ID\n",
    "                    self.logger.debug(f\"文档 '{doc_id_str}' 的元数据已成功插入数据库，获得的 internal_id: {internal_id}\")\n",
    "\n",
    "                except sqlite3.IntegrityError:\n",
    "                    # 当尝试插入的 `doc_id` 违反了表的 UNIQUE 约束时（理论上不应发生，因为前面已检查过）。\n",
    "                    # 这是一个额外的防御层。\n",
    "                    self.logger.error(f\"数据库完整性错误：尝试插入已存在的文档 ID '{doc_id_str}'（可能是并发问题或检查逻辑遗漏）。将跳过此文档。\")\n",
    "                    # 之前检查过，理论上不应该到这里，但作为安全措施计数。\n",
    "                    skipped_duplicate_count += 1\n",
    "                    continue\n",
    "                except sqlite3.Error as db_e:\n",
    "                    self.logger.error(f\"数据库错误：在为文档 '{doc_id_str}' 插入元数据时发生错误: {db_e}。将跳过此文档的 Faiss 添加。\")\n",
    "                    db_insert_error_count += 1\n",
    "                    continue # 跳过 Faiss 添加，因为元数据插入失败\n",
    "\n",
    "                # --- 2d. 将成功编码的向量添加到对应的批处理列表中 ---\n",
    "                # 只有在编码成功 且 数据库插入成功 (获取到internal_id) 后才执行。\n",
    "                if internal_id is not None and encoded_data is not None:\n",
    "                    at_least_one_vector_added_for_doc = False # 跟踪该文档是否至少有一个向量被添加到批处理。\n",
    "                    if encoded_data.get('text_vector') is not None:\n",
    "                        text_vectors_batch.append(encoded_data['text_vector']) # type: ignore\n",
    "                        text_ids_batch.append(internal_id)\n",
    "                        at_least_one_vector_added_for_doc = True\n",
    "                        self.logger.debug(f\"  文本向量已为文档 '{doc_id_str}' (internal_id: {internal_id}) 准备好加入批处理。\")\n",
    "\n",
    "                    if encoded_data.get('image_vector') is not None:\n",
    "                        image_vectors_batch.append(encoded_data['image_vector']) # type: ignore\n",
    "                        image_ids_batch.append(internal_id)\n",
    "                        at_least_one_vector_added_for_doc = True\n",
    "                        self.logger.debug(f\"  图像向量已为文档 '{doc_id_str}' (internal_id: {internal_id}) 准备好加入批处理。\")\n",
    "\n",
    "                    if encoded_data.get('mean_vector') is not None:\n",
    "                        mean_vectors_batch.append(encoded_data['mean_vector']) # type: ignore\n",
    "                        mean_ids_batch.append(internal_id)\n",
    "                        at_least_one_vector_added_for_doc = True\n",
    "                        self.logger.debug(f\"  平均向量已为文档 '{doc_id_str}' (internal_id: {internal_id}) 准备好加入批处理。\")\n",
    "\n",
    "                    # 如果编码成功，数据库插入成功，但没有向量被添加到批处理（理论上不可能，因为encoding_succeeded保证了至少一个向量存在）\n",
    "                    # 这是一个内部逻辑检查。\n",
    "                    if not at_least_one_vector_added_for_doc:\n",
    "                        self.logger.error(f\"内部逻辑错误：文档 '{doc_id_str}' (internal_id: {internal_id}) 编码和数据库插入均成功，但没有向量被添加到批处理！\")\n",
    "                        # 这种情况不应该发生，但如果发生了，也算作处理失败的一种形式\n",
    "                        encoding_failure_count += 1 # 归类为编码相关问题\n",
    "                    else:\n",
    "                        processed_count += 1 # 只有编码成功、DB插入成功、向量准备好加入批处理，才算成功处理。\n",
    "\n",
    "            # --- 文档遍历和初步处理完成 ---\n",
    "            self.logger.info(f\"所有 {len(documents)} 个输入文档已遍历处理完毕。\")\n",
    "            self.logger.info(f\"准备将收集到的向量批量添加到 Faiss 索引中...\")\n",
    "            self.logger.info(f\"  - 待添加文本向量数量: {len(text_ids_batch)}\")\n",
    "            self.logger.info(f\"  - 待添加图像向量数量: {len(image_ids_batch)}\")\n",
    "            self.logger.info(f\"  - 待添加平均向量数量: {len(mean_ids_batch)}\")\n",
    "\n",
    "            # --- 步骤 3: 批量将向量和 ID 添加到对应的 Faiss 索引 ---\n",
    "            # 批量添加比逐个添加效率高得多。\n",
    "            faiss_add_errors = 0\n",
    "            try:\n",
    "                if text_vectors_batch:\n",
    "                    ids_np_text = np.array(text_ids_batch, dtype='int64') # Faiss IndexIDMap2 需要 int64 类型的 ID。\n",
    "                    vectors_np_text = np.array(text_vectors_batch, dtype='float32') # Faiss 通常使用 float32。\n",
    "                    self.text_index.add_with_ids(vectors_np_text, ids_np_text) # 将向量和对应的 ID 批量添加到文本索引。\n",
    "                    self.logger.info(f\"已成功向文本(Text) Faiss 索引批量添加 {len(text_vectors_batch)} 个向量。当前索引总数: {self.text_index.ntotal}\")\n",
    "            except Exception as faiss_e_text:\n",
    "                self.logger.error(f\"错误：向文本(Text) Faiss 索引批量添加向量时失败: {faiss_e_text}\", exc_info=True)\n",
    "                faiss_add_errors += 1\n",
    "\n",
    "            try:\n",
    "                if image_vectors_batch:\n",
    "                    ids_np_image = np.array(image_ids_batch, dtype='int64')\n",
    "                    vectors_np_image = np.array(image_vectors_batch, dtype='float32')\n",
    "                    self.image_index.add_with_ids(vectors_np_image, ids_np_image)\n",
    "                    self.logger.info(f\"已成功向图像(Image) Faiss 索引批量添加 {len(image_vectors_batch)} 个向量。当前索引总数: {self.image_index.ntotal}\")\n",
    "            except Exception as faiss_e_image:\n",
    "                self.logger.error(f\"错误：向图像(Image) Faiss 索引批量添加向量时失败: {faiss_e_image}\", exc_info=True)\n",
    "                faiss_add_errors += 1\n",
    "\n",
    "            try:\n",
    "                if mean_vectors_batch:\n",
    "                    ids_np_mean = np.array(mean_ids_batch, dtype='int64')\n",
    "                    vectors_np_mean = np.array(mean_vectors_batch, dtype='float32')\n",
    "                    self.mean_index.add_with_ids(vectors_np_mean, ids_np_mean)\n",
    "                    self.logger.info(f\"已成功向平均(Mean) Faiss 索引批量添加 {len(mean_vectors_batch)} 个向量。当前索引总数: {self.mean_index.ntotal}\")\n",
    "            except Exception as faiss_e_mean:\n",
    "                self.logger.error(f\"错误：向平均(Mean) Faiss 索引批量添加向量时失败: {faiss_e_mean}\", exc_info=True)\n",
    "                faiss_add_errors += 1\n",
    "\n",
    "            # --- 步骤 4: 提交数据库事务 ---\n",
    "            # 如果 Faiss 添加过程中出现任何错误，也应该提交数据库更改，因为元数据插入是在此之前完成的。\n",
    "            # 但需要记录 Faiss 添加失败的情况。\n",
    "            if conn:\n",
    "                conn.commit()\n",
    "                self.logger.info(\"数据库事务已成功提交。元数据更改已持久化。\")\n",
    "                if faiss_add_errors > 0:\n",
    "                    self.logger.warning(f\"警告：虽然数据库事务已提交，但在向 {faiss_add_errors} 个 Faiss 索引添加向量时发生了错误。数据库和 Faiss 索引可能存在不一致！\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.critical(f\"严重错误：在文档索引过程中发生意外的顶级异常: {e}\", exc_info=True)\n",
    "            # 如果在处理过程中发生任何未捕获的异常，回滚数据库事务是必要的，以避免数据库处于不一致状态。\n",
    "            if conn:\n",
    "                self.logger.warning(\"检测到严重错误，正在尝试回滚数据库事务以撤销本批次未提交的更改...\")\n",
    "                try:\n",
    "                    conn.rollback()\n",
    "                    self.logger.info(\"数据库事务已成功回滚。\")\n",
    "                except Exception as rb_e:\n",
    "                    self.logger.error(f\"错误：尝试回滚数据库事务时失败: {rb_e}\", exc_info=True)\n",
    "        finally:\n",
    "            # 无论发生什么，最后都要关闭数据库连接。\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                self.logger.debug(\"数据库连接已关闭。\")\n",
    "\n",
    "        # --- 打印索引过程的最终总结信息 ---\n",
    "        # 这是一个重要的报告，总结了本次索引操作的结果。\n",
    "        self.logger.info(f\"\\n--- 文档索引过程总结 ---\")\n",
    "        self.logger.info(f\"- 输入文档总数: {len(documents)}\")\n",
    "        self.logger.info(f\"- 因输入无效(缺少ID或内容)跳过的文档数: {skipped_invalid_input_count}\")\n",
    "        self.logger.info(f\"- 因 'doc_id' 在数据库中已存在而跳过的文档数: {skipped_duplicate_count}\")\n",
    "        self.logger.info(f\"- 因检查重复项时数据库错误跳过的文档数: {db_check_error_count}\")\n",
    "        self.logger.info(f\"- 因编码未能生成有效向量而跳过的文档数: {encoding_failure_count}\")\n",
    "        self.logger.info(f\"- 因数据库插入错误而跳过的文档数: {db_insert_error_count}\")\n",
    "        self.logger.info(f\"- 成功处理(编码成功+DB插入成功+准备添加到Faiss)的文档数: {processed_count}\")\n",
    "        self.logger.info(f\"- 向 Faiss 添加向量时发生错误的索引数量: {faiss_add_errors if 'faiss_add_errors' in locals() else 'N/A'}\") # Check if var exists\n",
    "\n",
    "        # 获取当前索引中的向量总数，使用 getattr 安全访问 ntotal 属性。\n",
    "        text_final_count = getattr(self.text_index, 'ntotal', 'N/A')\n",
    "        image_final_count = getattr(self.image_index, 'ntotal', 'N/A')\n",
    "        mean_final_count = getattr(self.mean_index, 'ntotal', 'N/A')\n",
    "        self.logger.info(f\"- 当前文本 Faiss 索引中的向量总数: {text_final_count}\")\n",
    "        self.logger.info(f\"- 当前图像 Faiss 索引中的向量总数: {image_final_count}\")\n",
    "        self.logger.info(f\"- 当前平均 Faiss 索引中的向量总数: {mean_final_count}\")\n",
    "\n",
    "        # 获取数据库中的文档总数，并与Faiss索引中的向量数进行比较，检查一致性。\n",
    "        db_final_count = self.get_document_count()\n",
    "        self.logger.info(f\"- 当前 SQLite 数据库中存储的文档元数据记录总数: {db_final_count}\")\n",
    "\n",
    "        # 进行一致性检查\n",
    "        if isinstance(text_final_count, int) and isinstance(image_final_count, int) and isinstance(mean_final_count, int):\n",
    "            max_faiss_vectors = max(text_final_count, image_final_count, mean_final_count)\n",
    "            # 理想情况下，成功处理的文档数 (processed_count) 应该等于添加到 Faiss 的向量数（对于存在对应向量类型的文档）\n",
    "            # 数据库的最终记录数 (db_final_count) 应该等于处理过程中成功插入数据库的总数。\n",
    "            # 注意：processed_count 是本轮成功处理的数量，而 db_final_count 和 Faiss count 是累积总量。\n",
    "            # 更准确的检查是比较本轮成功添加到 Faiss 的批次大小与 processed_count。\n",
    "\n",
    "            # 粗略检查：比较数据库总数和Faiss最大总数\n",
    "            if db_final_count > max_faiss_vectors and max_faiss_vectors >= 0: # Allow max_faiss_vectors == 0\n",
    "                 self.logger.warning(f\"数据一致性提示：数据库记录数 ({db_final_count}) 多于 Faiss 索引中的最大向量数 ({max_faiss_vectors})。\")\n",
    "                 self.logger.warning(f\"                 这可能是正常的（例如，部分文档只有文本没有图像，或反之），但也可能表示部分向量未能添加成功。请检查日志。\")\n",
    "            elif db_final_count < max_faiss_vectors:\n",
    "                 self.logger.error(f\"数据一致性错误：数据库记录数 ({db_final_count}) 少于某个 Faiss 索引中的最大向量数 ({max_faiss_vectors})。数据可能存在严重不一致！\")\n",
    "                 self.logger.error(f\"                 这可能意味着部分文档的向量被添加到了Faiss，但其元数据未能插入数据库！请检查错误日志。\")\n",
    "        else:\n",
    "            self.logger.warning(\"未能执行完整的数据库/Faiss数量一致性检查，因为无法获取所有索引的数值计数。\")\n",
    "\n",
    "        self.logger.info(f\"--- 文档索引过程结束 ---\")\n",
    "\n",
    "\n",
    "    def get_document_by_internal_id(self, internal_id: int) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        根据 Faiss 搜索返回的 `internal_id` (即数据库中的主键)，从 SQLite 数据库中检索对应的原始文档元数据。\n",
    "        这是从向量检索结果回溯到原始文档信息的必要步骤。\n",
    "\n",
    "        Args:\n",
    "            internal_id (int): 要查询的文档在数据库中的 `internal_id` (通常由 Faiss 搜索返回)。\n",
    "\n",
    "        Returns:\n",
    "            Optional[Dict[str, Any]]: 如果找到文档，则返回一个包含文档信息的字典。\n",
    "                            该字典通常包含 'id' (原始 doc_id), 'text', 'image_path', 和 'internal_id'。\n",
    "                            如果数据库中找不到具有该 `internal_id` 的记录，则返回 None。\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"尝试从数据库根据 internal_id '{internal_id}' 获取文档元数据...\")\n",
    "        try:\n",
    "            # 连接到 SQLite 数据库。\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                # 设置 conn.row_factory = sqlite3.Row 使得查询结果可以像字典一样通过列名访问，更方便。\n",
    "                conn.row_factory = sqlite3.Row\n",
    "                cursor = conn.cursor()\n",
    "                # 执行 SELECT 查询，根据 internal_id 查找记录。\n",
    "                cursor.execute(\n",
    "                    \"SELECT internal_id, doc_id, text, image_path FROM documents WHERE internal_id = ?\",\n",
    "                    (internal_id,) # 注意参数是一个元组。\n",
    "                )\n",
    "                row = cursor.fetchone() # 获取一行结果。\n",
    "\n",
    "                if row:\n",
    "                    # 如果找到了记录 (row 不为 None)。\n",
    "                    doc_data = dict(row) # 将 sqlite3.Row 对象转换为标准的 Python 字典。\n",
    "                    # 为了与外部接口或数据结构保持一致（例如，原始输入时的 'id'），\n",
    "                    # 将从数据库中取出的 'doc_id' 键重命名为 'id'。\n",
    "                    doc_data['id'] = doc_data.pop('doc_id')\n",
    "                    self.logger.debug(f\"成功为 internal_id '{internal_id}' 找到文档元数据: ID='{doc_data['id']}'\")\n",
    "                    return doc_data # 返回包含文档信息的字典。\n",
    "                else:\n",
    "                    # 如果没有找到具有该 internal_id 的记录。\n",
    "                    self.logger.warning(f\"未能在数据库中找到 internal_id 为 '{internal_id}' 的文档元数据。\")\n",
    "                    return None # 返回 None。\n",
    "        except sqlite3.Error as e_sql:\n",
    "             self.logger.error(f\"数据库错误：从数据库根据 internal_id '{internal_id}' 获取文档时发生错误: {e_sql}\", exc_info=True)\n",
    "             return None\n",
    "        except Exception as e_general:\n",
    "             self.logger.error(f\"未知错误：从数据库根据 internal_id '{internal_id}' 获取文档时发生: {e_general}\", exc_info=True)\n",
    "             return None\n",
    "\n",
    "    def get_documents_by_internal_ids(self, internal_ids: List[int]) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        根据一个 `internal_id` 的列表，从 SQLite 数据库中批量检索对应的多个文档的元数据。\n",
    "        使用批量查询 (SELECT ... WHERE internal_id IN (...)) 通常比多次单独查询更高效，\n",
    "        尤其是在处理 Faiss 返回的 Top-K 结果列表时。这是为了提高检索后获取元数据的效率。\n",
    "\n",
    "        Args:\n",
    "            internal_ids (List[int]): 一个包含多个数据库 `internal_id` 的整数列表。\n",
    "\n",
    "        Returns:\n",
    "            Dict[int, Dict[str, Any]]: 一个字典，其中键是 `internal_id`，值是对应的文档数据字典\n",
    "                             (通常包含 'id', 'text', 'image_path', 'internal_id')。\n",
    "                             如果列表中的某个 ID 在数据库中找不到，则结果字典中不会包含该 ID 的条目。\n",
    "                             如果输入的 `internal_ids` 列表为空，则返回一个空字典。\n",
    "        \"\"\"\n",
    "        # 如果输入的 ID 列表为空，直接返回空字典，无需查询数据库。\n",
    "        if not internal_ids:\n",
    "            self.logger.debug(\"请求批量获取文档，但提供的 internal_id 列表为空。返回空结果。\")\n",
    "            return {}\n",
    "\n",
    "        # 限制一次批量查询的ID数量，避免SQL语句过长导致的问题。\n",
    "        # SQLite 默认的 SQLITE_MAX_VARIABLE_NUMBER 是 999，但可以被编译时修改。\n",
    "        # 为保险起见，设置一个稍小的值，或者如果需要处理大量 ID，应实现分块查询逻辑。\n",
    "        max_ids_per_query = 900 # Use a slightly conservative limit\n",
    "        results: Dict[int, Dict[str, Any]] = {} # 初始化结果字典。\n",
    "\n",
    "        # 对 ID 进行分块处理，以防列表过长\n",
    "        for i in range(0, len(internal_ids), max_ids_per_query):\n",
    "            id_chunk = internal_ids[i:i + max_ids_per_query]\n",
    "            if not id_chunk: continue # Skip empty chunks (shouldn't happen with correct slicing)\n",
    "\n",
    "            self.logger.debug(f\"尝试从数据库根据 internal_id 列表块 (块大小 {len(id_chunk)}) 批量获取文档元数据...\")\n",
    "\n",
    "            try:\n",
    "                # 连接到 SQLite 数据库。\n",
    "                with sqlite3.connect(self.db_path) as conn:\n",
    "                    conn.row_factory = sqlite3.Row # 设置行工厂，方便处理查询结果。\n",
    "                    cursor = conn.cursor()\n",
    "\n",
    "                    # 构建 SQL 查询语句，使用 IN 操作符和参数占位符进行批量查询。\n",
    "                    # 1. 创建占位符字符串: \"(?, ?, ..., ?)\" - 每个 ID 对应一个 '?'。\n",
    "                    placeholders = ','.join('?' for _ in id_chunk)\n",
    "                    # 2. 构建完整的 SQL 查询语句。\n",
    "                    query = f\"SELECT internal_id, doc_id, text, image_path FROM documents WHERE internal_id IN ({placeholders})\"\n",
    "                    self.logger.debug(f\"执行批量查询SQL (块 {i // max_ids_per_query + 1}): {query[:100]}... (参数数量: {len(id_chunk)})\")\n",
    "\n",
    "                    # 执行查询，将 ID 列表块作为参数传递给 execute 方法。\n",
    "                    cursor.execute(query, id_chunk)\n",
    "                    rows = cursor.fetchall() # 获取所有匹配的行。\n",
    "                    self.logger.debug(f\"数据库批量查询块返回了 {len(rows)} 行记录。\")\n",
    "\n",
    "                    # 遍历查询结果。\n",
    "                    for row in rows:\n",
    "                        doc_data = dict(row) # 将 sqlite3.Row 转换为字典。\n",
    "                        doc_data['id'] = doc_data.pop('doc_id') # 重命名 'doc_id' 为 'id'。\n",
    "                        # 使用 internal_id 作为键，将文档数据存入结果字典。\n",
    "                        results[doc_data['internal_id']] = doc_data\n",
    "\n",
    "                    # 检查当前块中是否有ID未找到 (如果 len(rows) < len(id_chunk))，并记录警告。\n",
    "                    if len(rows) < len(id_chunk):\n",
    "                        found_ids_in_chunk_set = set(row['internal_id'] for row in rows) # Use set for efficiency\n",
    "                        missing_ids_in_chunk = [id_val for id_val in id_chunk if id_val not in found_ids_in_chunk_set]\n",
    "                        if missing_ids_in_chunk:\n",
    "                             self.logger.warning(f\"在批量获取文档块时，以下 internal_id 未在数据库中找到: {missing_ids_in_chunk}\")\n",
    "                             # Removed the second warning line to reduce noise, the implication is clear.\n",
    "\n",
    "            except sqlite3.Error as e_sql:\n",
    "                 self.logger.error(f\"数据库错误：从数据库根据 internal_id 列表块获取文档时发生: {e_sql}\", exc_info=True)\n",
    "                 # 返回当前已收集的结果，并可能跳过后续块（或可以设计为继续尝试其他块）。\n",
    "                 # 这里选择继续处理下一个块，但错误已被记录。\n",
    "            except Exception as e_general:\n",
    "                self.logger.error(f\"未知错误：从数据库根据 internal_id 列表块获取文档时发生: {e_general}\", exc_info=True)\n",
    "                # 同上，记录错误并尝试继续。\n",
    "\n",
    "        self.logger.debug(f\"批量获取文档元数据完成，共处理 {len(internal_ids)} 个请求 ID，返回 {len(results)} 个文档的信息。\")\n",
    "        # 最后再检查一次总数是否匹配，以防分块逻辑隐藏问题。\n",
    "        if len(results) < len(internal_ids):\n",
    "             all_requested_ids_set = set(internal_ids)\n",
    "             all_found_ids_set = set(results.keys())\n",
    "             final_missing_ids = list(all_requested_ids_set - all_found_ids_set)\n",
    "             if final_missing_ids:\n",
    "                  self.logger.warning(f\"最终检查：在所有请求的 internal_id 中，以下 ID 未在数据库中找到: {final_missing_ids[:20]}{'...' if len(final_missing_ids)>20 else ''} (总计缺失 {len(final_missing_ids)} 个)\")\n",
    "                  self.logger.warning(\"  这可能表示 Faiss 索引与数据库元数据之间存在不一致。请检查日志和数据源。\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_document_count(self) -> int:\n",
    "         \"\"\"\n",
    "         获取当前 SQLite 数据库 'documents' 表中存储的文档总数量。\n",
    "         这是一个简单的计数功能，但对于监控知识库大小非常有用。\n",
    "\n",
    "         Returns:\n",
    "             int: 数据库中 'documents' 表的总行数。如果发生错误，则返回 0。\n",
    "         \"\"\"\n",
    "         self.logger.debug(f\"开始从数据库 '{self.db_path}' 获取文档总数...\")\n",
    "         try:\n",
    "             # 连接到 SQLite 数据库。\n",
    "             with sqlite3.connect(self.db_path) as conn:\n",
    "                 cursor = conn.cursor()\n",
    "                 # 执行 COUNT(*) 查询获取总行数。\n",
    "                 cursor.execute(\"SELECT COUNT(*) FROM documents\")\n",
    "                 # fetchone() 返回一个包含单个值的元组，例如 (50,)。\n",
    "                 count_result = cursor.fetchone()\n",
    "                 # 提取元组中的计数值。如果查询无结果（理论上 COUNT(*) 总有结果，但做个健壮性检查），则默认为 0。\n",
    "                 count = count_result[0] if count_result and count_result[0] is not None else 0\n",
    "                 self.logger.debug(f\"数据库中文档总数为: {count}\")\n",
    "                 return count\n",
    "         except sqlite3.Error as e_sql:\n",
    "              self.logger.error(f\"数据库错误：从数据库获取文档总数时发生错误: {e_sql}\", exc_info=True)\n",
    "              return 0\n",
    "         except Exception as e_general:\n",
    "              self.logger.error(f\"未知错误：从数据库获取文档总数时发生: {e_general}\", exc_info=True)\n",
    "              return 0\n",
    "\n",
    "    def save_indices(self):\n",
    "        \"\"\"\n",
    "        将内存中的所有三个 Faiss 索引（文本、图像、平均）分别保存到它们对应的文件路径中。\n",
    "        这个方法用于持久化索引的状态，以便在下次程序启动时可以加载这些索引，从而避免重新处理和编码所有文档。\n",
    "        只有当索引非空（即包含至少一个向量）时，才会执行保存操作。确保辛苦构建的索引不会丢失。\n",
    "        \"\"\"\n",
    "        self.logger.info(\"开始尝试将所有 Faiss 索引保存到磁盘文件...\")\n",
    "        # 调用内部辅助方法 `_save_single_index` 分别保存每个索引。\n",
    "        # 传递索引对象、目标文件路径和索引类型描述（用于日志）。\n",
    "        if hasattr(self, 'text_index'): # 检查对象是否存在且有效\n",
    "             self._save_single_index(self.text_index, self.faiss_text_index_path, \"文本(Text)\")\n",
    "        else:\n",
    "            self.logger.warning(\"文本(Text)索引对象不存在，无法保存。\")\n",
    "\n",
    "        if hasattr(self, 'image_index'):\n",
    "            self._save_single_index(self.image_index, self.faiss_image_index_path, \"图像(Image)\")\n",
    "        else:\n",
    "            self.logger.warning(\"图像(Image)索引对象不存在，无法保存。\")\n",
    "\n",
    "        if hasattr(self, 'mean_index'):\n",
    "            self._save_single_index(self.mean_index, self.faiss_mean_index_path, \"平均(Mean)\")\n",
    "        else:\n",
    "            self.logger.warning(\"平均(Mean)索引对象不存在，无法保存。\")\n",
    "\n",
    "        self.logger.info(\"所有 Faiss 索引的保存操作已完成（或已跳过空索引/不存在的索引）。\")\n",
    "\n",
    "    def _save_single_index(self, index: Optional[faiss.Index], index_path: str, index_type_description: str):\n",
    "        \"\"\"\n",
    "        辅助方法：保存单个 Faiss 索引到指定的文件路径。\n",
    "        仅当索引对象有效且包含至少一个向量时才执行保存。\n",
    "        此方法还会确保索引文件要保存到的目录存在。每一个细节都不能忽略。\n",
    "\n",
    "        Args:\n",
    "            index (Optional[faiss.Index]): 需要保存的 Faiss 索引对象。可能是 None。\n",
    "            index_path (str): 保存索引的目标文件完整路径。\n",
    "            index_type_description (str): 索引类型的描述性名称 (例如 \"文本\", \"图像\")，用于日志记录。\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"准备保存 '{index_type_description}' Faiss 索引到路径: '{index_path}'...\")\n",
    "\n",
    "        if index is None:\n",
    "            self.logger.warning(f\"  警告：'{index_type_description}' Faiss 索引对象为 None，无法执行保存操作。\")\n",
    "            return\n",
    "\n",
    "        # 检查索引对象是否有效（存在 `ntotal` 属性，表示向量数量）以及向量数量是否大于 0。只有非空索引才有保存的价值。\n",
    "        if hasattr(index, 'ntotal') and index.ntotal > 0:\n",
    "            try:\n",
    "                # 确保索引文件要保存到的目录存在，如果不存在则创建它。\n",
    "                index_directory = os.path.dirname(index_path)\n",
    "                if index_directory and not os.path.exists(index_directory):\n",
    "                    os.makedirs(index_directory, exist_ok=True)\n",
    "                    self.logger.debug(f\"  已确保 '{index_type_description}' 索引的保存目录 '{index_directory}' 存在 (或已创建)。\")\n",
    "\n",
    "                # 使用 faiss.write_index 函数将内存中的索引对象写入到指定的磁盘文件。\n",
    "                faiss.write_index(index, index_path)\n",
    "                self.logger.info(f\"  成功：'{index_type_description}' Faiss 索引 (包含 {index.ntotal} 个向量) 已保存到: {index_path}\")\n",
    "            except Exception as e:\n",
    "                # 处理在保存索引过程中可能发生的错误 (例如，磁盘空间不足、文件写入权限问题)。\n",
    "                self.logger.error(f\"  错误：保存 '{index_type_description}' Faiss 索引到 '{index_path}' 失败。错误详情: {e}\", exc_info=True)\n",
    "        elif hasattr(index, 'ntotal'): # 索引存在但为空 (ntotal == 0)\n",
    "             self.logger.info(f\"  跳过：'{index_type_description}' Faiss 索引为空 (ntotal={index.ntotal})，因此不保存到 '{index_path}'。\")\n",
    "        else: # 索引对象无效或未正确初始化\n",
    "             self.logger.warning(f\"  警告：'{index_type_description}' Faiss 索引似乎未正确初始化 (缺少 ntotal 属性)，无法执行保存操作。\")\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        关闭 Indexer 实例时调用的清理方法。\n",
    "        主要职责是确保所有内存中的 Faiss 索引都已尝试保存到磁盘。\n",
    "        SQLite 数据库连接是通过 `with sqlite3.connect(...)` 语句管理的，在每个相关方法结束时会自动关闭，\n",
    "        因此这里不需要显式关闭数据库连接。\n",
    "        Faiss 索引对象本身在 Python 中是内存对象，它们不需要像文件句柄那样显式关闭；保存它们的状态即是“关闭”操作。\n",
    "        这是一个负责任的结束流程，确保资源得到妥善处理。\n",
    "        \"\"\"\n",
    "        self.logger.info(\"开始关闭 Indexer 实例...\")\n",
    "        # 调用 save_indices 方法，确保存储所有 Faiss 索引的最新状态。\n",
    "        self.save_indices()\n",
    "        self.logger.info(\"Indexer 实例关闭完成。所有 Faiss 索引已尝试保存。\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 检索器类 (Retriever)\n",
    "# Retriever 是系统的“搜索大脑”，它根据用户的查询在知识库中找到最相关的文档。\n",
    "# 它的效率和准确性是提供良好RAG体验的关键。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "class Retriever:\n",
    "    \"\"\"\n",
    "    Retriever 类负责处理用户的查询（可以是文本、图像路径或两者结合的多模态查询），\n",
    "    并从已建立的索引中检索最相关的文档。其工作流程如下：\n",
    "\n",
    "    1.  **接收查询**: 用户通过 `retrieve` 方法提交查询。\n",
    "    2.  **查询编码**: 利用与 `Indexer` 中相同的 `MultimodalEncoder` 实例对用户查询进行向量化，\n",
    "        将其转换为与索引文档相同向量空间中的特征向量（可能包括文本向量、图像向量和/或平均向量）。保持编码器一致性是保证向量空间对齐的基础。\n",
    "    3.  **选择策略**: 根据查询的类型（纯文本、纯图像、多模态）和可用性，\n",
    "        选择最合适的 Faiss 索引（文本索引、图像索引或平均向量索引）以及对应的查询向量进行搜索。\n",
    "        例如，纯文本查询将使用文本向量在文本索引中搜索。如果首选索引不可用，会尝试回退到其他可用的策略。\n",
    "    4.  **相似度搜索**: 在选定的 Faiss 索引中执行 Top-K 相似度搜索，找出与查询向量最相似的 K 个向量。\n",
    "        搜索结果是这些向量的 `internal_id` (与数据库主键对应) 和它们与查询向量的相似度得分。\n",
    "    5.  **获取元数据**: 使用检索到的 `internal_id` 列表，通过 `Indexer` 的接口从 SQLite 数据库中批量获取\n",
    "        这些最相关文档的完整元数据（如原始ID、文本内容、图像路径等）。这是将向量结果转化为有用信息的过程。\n",
    "    6.  **结果组合与返回**: 将获取到的文档元数据与它们各自的相似度得分结合起来，\n",
    "        并按照相似度得分从高到低（表示最相关）排序，最终返回一个包含这些信息的文档列表。\n",
    "\n",
    "    Retriever 依赖于一个已经初始化并填充了数据和索引的 `Indexer` 实例。\n",
    "    它复用 `Indexer` 的编码器以保证查询和文档编码的一致性，并访问 `Indexer` 中的 Faiss 索引和数据库。\n",
    "    我必须确保它能准确、高效地从 Indexer 获取信息。\n",
    "    \"\"\"\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        \"\"\"\n",
    "        初始化 Retriever 实例。我必须确保它能正确地连接到并使用 Indexer 提供的资源。\n",
    "\n",
    "        Args:\n",
    "            indexer (Indexer): 一个已经初始化并包含了数据和索引的 `Indexer` 类的实例。\n",
    "                               Retriever 的所有操作都依赖于这个 `Indexer` 实例提供的资源。\n",
    "\n",
    "        Raises:\n",
    "            ValueError: 如果传入的 `indexer` 不是 `Indexer` 类的有效实例，或者该实例似乎缺少\n",
    "                        必要的 Faiss 索引属性 (text_index, image_index, mean_index) 或编码器，则抛出此异常。\n",
    "                        一个没有有效索引源的 Retriever 是无法工作的。\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__ + \".\" + self.__class__.__name__)\n",
    "        self.logger.info(\"开始初始化 Retriever...\")\n",
    "\n",
    "        # 验证传入的 indexer 参数的有效性。Retriever 的工作完全依赖于 Indexer，所以这个检查非常重要。\n",
    "        if not isinstance(indexer, Indexer):\n",
    "             msg = \"Retriever 初始化错误: 需要一个有效的 Indexer 实例，但收到的不是预期的类型。\"\n",
    "             self.logger.critical(msg)\n",
    "             raise ValueError(msg)\n",
    "\n",
    "        # 进一步验证 Indexer 实例是否已成功创建了所需的 Faiss 索引对象和编码器。\n",
    "        # 缺少这些核心组件意味着 Indexer 初始化失败，Retriever 也无法工作。\n",
    "        required_indices_attributes = ['text_index', 'image_index', 'mean_index', 'encoder', 'vector_dimension']\n",
    "        missing_attrs = [attr for attr in required_indices_attributes if not hasattr(indexer, attr) or getattr(indexer, attr) is None]\n",
    "        if missing_attrs:\n",
    "            msg = f\"Retriever 初始化错误: 提供的 Indexer 实例缺少以下必需的属性: {', '.join(missing_attrs)}。请确保 Indexer 已成功初始化。\"\n",
    "            self.logger.critical(msg)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        # 保存对传入的 Indexer 实例的引用。Retriever 将通过这个引用访问 Indexer 的资源。\n",
    "        self.indexer: Indexer = indexer\n",
    "        # 复用 Indexer 内部的 Encoder 实例。这是确保查询向量和文档向量在同一空间的关键。\n",
    "        self.encoder: MultimodalEncoder = self.indexer.encoder\n",
    "        # 从 Indexer 获取向量维度。\n",
    "        self.vector_dimension: int = self.indexer.vector_dimension\n",
    "        self.logger.info(f\"  Retriever 将使用 Indexer 的编码器 (向量维度: {self.vector_dimension})。\")\n",
    "\n",
    "        # 获取对 Indexer 中三个 Faiss 索引的直接引用。\n",
    "        self.text_index: faiss.Index = self.indexer.text_index\n",
    "        self.image_index: faiss.Index = self.indexer.image_index\n",
    "        self.mean_index: faiss.Index = self.indexer.mean_index\n",
    "\n",
    "        # 检查所有关联的 Faiss 索引是否都为空。如果索引为空，Retriever 就无法找到任何结果。\n",
    "        text_index_ntotal = getattr(self.text_index, 'ntotal', 0)\n",
    "        image_index_ntotal = getattr(self.image_index, 'ntotal', 0)\n",
    "        mean_index_ntotal = getattr(self.mean_index, 'ntotal', 0)\n",
    "\n",
    "        if text_index_ntotal == 0 and image_index_ntotal == 0 and mean_index_ntotal == 0:\n",
    "             self.logger.warning(\"Retriever 初始化警告: Indexer 中的所有 Faiss 索引当前都为空。\")\n",
    "             self.logger.warning(\"  这意味着任何检索操作都将无法找到任何匹配的文档。\")\n",
    "             self.logger.warning(\"  请确保 Indexer 已成功索引了数据，或者检查索引建立过程的日志。\")\n",
    "        else:\n",
    "             self.logger.info(f\"Retriever 初始化成功。关联的 Indexer 状态如下:\")\n",
    "             self.logger.info(f\"    - 文本(Text)索引中向量数: {text_index_ntotal}\")\n",
    "             self.logger.info(f\"    - 图像(Image)索引中向量数: {image_index_ntotal}\")\n",
    "             self.logger.info(f\"    - 平均(Mean)索引中向量数: {mean_index_ntotal}\")\n",
    "        self.logger.info(\"Retriever 初始化完成。我已经准备好根据您的查询进行搜索了。\")\n",
    "\n",
    "\n",
    "    def retrieve(self, query: Union[str, Dict[str, str]], k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        执行完整的检索流程：接收用户查询 -> 对查询进行编码 -> 根据查询类型选择合适的索引和查询向量 ->\n",
    "        在选定的 Faiss 索引中搜索相似向量 -> 获取这些向量对应的原始文档元数据 -> 组合信息并返回结果。\n",
    "        这是 Retriever 的核心功能实现。\n",
    "\n",
    "        Args:\n",
    "            query (Union[str, Dict[str, str]]): 用户提交的查询。可以是：\n",
    "                - str: 一个纯文本查询字符串。\n",
    "                - Dict: 一个字典，用于表示更复杂的查询类型：\n",
    "                    - {'text': '文本内容', 'image_path': '图像路径'} : 多模态查询，结合文本和图像信息。\n",
    "                    - {'image_path': '图像路径'} : 纯图像查询，仅使用图像内容进行检索。\n",
    "                    - {'text': '文本内容'} : 纯文本查询 (与直接传入字符串的效果相同，但通过字典形式提供)。\n",
    "                    字典中必须至少包含 'text' 或 'image_path' 键及其对应值，且值不能为空字符串。\n",
    "            k (int): 指定希望检索的最相似文档的数量 (Top-K)。默认为 5。\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: 一个按相似度得分降序排列的文档列表。\n",
    "                        列表中的每个字典代表一个检索到的文档，包含以下键（但不限于）：\n",
    "                        - 'id': 原始文档 ID (str)。\n",
    "                        - 'text': 文档的文本内容 (str 或 None)。\n",
    "                        - 'image_path': 关联图像的路径 (str 或 None)。\n",
    "                        - 'internal_id': 数据库和 Faiss 使用的内部 ID (int)。\n",
    "                        - 'score': 该文档与查询的相似度得分 (float)。对于内积搜索，得分越高表示越相似。\n",
    "                        如果查询无效、编码失败、所选索引为空或搜索无结果，则返回空列表 `[]`。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"开始执行检索流程，目标是获取 Top-{k} 最相关的文档...\")\n",
    "        # 为了日志清晰，截断长查询字符串。\n",
    "        query_str_for_log = str(query)\n",
    "        self.logger.debug(f\"  接收到的原始查询: {query_str_for_log[:200]}{'...' if len(query_str_for_log)>200 else ''}, k={k}\")\n",
    "\n",
    "        query_text: Optional[str] = None\n",
    "        query_image_path: Optional[str] = None\n",
    "        query_type: str = \"unknown\"\n",
    "\n",
    "        # --- 步骤 1: 解析查询输入，确定查询类型和具体内容 ---\n",
    "        self.logger.debug(\"  - Retriever步骤 1: 解析用户查询输入...\")\n",
    "        if isinstance(query, str):\n",
    "            query_text_stripped = query.strip()\n",
    "            if query_text_stripped:\n",
    "                query_text = query_text_stripped\n",
    "                query_type = \"纯文本\"\n",
    "                self.logger.info(f\"    查询类型确定为: {query_type} (字符串输入)\")\n",
    "                self.logger.info(f\"    查询文本内容: '{query_text[:100]}{'...' if len(query_text)>100 else ''}'\")\n",
    "            else:\n",
    "                self.logger.error(\"查询错误: 纯文本查询字符串为空或只包含空白。\")\n",
    "                return []\n",
    "        elif isinstance(query, dict):\n",
    "            # 从字典中安全地获取文本和图像路径。\n",
    "            query_text_from_dict = query.get('text')\n",
    "            query_image_path_from_dict = query.get('image_path')\n",
    "\n",
    "            # 确保获取到的值是字符串，且去除空白后非空。\n",
    "            query_text = query_text_from_dict.strip() if isinstance(query_text_from_dict, str) and query_text_from_dict.strip() else None\n",
    "            query_image_path = query_image_path_from_dict.strip() if isinstance(query_image_path_from_dict, str) and query_image_path_from_dict.strip() else None\n",
    "\n",
    "            # 根据有效的输入组合确定查询类型。\n",
    "            if query_text and query_image_path:\n",
    "                query_type = \"多模态\"\n",
    "                self.logger.info(f\"    查询类型确定为: {query_type}\")\n",
    "                self.logger.info(f\"    查询文本部分: '{query_text[:50]}{'...' if len(query_text)>50 else ''}'\")\n",
    "                self.logger.info(f\"    查询图像部分: '{os.path.basename(query_image_path)}'\")\n",
    "            elif query_image_path:\n",
    "                # 检查图像文件是否存在\n",
    "                if os.path.exists(query_image_path) and os.path.isfile(query_image_path):\n",
    "                    query_type = \"纯图像\"\n",
    "                    self.logger.info(f\"    查询类型确定为: {query_type}\")\n",
    "                    self.logger.info(f\"    查询图像路径: '{os.path.basename(query_image_path)}' (文件存在)\")\n",
    "                else:\n",
    "                    self.logger.error(f\"查询错误: 纯图像查询指定的图像文件路径无效或不存在: '{query_image_path}'\")\n",
    "                    return []\n",
    "            elif query_text:\n",
    "                query_type = \"纯文本\"\n",
    "                self.logger.info(f\"    查询类型确定为: {query_type} (字典输入)\")\n",
    "                self.logger.info(f\"    查询文本内容: '{query_text[:100]}{'...' if len(query_text)>100 else ''}'\")\n",
    "            else:\n",
    "                self.logger.error(\"查询错误: 查询字典无效，必须至少包含有效的非空 'text' 或有效的非空 'image_path' 键及其对应值。\")\n",
    "                return []\n",
    "        else:\n",
    "            self.logger.error(f\"查询错误: 不支持的查询类型 ({type(query)}) 或查询内容为空。查询必须是有效的非空字符串或包含有效内容的字典。\")\n",
    "            return []\n",
    "\n",
    "        # --- 步骤 2: 使用内部的 MultimodalEncoder 对查询进行编码 ---\n",
    "        self.logger.debug(f\"  - Retriever步骤 2: 使用 MultimodalEncoder 对 '{query_type}' 查询进行编码...\")\n",
    "        encoded_query_vectors: Dict[str, Optional[np.ndarray]]\n",
    "        try:\n",
    "            # 调用 Indexer 的编码器对查询进行编码。\n",
    "            encoded_query_vectors = self.encoder.encode(text=query_text, image_path=query_image_path)\n",
    "            query_text_vec = encoded_query_vectors.get('text_vector')\n",
    "            query_image_vec = encoded_query_vectors.get('image_vector')\n",
    "            query_mean_vec = encoded_query_vectors.get('mean_vector')\n",
    "\n",
    "            # 如果编码器未能生成任何向量，则无法进行检索。\n",
    "            if query_text_vec is None and query_image_vec is None and query_mean_vec is None:\n",
    "                 self.logger.warning(\"查询编码警告: MultimodalEncoder 未能为当前查询生成任何有效的特征向量。无法继续检索。\")\n",
    "                 return []\n",
    "            self.logger.info(\"    查询编码完成。\")\n",
    "            if query_text_vec is not None: self.logger.debug(\"      - 生成了文本查询向量。\")\n",
    "            if query_image_vec is not None: self.logger.debug(\"      - 生成了图像查询向量。\")\n",
    "            if query_mean_vec is not None: self.logger.debug(\"      - 生成了平均查询向量。\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"查询编码严重错误: 在对查询进行编码时发生意外错误: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "        # --- 步骤 3: 根据查询类型选择目标 Faiss 索引和相应的查询向量 ---\n",
    "        # 这是决定使用哪个索引进行搜索的逻辑。优先级通常是 多模态 -> 文本 -> 图像。\n",
    "        self.logger.debug(f\"  - Retriever步骤 3: 根据查询类型 '{query_type}' 选择搜索策略 (Faiss索引和查询向量)...\")\n",
    "        target_faiss_index: Optional[faiss.Index] = None   # 最终选定的 Faiss 索引对象。\n",
    "        search_query_vector: Optional[np.ndarray] = None   # 最终用于搜索的查询向量。\n",
    "        selected_index_name: str = \"N/A\"                   # 用于日志记录的索引名称。\n",
    "\n",
    "        # 获取当前索引中的向量数量，用于判断索引是否可用。\n",
    "        text_index_ntotal = getattr(self.text_index, 'ntotal', 0)\n",
    "        image_index_ntotal = getattr(self.image_index, 'ntotal', 0)\n",
    "        mean_index_ntotal = getattr(self.mean_index, 'ntotal', 0)\n",
    "\n",
    "        if query_type == \"纯文本\":\n",
    "            # 纯文本查询优先使用文本向量和文本索引。\n",
    "            if query_text_vec is not None and text_index_ntotal > 0:\n",
    "                target_faiss_index = self.text_index\n",
    "                search_query_vector = query_text_vec\n",
    "                selected_index_name = \"文本(Text)索引\"\n",
    "                self.logger.info(f\"    搜索策略: 使用文本查询向量，在 {selected_index_name} (含 {text_index_ntotal} 个向量) 中搜索。\")\n",
    "            else:\n",
    "                 reason = \"文本查询向量编码失败\" if query_text_vec is None else f\"文本(Text) Faiss 索引为空 (仅含 {text_index_ntotal} 个向量)\"\n",
    "                 self.logger.warning(f\"无法执行纯文本查询，因为: {reason}。\")\n",
    "                 return []\n",
    "        elif query_type == \"纯图像\":\n",
    "            # 纯图像查询优先使用图像向量和图像索引。\n",
    "            if query_image_vec is not None and image_index_ntotal > 0:\n",
    "                target_faiss_index = self.image_index\n",
    "                search_query_vector = query_image_vec\n",
    "                selected_index_name = \"图像(Image)索引\"\n",
    "                self.logger.info(f\"    搜索策略: 使用图像查询向量，在 {selected_index_name} (含 {image_index_ntotal} 个向量) 中搜索。\")\n",
    "            else:\n",
    "                reason = \"图像查询向量编码失败\" if query_image_vec is None else f\"图像(Image) Faiss 索引为空 (仅含 {image_index_ntotal} 个向量)\"\n",
    "                self.logger.warning(f\"无法执行纯图像查询，因为: {reason}。\")\n",
    "                return []\n",
    "        elif query_type == \"多模态\":\n",
    "            # 多模态查询优先使用平均向量和平均索引。\n",
    "            if query_mean_vec is not None and mean_index_ntotal > 0:\n",
    "                target_faiss_index = self.mean_index\n",
    "                search_query_vector = query_mean_vec\n",
    "                selected_index_name = \"平均(Mean)索引\"\n",
    "                self.logger.info(f\"    搜索策略: 使用平均查询向量，在 {selected_index_name} (含 {mean_index_ntotal} 个向量) 中搜索。\")\n",
    "            # 如果平均向量或平均索引不可用，尝试回退到使用文本向量在文本索引中搜索。\n",
    "            elif query_text_vec is not None and text_index_ntotal > 0:\n",
    "                 self.logger.warning(\"多模态查询警告: 平均(Mean)索引或平均查询向量不可用/索引为空。\")\n",
    "                 self.logger.info(f\"    应用回退策略: 改为使用文本查询向量，在文本(Text)索引 (含 {text_index_ntotal} 个向量) 中搜索。\")\n",
    "                 target_faiss_index = self.text_index\n",
    "                 search_query_vector = query_text_vec\n",
    "                 selected_index_name = \"文本(Text)索引 (作为多模态查询的回退)\"\n",
    "            # 如果所有首选和回退策略都不可用。\n",
    "            else:\n",
    "                reason_parts = []\n",
    "                if query_mean_vec is None: reason_parts.append(\"平均查询向量编码失败\")\n",
    "                if query_text_vec is None: reason_parts.append(\"文本查询向量编码失败\")\n",
    "                if mean_index_ntotal == 0: reason_parts.append(f\"平均(Mean)索引为空 (含{mean_index_ntotal}向量)\")\n",
    "                if text_index_ntotal == 0: reason_parts.append(f\"文本(Text)索引为空 (含{text_index_ntotal}向量)\")\n",
    "\n",
    "                final_reason = \"; \".join(reason_parts) if reason_parts else \"由于所有可能的搜索策略（平均、文本回退）都不可用或对应的查询向量缺失\"\n",
    "\n",
    "                self.logger.warning(f\"无法执行多模态查询，因为: {final_reason}。\")\n",
    "                return []\n",
    "        else:\n",
    "             # 如果查询类型是未知的，这是一个内部逻辑问题。\n",
    "             self.logger.error(\"内部逻辑错误: 无法为当前查询确定有效的查询类型或找不到可用的查询向量/索引组合。无法继续搜索。\")\n",
    "             return []\n",
    "\n",
    "        # 最终确认是否成功选择了搜索目标。\n",
    "        if target_faiss_index is None or search_query_vector is None:\n",
    "            self.logger.error(\"内部错误: 搜索目标 Faiss 索引或查询向量未能正确设置，尽管已尝试选择策略。无法继续搜索。\")\n",
    "            return []\n",
    "\n",
    "\n",
    "        # --- 步骤 4: 在选定的 Faiss 索引中执行 Top-K 相似度搜索 ---\n",
    "        # 这是使用 Faiss 核心功能的步骤。\n",
    "        self.logger.debug(f\"  - Retriever步骤 4: 在选定的 '{selected_index_name}' 中执行 Faiss Top-{k} 搜索...\")\n",
    "        try:\n",
    "            # Faiss search 方法期望输入的是一个二维数组 (batch_size, vector_dimension)。\n",
    "            # 我们的查询向量是单个向量，所以需要将其 reshape 成 (1, vector_dimension)。\n",
    "            # 确保向量是 float32 类型，Faiss 通常需要这个类型。\n",
    "            query_vector_for_faiss = search_query_vector.astype('float32').reshape(1, self.vector_dimension)\n",
    "\n",
    "            self.logger.debug(f\"    Faiss search: k={k}, query_vector_shape={query_vector_for_faiss.shape}\")\n",
    "            # 执行 Faiss 搜索。它返回距离/得分矩阵和对应的 ID 矩阵。\n",
    "            scores_matrix, internal_ids_matrix = target_faiss_index.search(query_vector_for_faiss, k)\n",
    "            self.logger.debug(f\"    Faiss search returned scores_matrix shape: {scores_matrix.shape}, ids_matrix shape: {internal_ids_matrix.shape}\")\n",
    "\n",
    "            retrieved_internal_ids: List[int] = []\n",
    "            retrieved_scores: List[float] = []\n",
    "\n",
    "            # 遍历搜索结果。Faiss 返回的 ID 矩阵可能包含 -1，表示未找到足够多的结果或填充。\n",
    "            # 我们只收集有效的 ID (不等于 -1)。\n",
    "            if internal_ids_matrix.size > 0 and scores_matrix.size > 0: # Check if results are not empty\n",
    "                for id_val, score_val in zip(internal_ids_matrix[0], scores_matrix[0]):\n",
    "                    if id_val != -1:\n",
    "                        retrieved_internal_ids.append(int(id_val))   # 确保 ID 是整数类型。\n",
    "                        retrieved_scores.append(float(score_val)) # 确保得分是浮点数类型。\n",
    "                    else:\n",
    "                        # Faiss often pads with -1 when fewer than k results are found.\n",
    "                        self.logger.debug(f\"    Faiss search: Encountered -1 ID, indicating fewer than k={k} results or padding. Stopping collection for this query.\")\n",
    "                        break # Stop collecting if -1 is encountered\n",
    "            else:\n",
    "                 self.logger.debug(\"    Faiss search: Returned empty ID or score matrices.\")\n",
    "\n",
    "\n",
    "            if not retrieved_internal_ids:\n",
    "                self.logger.info(f\"    Faiss 搜索在 '{selected_index_name}' 中完成，但未返回任何有效的结果 ID。可能该索引为空或没有相似度足够高的向量。\")\n",
    "                return []\n",
    "\n",
    "            self.logger.info(f\"    Faiss 搜索在 '{selected_index_name}' 中完成，初步找到 {len(retrieved_internal_ids)} 个候选文档的 internal_id。\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Faiss 搜索错误: 在 '{selected_index_name}' 中执行 Faiss 搜索时发生错误: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "        # --- 步骤 5: 根据检索到的 internal_ids 从 SQLite 数据库批量获取这些文档的完整元数据 ---\n",
    "        # 有了 internal_id，我们需要从数据库获取原始的文本、图像路径等信息。\n",
    "        self.logger.debug(f\"  - Retriever步骤 5: 使用找到的 {len(retrieved_internal_ids)} 个 internal_id，从 SQLite 数据库批量获取文档元数据...\")\n",
    "        documents_map_from_db: Dict[int, Dict[str, Any]]\n",
    "        if retrieved_internal_ids:\n",
    "             documents_map_from_db = self.indexer.get_documents_by_internal_ids(retrieved_internal_ids)\n",
    "             self.logger.info(f\"    已成功从数据库中获取了 {len(documents_map_from_db)} 条与 internal_id 对应的文档记录。\")\n",
    "        else:\n",
    "             # 如果 Faiss 没有返回有效 ID，则无需查询数据库。\n",
    "             self.logger.info(\"    由于 Faiss 未返回有效 ID，跳过数据库元数据获取步骤。\")\n",
    "             documents_map_from_db = {}\n",
    "\n",
    "\n",
    "        # --- 步骤 6: 组合结果：将元数据与相似度得分结合，并保持 Faiss 返回的原始排序 ---\n",
    "        # Faiss 返回的结果已经是按相似度排序的，我们只需将元数据和得分结合起来。\n",
    "        self.logger.debug(f\"  - Retriever步骤 6: 组合文档元数据与相似度得分，并按 Faiss 原始顺序排列...\")\n",
    "        final_retrieved_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "        # 遍历 Faiss 返回的 internal_id 和 score 列表（它们是同步排序的）。\n",
    "        for internal_id, score in zip(retrieved_internal_ids, retrieved_scores):\n",
    "            # 从数据库查询结果字典中查找对应的文档元数据。\n",
    "            doc_data_from_db = documents_map_from_db.get(internal_id)\n",
    "\n",
    "            if doc_data_from_db:\n",
    "                # 如果找到了元数据，将相似度得分添加到文档字典中。\n",
    "                doc_data_from_db['score'] = score\n",
    "                final_retrieved_docs.append(doc_data_from_db)\n",
    "            else:\n",
    "                # 如果 Faiss 返回的 ID 在数据库中找不到，说明存在数据不一致。这是一个警告，需要记录。\n",
    "                self.logger.warning(f\"数据不一致警告: 在数据库中未能找到 Faiss 返回的 internal_id: {internal_id}。\")\n",
    "                self.logger.warning(f\"                 这可能表示 Faiss 索引与数据库元数据之间存在不一致。将跳过此条检索结果。\")\n",
    "\n",
    "        self.logger.info(f\"检索流程成功完成，最终返回 {len(final_retrieved_docs)} 个文档（已按相似度排序）。\")\n",
    "        return final_retrieved_docs\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        关闭 Retriever 实例时调用的清理方法。\n",
    "        Retriever 本身通常没有需要显式关闭的外部资源 (因为它主要依赖于 Indexer 提供的资源)。\n",
    "        此方法主要用于记录 Retriever 的关闭事件。这是负责任的结束流程的一部分。\n",
    "        \"\"\"\n",
    "        self.logger.info(\"开始关闭 Retriever 实例...\")\n",
    "        # 通常无需执行特定的资源释放操作，因为 Encoder, Faiss索引, DB连接等由 Indexer 管理。\n",
    "        self.logger.info(\"Retriever 实例关闭完成。\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 生成器类 (Generator)\n",
    "# Generator 是系统的“问答大脑”，它根据检索到的信息生成用户最终看到的自然语言答案。\n",
    "# 它的表现依赖于底层的LLM能力和构建Prompt的质量。\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "class Generator:\n",
    "    \"\"\"\n",
    "    Generator 类负责与大语言模型 (LLM) API (此处特指 ZhipuAI 的 API) 进行交互，\n",
    "    以根据用户查询和检索到的上下文信息生成最终的自然语言答案。\n",
    "\n",
    "    其核心工作流程包括：\n",
    "    1.  **构建提示 (Prompt)**: 将用户的原始查询和由 Retriever 检索到的相关文档上下文列表，\n",
    "        组合成一个结构化的提示 (prompt)。这个提示会指导 LLM 如何根据提供的上下文来回答问题，\n",
    "        并遵循特定的规则（例如，要求 LLM 仅基于提供的上下文作答、如何处理信息不足的情况等）。\n",
    "    2.  **调用 LLM API**: 将构建好的提示发送给指定的 ZhipuAI 大语言模型 API。\n",
    "    3.  **处理响应**: 对 LLM API 返回的原始文本响应进行基本的后处理（例如，去除多余的空白字符）。\n",
    "\n",
    "    此类依赖于 `zhipuai` Python 库来与 ZhipuAI API 进行通信，并且需要一个有效的 API Key。\n",
    "    API Key 的获取优先级如下：\n",
    "    - 首先，从构造函数参数 `api_key` 获取。\n",
    "    - 如果构造函数未提供，则尝试从环境变量 `ZHIPUAI_API_KEY` 读取。\n",
    "    如果没有有效的 API Key，Generator 将无法工作。我必须确保 API Key 的可用性。\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: Optional[str] = None, model_name: str = \"glm-4-flash\"):\n",
    "        \"\"\"\n",
    "        初始化 Generator 实例。我需要确保能够成功连接到智谱AI的API服务。\n",
    "\n",
    "        Args:\n",
    "            api_key (Optional[str]): ZhipuAI 的 API Key。如果在此处提供，将优先使用这个 Key。\n",
    "                                     如果为 None，则会尝试从环境变量 `ZHIPUAI_API_KEY` 中读取。\n",
    "                                     API Key 是调用服务的凭证，必须确保获取到。\n",
    "            model_name (str): 指定要调用的 ZhipuAI 平台的模型名称。例如 \"glm-4-flash\", \"glm-4\" 等。\n",
    "                              不同的模型具有不同的能力、速度、上下文窗口大小和调用成本。\n",
    "                              默认值为 \"glm-4-flash\"，这是一个速度较快且性价比较高的模型，适合示例使用。\n",
    "                              请查阅 ZhipuAI 官方文档以获取最新的可用模型列表和特性。\n",
    "\n",
    "        Raises:\n",
    "            ValueError: 如果 `api_key` 参数为 None 并且在环境变量 `ZHIPUAI_API_KEY` 中也找不到有效的 Key。\n",
    "                        没有 API Key，Generator 将无法与 ZhipuAI 服务通信，这是致命的初始化失败。\n",
    "            RuntimeError: 如果 ZhipuAI 客户端在初始化过程中发生其他错误 (例如，网络问题、`zhipuai`库安装问题等)。\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__ + \".\" + self.__class__.__name__)\n",
    "        self.logger.info(f\"开始初始化 Generator，准备使用 ZhipuAI 模型: {model_name}\")\n",
    "\n",
    "        # 决定最终使用的 API Key：优先使用通过参数传入的，否则尝试从环境变量获取。\n",
    "        final_api_key = api_key if api_key else os.getenv(\"ZHIPUAI_API_KEY\")\n",
    "\n",
    "        # 检查是否成功获取到 API Key。如果获取不到，必须报错并终止初始化。\n",
    "        if not final_api_key:\n",
    "            error_message = (\"Generator 初始化错误: ZhipuAI API Key 未提供。\\n\"\n",
    "                             \"调用大语言模型需要有效的 API Key。请通过以下方式之一提供 API Key：\\n\"\n",
    "                             \"  1. 在初始化 Generator 时，通过 'api_key' 参数传入。\\n\"\n",
    "                             \"  2. 将 API Key 设置到名为 'ZHIPUAI_API_KEY' 的环境变量中。\")\n",
    "            self.logger.critical(error_message)\n",
    "            raise ValueError(error_message)\n",
    "        else:\n",
    "            # 为了安全，不记录完整的 API Key，只记录获取成功。\n",
    "            self.logger.info(\"成功获取到 ZhipuAI API Key (来源可能是参数或环境变量)。\")\n",
    "\n",
    "        try:\n",
    "            # 初始化 ZhipuAI 客户端。\n",
    "            self.client = zhipuai.ZhipuAI(api_key=final_api_key)\n",
    "            self.model_name = model_name\n",
    "            self.logger.info(f\"ZhipuAI 客户端已使用模型 '{self.model_name}' 成功初始化。\")\n",
    "        except Exception as e:\n",
    "             # 捕获客户端初始化过程中可能发生的各种错误，并提供诊断建议。\n",
    "             self.logger.error(f\"Generator 初始化错误: 初始化 ZhipuAI 客户端失败。错误详情: {e}\", exc_info=True)\n",
    "             self.logger.error(f\"请确认以下几点：\")\n",
    "             self.logger.error(f\"  - 提供的 API Key 是否有效且具有调用模型 '{self.model_name}' 的权限。\")\n",
    "             self.logger.error(f\"  - 'zhipuai' Python 库是否已正确安装 (例如，通过 pip install zhipuai)。\")\n",
    "             self.logger.error(f\"  - 网络连接是否正常，能否访问 ZhipuAI API 服务端点。\")\n",
    "             raise RuntimeError(f\"ZhipuAI客户端初始化失败: {e}\") from e\n",
    "\n",
    "        self.logger.info(\"Generator 初始化成功完成。我已经准备好与大模型交互，生成答案了。\")\n",
    "\n",
    "    def generate(self, query: str, context: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        根据用户提供的原始查询和由 Retriever 返回的文档上下文列表，调用大语言模型 (LLM) 生成回答。\n",
    "        这是RAG流程的最后一个关键步骤。\n",
    "\n",
    "        Args:\n",
    "            query (str): 用户提出的原始问题或查询字符串。\n",
    "            context (List[Dict[str, Any]]): 一个文档字典列表，通常由 Retriever 的 `retrieve` 方法返回。\n",
    "                                 每个字典代表一个检索到的相关文档，应包含诸如 'id', 'text',\n",
    "                                 'image_path', 'score' 等信息。这些信息将作为LLM生成答案的依据。\n",
    "\n",
    "        Returns:\n",
    "            str: 由大语言模型生成并经过基本后处理的文本响应。\n",
    "                 如果在调用 LLM API 时发生错误，会返回一条包含错误信息的提示性字符串。\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"开始为查询生成最终响应...\")\n",
    "        self.logger.info(f\"  接收到的用户查询: '{query[:100]}{'...' if len(query)>100 else ''}'\")\n",
    "        self.logger.info(f\"  使用 {len(context)} 个检索到的文档作为生成上下文。\")\n",
    "\n",
    "        # --- 步骤 1: 构建发送给 LLM 的 Prompt (通常表现为消息列表 `messages`) ---\n",
    "        # Prompt的质量直接影响LLM的输出。我必须仔细构建系统指令和格式化上下文。\n",
    "        self.logger.debug(\"  - Generator步骤 1: 构建 Prompt (包含系统指令、上下文和用户查询)...\")\n",
    "        messages_for_llm: List[Dict[str, str]] = []\n",
    "        try:\n",
    "            messages_for_llm = self._build_messages(query, context)\n",
    "\n",
    "            if messages_for_llm:\n",
    "                # 为了日志清晰，打印系统指令和用户查询的部分内容。\n",
    "                if messages_for_llm[0]['role'] == 'system':\n",
    "                     system_prompt_content = messages_for_llm[0]['content']\n",
    "                     context_start_marker = \"# 参考文档:\"\n",
    "                     context_start_index = system_prompt_content.find(context_start_marker)\n",
    "                     if context_start_index != -1:\n",
    "                         system_instructions_part = system_prompt_content[:context_start_index].strip()\n",
    "                         self.logger.debug(f\"    生成的系统消息 (指令部分): {system_instructions_part[:400]}{'...' if len(system_instructions_part)>400 else ''}\")\n",
    "                     else: # Fallback if marker not found (unlikely with current prompt)\n",
    "                         self.logger.debug(f\"    生成的系统消息 (部分): {system_prompt_content[:400]}{'...' if len(system_prompt_content)>400 else ''}\")\n",
    "\n",
    "                if len(messages_for_llm) > 1 and messages_for_llm[1]['role'] == 'user':\n",
    "                     self.logger.debug(f\"    生成的用户消息 (原始查询): {messages_for_llm[1]['content']}\")\n",
    "            self.logger.debug(\"    Prompt 构建完成。\")\n",
    "        except Exception as e_build_prompt:\n",
    "             self.logger.error(f\"错误: 构建 Prompt 时发生异常: {e_build_prompt}\", exc_info=True)\n",
    "             return \"抱歉，在准备向语言模型发送请求时遇到了内部错误（Prompt构建失败）。\"\n",
    "\n",
    "\n",
    "        # --- 步骤 2: 调用 ZhipuAI Chat Completions API ---\n",
    "        # 这是与外部API交互的关键步骤，必须处理各种可能的API错误。\n",
    "        self.logger.info(f\"  - Generator步骤 2: 开始调用 ZhipuAI Chat API (使用模型: {self.model_name})...\")\n",
    "        llm_raw_response_content = \"抱歉，在尝试从语言模型生成响应时遇到了一个未知问题。\" # 默认错误消息\n",
    "        try:\n",
    "            # 确保 messages_for_llm 是有效的列表\n",
    "            if not isinstance(messages_for_llm, list) or not messages_for_llm:\n",
    "                 self.logger.error(\"错误：构建的 Prompt 消息列表无效或为空，无法调用 LLM API。\")\n",
    "                 return \"抱歉，在准备向语言模型发送请求时遇到了内部错误（Prompt为空）。\"\n",
    "\n",
    "            # 调用 API，传入模型名称、消息列表、温度和最大Token数。\n",
    "            # temperature 控制随机性，max_tokens 控制输出长度。\n",
    "            api_response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages_for_llm,\n",
    "                temperature=0.7,            # 一个常用的值，平衡创造性和事实性\n",
    "                max_tokens=1500,            # 控制生成的最长响应\n",
    "                # stream=False # 确保不是流式输出，等待完整响应\n",
    "            )\n",
    "\n",
    "            # 检查API响应的结构，提取生成的文本内容。\n",
    "            # ZhipuAI SDK v2 的响应结构可能略有不同，需要适配\n",
    "            if api_response and api_response.choices and len(api_response.choices) > 0:\n",
    "                 choice = api_response.choices[0]\n",
    "                 if choice.message and choice.message.content:\n",
    "                      llm_raw_response_content = choice.message.content\n",
    "                      self.logger.info(f\"    ZhipuAI API 调用成功。已接收到模型的响应。\")\n",
    "                 else:\n",
    "                      self.logger.warning(\"    ZhipuAI API 调用似乎成功，但响应中的 message 或 content 为空。将使用默认错误消息。\")\n",
    "                      self.logger.debug(f\"    实际的 API choice 对象: {choice.model_dump_json(indent=2)}\") # Use model_dump_json for Pydantic models\n",
    "            else:\n",
    "                self.logger.warning(\"    ZhipuAI API 调用似乎成功，但响应结构不符合预期 (choices 列表为空或不存在)。将使用默认错误消息。\")\n",
    "                # Log the actual response for debugging if it's not as expected\n",
    "                if api_response:\n",
    "                    self.logger.debug(f\"    实际的 API 响应对象: {api_response.model_dump_json(indent=2)}\") # Use model_dump_json\n",
    "                else:\n",
    "                    self.logger.debug(\"    实际的 API 响应对象为 None 或 False。\")\n",
    "\n",
    "\n",
    "            # 记录Token使用情况，这对成本控制很重要。\n",
    "            if hasattr(api_response, 'usage') and api_response.usage:\n",
    "                completion_tokens = getattr(api_response.usage, 'completion_tokens', 'N/A')\n",
    "                prompt_tokens = getattr(api_response.usage, 'prompt_tokens', 'N/A')\n",
    "                total_tokens = getattr(api_response.usage, 'total_tokens', 'N/A')\n",
    "                self.logger.info(f\"      Token 使用情况 -> 输入提示: {prompt_tokens} tokens, 生成响应: {completion_tokens} tokens, 总计: {total_tokens} tokens.\")\n",
    "            else:\n",
    "                self.logger.info(\"      未能从 API 响应中获取详细的 token 使用情况。\")\n",
    "\n",
    "        # 处理不同类型的 ZhipuAI API 调用错误，提供有针对性的错误信息。\n",
    "        # 注意：错误类型可能随 zhipuai SDK 版本变化，以下是基于常见情况的示例。\n",
    "        except zhipuai.APIStatusError as e_status:\n",
    "             self.logger.error(f\"  错误：ZhipuAI API 返回了状态错误。这通常是由于请求参数、权限或账户问题。\")\n",
    "             self.logger.error(f\"        HTTP 状态码: {e_status.status_code}\")\n",
    "             # Try to get more details if available in the response body\n",
    "             error_body = getattr(e_status, 'body', None)\n",
    "             error_message_detail = str(error_body) if error_body else str(e_status)\n",
    "             self.logger.error(f\"        错误详情: {error_message_detail}\")\n",
    "             llm_raw_response_content = (f\"抱歉，调用语言模型时遇到 API 错误 (状态码: {e_status.status_code})。 \"\n",
    "                                         f\"请检查您的 API Key、账户状态或请求参数，或稍后重试。错误信息: {error_message_detail[:200]}{'...' if len(error_message_detail)>200 else ''}\") # Limit length\n",
    "        except zhipuai.APIConnectionError as e_conn:\n",
    "             self.logger.error(f\"  错误：无法连接到 ZhipuAI API 服务器: {e_conn}\")\n",
    "             llm_raw_response_content = (\"抱歉，无法连接到语言模型服务。 \"\n",
    "                                         \"请检查您的网络连接，或确认 ZhipuAI API 端点是否正确且可访问。\")\n",
    "        except zhipuai.APIRequestFailedError as e_req_failed: # Catching a potentially relevant error type\n",
    "            self.logger.error(f\"  错误: ZhipuAI API 请求失败: {e_req_failed}\")\n",
    "            error_message_detail = str(e_req_failed) # Get the string representation\n",
    "            llm_raw_response_content = f\"抱歉，语言模型API请求失败。可能原因包括请求参数无效或服务内部错误。详情: {error_message_detail[:200]}{'...' if len(error_message_detail)>200 else ''}\" # Limit length\n",
    "        except zhipuai.APITimeoutError as e_timeout:\n",
    "            self.logger.error(f\"  错误: ZhipuAI API 请求超时: {e_timeout}\")\n",
    "            llm_raw_response_content = \"抱歉，与语言模型的通信超时。请稍后重试，或检查网络延迟。\"\n",
    "        except Exception as e_unknown:\n",
    "             self.logger.error(f\"  错误：调用 LLM 时发生未预料的异常: {e_unknown}\", exc_info=True)\n",
    "             llm_raw_response_content = (\"抱歉，在与语言模型交互并生成响应的过程中，发生了一个意外的内部错误。 \"\n",
    "                                         \"请查看详细日志以获取更多信息。\")\n",
    "\n",
    "        # --- 步骤 3: 对 LLM 的原始响应进行后处理 ---\n",
    "        # 对原始响应进行清理，使其更适合最终展示。\n",
    "        self.logger.debug(\"  - Generator步骤 3: 对 LLM 的原始响应进行后处理...\")\n",
    "        final_processed_response = self._postprocess_response(llm_raw_response_content)\n",
    "        self.logger.debug(\"    LLM 响应后处理完成。\")\n",
    "\n",
    "        self.logger.info(\"LLM 响应生成流程结束。\")\n",
    "        return final_processed_response\n",
    "\n",
    "    def _build_messages(self, query: str, context: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        根据用户查询和检索到的上下文文档，构建符合 ZhipuAI Chat API 要求的消息列表 (messages)。\n",
    "        这个列表通常包含两条主要消息：\n",
    "        1.  **System Message (`role: \"system\"`)**: 提供系统级的指令，设定 LLM 的角色、行为规则，\n",
    "            并在此处注入检索到的上下文信息（格式化为文本）。\n",
    "        2.  **User Message (`role: \"user\"`)**: 包含用户原始的查询字符串。\n",
    "\n",
    "        构建高质量的Prompt是引导LLM输出正确答案的关键，我必须仔细设计这里的指令。\n",
    "\n",
    "        Args:\n",
    "            query (str): 用户的原始查询。\n",
    "            context (List[Dict[str, Any]]): Retriever 返回的上下文文档列表。每个文档字典应包含 'id', 'text',\n",
    "                                 'image_path', 'score' 等键。\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, str]]: 一个包含字典的列表，每个字典有 'role' 和 'content' 键，\n",
    "                                  可以直接传递给 ZhipuAI Chat API 的 `messages` 参数。\n",
    "                                  如果发生内部错误，可能返回空列表。\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"开始构建用于 LLM 的消息列表...\")\n",
    "        # 定义系统消息的内容，包含详细的角色设定和行为约束。\n",
    "        system_message_content_parts = [\n",
    "            \"你是一个高度专业且严谨的文档问答助手。你的任务是根据下面提供的 \\\"参考文档\\\" 部分中的信息来精确地回答用户提出的问题。\",\n",
    "            \"\\n# 核心指令与行为准则:\",\n",
    "            \"1.  **严格依据参考信息**: 你的回答必须 **完全且仅** 基于 \\\"参考文档\\\" 中明确提供的信息。严禁使用任何你在训练数据中学习到的外部知识、个人观点、进行任何形式的推断、猜测或联想超出文档内容。这是确保回答可靠性的最重要原则。\",\n",
    "            \"2.  **处理信息不足**: 如果 \\\"参考文档\\\" 中的信息不足以回答用户的问题，或者问题与所有提供的文档内容均不相关，你必须明确指出信息的缺乏。标准回答是：“根据提供的参考文档，我无法找到回答该问题所需的信息。”或者类似表述，如“参考文档中没有包含足够的信息来回答关于...的问题。”。不要试图编造答案，诚实地报告信息不足是专业的表现。\",\n",
    "            \"3.  **关于图像内容的理解**: 你无法直接“看到”或解析图像文件本身的内容。你对图像的理解 **必须且只能** 来源于 \\\"参考文档\\\" 中与该图像关联的 **文本描述内容**，以及文档中可能提及的 **图像文件名**。绝不能声称你能直接感知图像内容或对其进行视觉分析。\",\n",
    "            \"4.  **回答涉及图像的问题**:\",\n",
    "            \"    - 如果用户的问题涉及到某张图片（例如，通过图片文件名或描述性提问），请首先在 \\\"参考文档\\\" 的文本描述中仔细查找是否有与该图片相关的说明。\",\n",
    "            \"    - 如果找到了相关的文本描述，请依据该文本描述来回答。\",\n",
    "            \"    - 如果文档中只提供了图片的文件名但没有相应的文本描述，你可以提及这个文件名（例如，“文档提到了一个名为 'circuit_diagram.png' 的关联图片”），并明确说明文档中缺少对该图片内容的具体文字描述，因此无法进一步回答。\",\n",
    "            \"    - 如果文档中既没有图片描述也没有文件名信息，或者问题与文档中提及的任何图片都无关，请按照上述第2条“处理信息不足”的规则进行回复。\",\n",
    "            \"5.  **引用来源 (推荐)**: 在可能的情况下，如果你的答案基于某一个或某几个特定的参考文档，请在回答中指明这些来源。例如：“根据文档 ID 'BGREF_01' 的描述...” 或 “参考文档 1 (ID: XXX) 和文档 3 (ID: YYY) 提到...”。这有助于用户追溯信息源，提高答案的可信度。\",\n",
    "            \"6.  **回答风格与格式**: 你的回答应尽可能地简洁、清晰、直接，并且专业。避免使用冗长的前缀、不必要的客套话或模棱两可的表述。如果答案包含多个要点，可以使用列表或分点来组织，以提高可读性。\",\n",
    "            \"\\n# 参考文档:\",\n",
    "            \"--- 开始参考文档部分 ---\"\n",
    "        ]\n",
    "        system_message_content = \"\\n\".join(system_message_content_parts).strip()\n",
    "\n",
    "        context_parts_for_prompt: List[str] = []\n",
    "        # 检查是否有检索到的上下文。如果没有，需要告知LLM。\n",
    "        if not context:\n",
    "            self.logger.info(\"    注意: 未向LLM提供任何检索到的上下文文档 (可能是因为检索无结果)。\")\n",
    "            context_parts_for_prompt.append(\"\\n（系统提示：本次未能从知识库中检索到与用户问题相关的文档。请基于此情况进行回答，并遵循“处理信息不足”的规则。）\")\n",
    "        else:\n",
    "            # 遍历每个检索到的文档，将其格式化为易于LLM理解的文本块。\n",
    "            self.logger.info(f\"    正在将 {len(context)} 个检索到的文档格式化为 LLM 的上下文...\")\n",
    "            for i, doc_info in enumerate(context):\n",
    "                doc_id = doc_info.get('id', '未知ID') # 获取文档ID，提供默认值。\n",
    "                score_value = doc_info.get('score', 'N/A') # 获取相关度得分。\n",
    "                text_content = doc_info.get('text', '无可用文本内容') # 获取文本内容，提供默认值。\n",
    "                image_file_path = doc_info.get('image_path') # 获取图像路径。\n",
    "\n",
    "                # 格式化图像信息，如果存在图像路径的话。\n",
    "                image_filename = os.path.basename(image_file_path) if image_file_path else None\n",
    "                image_info_str = f\"关联图片文件名: '{image_filename}'\" if image_filename else \"无明确关联的图片信息\"\n",
    "\n",
    "                # 截断过长的文本内容，避免Prompt超出LLM的上下文窗口限制。\n",
    "                # 需要估算Token长度而不是字符长度，但简单截断作为近似。\n",
    "                max_text_len_for_llm = 700 # Character limit (approximate token limit)\n",
    "                truncated_text_content = text_content[:max_text_len_for_llm] + \\\n",
    "                                         ('...' if len(text_content) > max_text_len_for_llm else '')\n",
    "\n",
    "                # 构建单个文档的格式化字符串。\n",
    "                doc_context_parts = [\n",
    "                    f\"\\n--- 参考文档 {i+1} ---\", # 添加文档分隔符和编号。\n",
    "                    f\"  原始文档ID: {doc_id}\", # 添加原始ID。\n",
    "                    f\"  与查询的相关度得分: {score_value:.4f}\" if isinstance(score_value, float) else f\"  与查询的相关度得分: {score_value}\", # 添加相关度得分，如果是浮点数则格式化。\n",
    "                    f\"  文本内容摘要: {truncated_text_content}\", # 添加文本内容摘要。\n",
    "                    f\"  {image_info_str}\" # 添加图像信息。\n",
    "                ]\n",
    "                context_parts_for_prompt.extend(doc_context_parts)\n",
    "\n",
    "        # 将所有文档的格式化字符串合并。\n",
    "        formatted_context_section = \"\\n\".join(context_parts_for_prompt)\n",
    "        # 将格式化后的上下文添加到系统消息的末尾。\n",
    "        system_message_content += \"\\n\" + formatted_context_section + \"\\n--- 结束参考文档部分 ---\"\n",
    "\n",
    "        # 构建最终的消息列表，包括系统消息和用户消息。\n",
    "        final_messages: List[Dict[str, str]] = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        self.logger.debug(f\"为 LLM 构建的消息列表完成。共 {len(final_messages)} 条消息。\")\n",
    "        # 增加一个简单的验证，确保内容不是空的\n",
    "        if not system_message_content.strip() or not query.strip():\n",
    "            self.logger.error(\"错误：构建的系统消息或用户查询内容为空！\")\n",
    "            return [] # Return empty list to indicate failure\n",
    "\n",
    "        return final_messages\n",
    "\n",
    "    def _postprocess_response(self, llm_raw_response: str) -> str:\n",
    "        \"\"\"\n",
    "        对从 LLM API 获取的原始响应字符串进行基本的后处理。\n",
    "        目前主要执行去除首尾空白字符的操作。\n",
    "        未来可以根据需要在这里添加更复杂的处理逻辑，例如移除特定的模型习语、修正格式等。\n",
    "\n",
    "        Args:\n",
    "            llm_raw_response (str): 从 LLM API 收到的原始文本响应。\n",
    "\n",
    "        Returns:\n",
    "            str: 经过后处理的文本响应，准备好呈现给用户或用于后续流程。\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"开始对 LLM 原始响应进行后处理。原始响应 (前100字符): '{llm_raw_response[:100]}...'\")\n",
    "        # 主要执行去除首尾空白\n",
    "        processed_response = llm_raw_response.strip()\n",
    "\n",
    "        # 可以在这里添加更多后处理逻辑，例如：\n",
    "        # 1. 移除模型可能添加的冗余前缀或后缀。\n",
    "        #    Example:\n",
    "        #    prefixes_to_remove = [\"好的，根据您提供的文档，\", \"根据参考文档：\"]\n",
    "        #    for prefix in prefixes_to_remove:\n",
    "        #        if processed_response.startswith(prefix):\n",
    "        #            processed_response = processed_response[len(prefix):].strip()\n",
    "        #            self.logger.debug(f\"  移除了前缀 '{prefix}'。\")\n",
    "        #            break # Assuming only one prefix needs removal\n",
    "        #\n",
    "        # 2. 格式修正（例如，确保列表格式正确）。\n",
    "        # 3. 敏感信息过滤（如果需要）。\n",
    "\n",
    "        self.logger.debug(f\"LLM 响应后处理完成。处理后响应 (前100字符): '{processed_response[:100]}...'\")\n",
    "        return processed_response\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        关闭 Generator 实例时调用的清理方法。\n",
    "        ZhipuAI 客户端通常不需要显式关闭。此方法主要用于记录Generator的生命周期结束。\n",
    "        \"\"\"\n",
    "        self.logger.info(\"开始关闭 Generator 实例...\")\n",
    "        # ZhipuAI 客户端通常由其内部管理连接，无需显式关闭。\n",
    "        # 如果未来需要管理特定资源（如文件句柄），应在此处添加关闭逻辑。\n",
    "        self.logger.info(\"Generator 实例关闭完成。\")\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# 主程序执行入口 (示例使用流程)\n",
    "# This is an end-to-end usage example demonstrating how to initialize components and execute the RAG flow.\n",
    "# I will ensure each step is clear, with appropriate error handling and logging.\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # =============================================================================================\n",
    "    # Step 0: Configure Run Parameters and Output Directory (Important! Modify as needed!)\n",
    "    # This is a critical setup phase. Ensure all paths and configurations are correct.\n",
    "    # =============================================================================================\n",
    "\n",
    "    # --- User-configurable Run Identifier (Used as the fixed top-level output directory name) ---\n",
    "    # Set a meaningful descriptive name for this run, e.g., project name or system identifier.\n",
    "    # This name, after sanitization, will be used directly as the top-level output directory name.\n",
    "    # Note: Running the script multiple times with the same identifier will overwrite content in the output directory.\n",
    "    RUN_IDENTIFIER_BASE: str = \"multimodal_rag_system_output\" # Example: Fixed base name for the output directory\n",
    "\n",
    "    # Sanitize the run identifier to ensure it's a valid directory name.\n",
    "    sanitized_run_identifier: str = sanitize_filename(RUN_IDENTIFIER_BASE, max_length=50)\n",
    "\n",
    "    # --- Construct the Fixed Top-Level Output Directory ---\n",
    "    # The directory name now directly uses the sanitized run identifier, without a timestamp.\n",
    "    OUTPUT_BASE_DIR: str = sanitized_run_identifier\n",
    "\n",
    "    # Create the output directory (exist_ok=True prevents errors if it already exists).\n",
    "    # Subsequent runs will reuse this directory.\n",
    "    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Configure Logging ---\n",
    "    # Log files will now be located in this fixed top-level directory under the 'logs' subdirectory.\n",
    "    # Note: The log file mode is still 'w' (write/overwrite), overwriting the old log file on each run.\n",
    "    # If you need to append logs, change mode='w' to mode='a' in the setup_logging function.\n",
    "    LOG_DIR: str = os.path.join(OUTPUT_BASE_DIR, \"logs\") # Log subdirectory name (English)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    # Define the full path for the log file.\n",
    "    LOG_FILE_PATH: str = os.path.join(LOG_DIR, \"system_execution_log.txt\") # Log file name (English)\n",
    "    setup_logging(LOG_FILE_PATH) # Call the logging setup function.\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"========= Multimodal Retrieval-Augmented Generation (RAG) System =========\") # English title\n",
    "    logger.info(\"=========                     Main Execution Start                   =========\") # English title\n",
    "    logger.info(\"=\"*80 + \"\\n\")\n",
    "    logger.info(f\"User-defined run identifier (used as fixed directory name): {RUN_IDENTIFIER_BASE}\")\n",
    "    logger.info(f\"Sanitized run identifier (final directory name): {sanitized_run_identifier}\")\n",
    "    logger.info(f\"All output data will be saved to the fixed top-level directory: {os.path.abspath(OUTPUT_BASE_DIR)}\")\n",
    "    logger.warning(\"NOTE: This top-level output directory name is fixed. Subsequent runs with the same identifier will OVERWRITE content in this directory (including logs, database, indices, and query results).\") # English warning\n",
    "\n",
    "    # --- Data Source Configuration ---\n",
    "    # Source of the input data. Ensure paths are correct.\n",
    "    JSON_DATA_PATH: str = 'data.json'\n",
    "    IMAGE_DIR_PATH: str = 'images'\n",
    "    logger.info(f\"Data source config: JSON metadata file='{JSON_DATA_PATH}', Image directory='{IMAGE_DIR_PATH}'\")\n",
    "\n",
    "    # --- Persistence Storage File Paths (within the fixed top-level directory) ---\n",
    "    # Database and Faiss indices need persistence for reloading on next runs. Define paths explicitly.\n",
    "    DB_STORAGE_DIR: str = os.path.join(OUTPUT_BASE_DIR, \"data_storage\") # Main data storage directory\n",
    "    DB_DIR: str = os.path.join(DB_STORAGE_DIR, \"database\") # Database subdirectory\n",
    "    DB_FILE: str = os.path.join(DB_DIR, 'multimodal_doc_store.db') # Database file name (English)\n",
    "\n",
    "    FAISS_DIR: str = os.path.join(DB_STORAGE_DIR, \"vector_indices\") # Faiss index subdirectory (English)\n",
    "    FAISS_TEXT_INDEX_FILE: str = os.path.join(FAISS_DIR, 'text_vector_index.faiss') # Text index file name (English)\n",
    "    FAISS_IMAGE_INDEX_FILE: str = os.path.join(FAISS_DIR, 'image_vector_index.faiss') # Image index file name (English)\n",
    "    FAISS_MEAN_INDEX_FILE: str = os.path.join(FAISS_DIR, 'mean_vector_index.faiss') # Mean index file name (English)\n",
    "\n",
    "    # Query Results Output Directory (within the fixed top-level directory)\n",
    "    # Used to save detailed input, retrieval results, and LLM generation results for each query.\n",
    "    QUERY_RESULTS_DIR: str = os.path.join(OUTPUT_BASE_DIR, \"query_session_results\") # English name\n",
    "    os.makedirs(QUERY_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "    logger.info(f\"Database file will be saved to: {DB_FILE}\")\n",
    "    logger.info(f\"Faiss index files will be saved to directory: {FAISS_DIR}\")\n",
    "    logger.info(f\"Query session results will be saved to directory: {QUERY_RESULTS_DIR}\")\n",
    "\n",
    "\n",
    "    # --- Model Configuration ---\n",
    "    # Choosing appropriate models is crucial for system performance. Using models specified in the example.\n",
    "    # Note: The following models were chosen deliberately to balance performance and resource consumption.\n",
    "    # CLIP Model: \"openai/clip-vit-base-patch32\" is a widely used baseline model.\n",
    "    # LLM Model: \"glm-4-flash\" from ZhipuAI is a lighter model suitable for faster responses and resource-constrained scenarios.\n",
    "    CLIP_MODEL: str = \"openai/clip-vit-base-patch32\"\n",
    "    LLM_MODEL: str = \"glm-4-flash\"\n",
    "    logger.info(f\"Model configuration: CLIP Model='{CLIP_MODEL}', Large Language Model (LLM)='{LLM_MODEL}'\")\n",
    "\n",
    "    # =============================================================================================\n",
    "    # Step 1: Load Raw Data and Attempt to Associate Images\n",
    "    # First step in building the knowledge base.\n",
    "    # =============================================================================================\n",
    "    logger.info(\"\\n--- [Main Flow] Step 1: Load document data from JSON and associate image files ---\")\n",
    "    documents_to_index: List[Dict[str, Any]] = load_data_from_json_and_associate_images(JSON_DATA_PATH, IMAGE_DIR_PATH)\n",
    "\n",
    "    # Check if data loading was successful. Subsequent steps depend on this.\n",
    "    if not documents_to_index:\n",
    "        logger.critical(\"CRITICAL ERROR: Failed to load any valid document data from the JSON file, or the document list is empty after image association.\") # English error\n",
    "        logger.critical(f\"          Please check if file '{JSON_DATA_PATH}' exists, is correctly formatted, and contains valid records.\") # English error\n",
    "        logger.critical(\"          Program will exit now due to data loading failure.\") # English error\n",
    "        exit(1) # Exit because necessary data is missing.\n",
    "    logger.info(f\"--- [Main Flow] Step 1 Complete: Successfully loaded and prepared {len(documents_to_index)} documents for indexing. ---\\n\")\n",
    "    time.sleep(0.2) # Short pause for clearer log output.\n",
    "\n",
    "    # =============================================================================================\n",
    "    # Step 2: Initialize Indexer and Build Index for Loaded Documents\n",
    "    # Key step to convert raw data into vectors and store in DB and Faiss indices.\n",
    "    # =============================================================================================\n",
    "    logger.info(\"--- [Main Flow] Step 2: Initialize Indexer and build index for loaded documents ---\")\n",
    "    indexer_instance: Optional[Indexer] = None # Initialize Indexer instance variable.\n",
    "    try:\n",
    "        # Initialize Indexer, passing all necessary paths and the model name.\n",
    "        indexer_instance = Indexer(\n",
    "            db_path=DB_FILE,\n",
    "            faiss_text_index_path=FAISS_TEXT_INDEX_FILE,\n",
    "            faiss_image_index_path=FAISS_IMAGE_INDEX_FILE,\n",
    "            faiss_mean_index_path=FAISS_MEAN_INDEX_FILE,\n",
    "            clip_model_name=CLIP_MODEL\n",
    "        )\n",
    "        # Call index_documents method to start the indexing process.\n",
    "        indexer_instance.index_documents(documents_to_index)\n",
    "\n",
    "        logger.info(\"Index building/loading complete. Current status:\") # English status\n",
    "        # Report current data counts in the database and Faiss indices.\n",
    "        text_count = getattr(indexer_instance.text_index, 'ntotal', 0)\n",
    "        image_count = getattr(indexer_instance.image_index, 'ntotal', 0)\n",
    "        mean_count = getattr(indexer_instance.mean_index, 'ntotal', 0)\n",
    "        db_doc_count = indexer_instance.get_document_count()\n",
    "        logger.info(f\"  - SQLite Database ('{os.path.basename(DB_FILE)}') document records: {db_doc_count}\") # English label\n",
    "        logger.info(f\"  - Text Faiss Index ('{os.path.basename(FAISS_TEXT_INDEX_FILE)}') vectors: {text_count}\") # English label\n",
    "        logger.info(f\"  - Image Faiss Index ('{os.path.basename(FAISS_IMAGE_INDEX_FILE)}') vectors: {image_count}\") # English label\n",
    "        logger.info(f\"  - Mean Faiss Index ('{os.path.basename(FAISS_MEAN_INDEX_FILE)}') vectors: {mean_count}\") # English label\n",
    "\n",
    "        # Check index status, warn if indices are empty but DB has records.\n",
    "        if text_count == 0 and image_count == 0 and mean_count == 0 and db_doc_count > 0:\n",
    "             logger.warning(\"WARNING: Database contains document records, but all Faiss indices are empty!\") # English warning\n",
    "             logger.warning(\"      This might indicate that the encoding process failed for all documents, or no valid vectors were generated.\") # English warning\n",
    "             logger.warning(\"      Subsequent retrieval operations will not be able to return results based on vector similarity. Please review Indexer and Encoder logs carefully.\") # English warning\n",
    "        elif db_doc_count == 0:\n",
    "             logger.warning(\"WARNING: Database and all Faiss indices are currently empty.\") # English warning\n",
    "             logger.warning(\"      This could be because the input JSON data was empty, or all entries were skipped during loading and processing.\") # English warning\n",
    "\n",
    "    except Exception as e:\n",
    "         logger.critical(f\"CRITICAL ERROR: Top-level exception occurred during Indexer initialization or index building: {e}\", exc_info=True) # English error\n",
    "         logger.critical(\"          Subsequent retrieval and generation steps may not work correctly as the Indexer failed to prepare.\") # English error\n",
    "         indexer_instance = None # Set instance to None to indicate initialization failure.\n",
    "\n",
    "    logger.info(\"--- [Main Flow] Step 2 Complete. ---\\n\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    # =============================================================================================\n",
    "    # Step 3: Initialize Retriever\n",
    "    # Retriever depends on the Indexer. Initialize only if Indexer is successful and contains searchable vectors.\n",
    "    # =============================================================================================\n",
    "    logger.info(\"--- [Main Flow] Step 3: Initialize Retriever ---\")\n",
    "    retriever_instance: Optional[Retriever] = None # Initialize Retriever instance variable.\n",
    "    # Attempt to initialize Retriever only if Indexer initialized successfully AND at least one Faiss index contains vectors.\n",
    "    if indexer_instance: # Check if indexer object exists first\n",
    "         text_index_ready = hasattr(indexer_instance, 'text_index') and getattr(indexer_instance.text_index, 'ntotal', 0) > 0\n",
    "         image_index_ready = hasattr(indexer_instance, 'image_index') and getattr(indexer_instance.image_index, 'ntotal', 0) > 0\n",
    "         mean_index_ready = hasattr(indexer_instance, 'mean_index') and getattr(indexer_instance.mean_index, 'ntotal', 0) > 0\n",
    "\n",
    "         if text_index_ready or image_index_ready or mean_index_ready:\n",
    "             try:\n",
    "                 retriever_instance = Retriever(indexer=indexer_instance) # Pass the Indexer instance.\n",
    "             except Exception as e:\n",
    "                  logger.error(f\"ERROR: Exception occurred during Retriever initialization: {e}\", exc_info=True) # English error\n",
    "                  retriever_instance = None # Initialization failed.\n",
    "         else:\n",
    "              logger.warning(\"Skipping Retriever initialization.\") # English warning\n",
    "              logger.warning(\"  Reason: Indexer initialized successfully, but all its Faiss indices are currently empty.\") # English reason\n",
    "              logger.warning(\"        This might be due to encoding issues, data problems, or index building logic. Please check detailed logs from Step 2.\") # English reason\n",
    "              logger.warning(\"        Retriever cannot perform effective operations without searchable vectors.\") # English reason\n",
    "    else:\n",
    "         logger.warning(\"Skipping Retriever initialization.\") # English warning\n",
    "         logger.warning(\"  Reason: Indexer initialization failed. Please check logs for Step 2 (Indexer initialization and indexing).\") # English reason\n",
    "\n",
    "\n",
    "    logger.info(\"--- [Main Flow] Step 3 Complete. ---\\n\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    # =============================================================================================\n",
    "    # Step 4: Initialize Generator\n",
    "    # Generator depends on the ZhipuAI API. Check if the API Key is available.\n",
    "    # =============================================================================================\n",
    "    logger.info(\"--- [Main Flow] Step 4: Initialize Generator (will interact with ZhipuAI API) ---\")\n",
    "    generator_instance: Optional[Generator] = None # Initialize Generator instance variable.\n",
    "    # Try to get ZhipuAI API Key from environment variable (recommended).\n",
    "    zhipuai_api_key_from_env: Optional[str] = os.getenv(\"ZHIPUAI_API_KEY\")\n",
    "    if not zhipuai_api_key_from_env:\n",
    "        # If API Key is not found in environment, log a warning and explain how to set it.\n",
    "        logger.warning(\"WARNING: Environment variable 'ZHIPUAI_API_KEY' not found.\") # English warning\n",
    "        logger.warning(\"      The Generator will not be able to communicate with the ZhipuAI API, and the answer generation step will be skipped.\") # English warning\n",
    "        logger.warning(\"      To enable LLM answer generation, please do one of the following:\") # English instruction\n",
    "        logger.warning(\"        1. (Recommended) Set your ZhipuAI API Key as an environment variable named 'ZHIPUAI_API_KEY'.\") # English instruction\n",
    "        logger.warning(\"           Example (Linux/macOS): export ZHIPUAI_API_KEY='your_valid_api_key'\") # English example\n",
    "        logger.warning(\"           Then, rerun this script in the same terminal session.\") # English instruction\n",
    "        logger.warning(\"        2. (Alternative) Pass the API Key directly via the `api_key` parameter when initializing the Generator in the code (less secure for this example).\") # English instruction\n",
    "    else:\n",
    "        logger.info(\"Environment variable 'ZHIPUAI_API_KEY' detected. Attempting to initialize Generator...\") # English info\n",
    "        try:\n",
    "            # Initialize Generator using the obtained API Key.\n",
    "            generator_instance = Generator(api_key=zhipuai_api_key_from_env, model_name=LLM_MODEL)\n",
    "        except Exception as e:\n",
    "             logger.error(f\"ERROR: Exception occurred during Generator initialization: {e}\", exc_info=True) # English error\n",
    "             generator_instance = None # Initialization failed.\n",
    "\n",
    "    logger.info(\"--- [Main Flow] Step 4 Complete. ---\\n\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "    # =============================================================================================\n",
    "    # Step 5: Execute RAG Query Examples (Retrieval + Generation stages)\n",
    "    # End-to-end demonstration of the system. Executes only if both Retriever and Generator are available.\n",
    "    # =============================================================================================\n",
    "    logger.info(\"--- [Main Flow] Step 5: Execute RAG Query Examples (Retrieve + Generate) ---\")\n",
    "\n",
    "    # Execute query examples only if both Retriever and Generator initialized successfully.\n",
    "    if retriever_instance and generator_instance:\n",
    "        logger.info(\"Retriever and Generator are both successfully initialized. Proceeding with example queries...\") # English info\n",
    "\n",
    "        def log_retrieved_docs_summary_for_main_process(docs_list: List[Dict[str, Any]], query_log_prefix: str = \"    \"):\n",
    "            \"\"\"Helper function to print a concise summary of retrieved documents in the main process log.\"\"\"\n",
    "            if not docs_list:\n",
    "                logger.info(f\"{query_log_prefix}>> Retrieval Result: No relevant documents found for the query.\") # English result\n",
    "                return\n",
    "            logger.info(f\"{query_log_prefix}>> Retrieval Result: Found Top-{len(docs_list)} relevant documents. Summary:\") # English result\n",
    "            for i, doc_item_data in enumerate(docs_list):\n",
    "                score = doc_item_data.get('score', 'N/A') # Get score.\n",
    "                score_str = f\"{score:.4f}\" if isinstance(score, float) else str(score) # Format score.\n",
    "                text_preview = doc_item_data.get('text', 'No text content')[:70] # Truncate text preview.\n",
    "                if len(doc_item_data.get('text', '')) > 70: text_preview += \"...\" # Add ellipsis.\n",
    "                img_filename_info = \"\" # Initialize image info string.\n",
    "                if doc_item_data.get('image_path'):\n",
    "                    img_filename_info = f\", Associated Image: '{os.path.basename(doc_item_data['image_path'])}'\" # Add image filename.\n",
    "                logger.info(f\"{query_log_prefix}  {i+1}. Document ID: {doc_item_data.get('id', 'N/A')} (Score: {score_str})\") # English output\n",
    "                logger.info(f\"{query_log_prefix}     Text Preview: '{text_preview}'{img_filename_info}\") # English output\n",
    "            logger.info(f\"{query_log_prefix}{'-'*40}\")\n",
    "\n",
    "        # --- Prepare Example Query Data (Reduced quantity for resource limits, keep only a few examples) ---\n",
    "        text_queries_examples: List[str] = [\n",
    "            \"What is a bandgap voltage reference and its main purpose?\", # Example: Pure text query, concept explanation\n",
    "            \"Explain how the PTAT current is generated and its role in a bandgap circuit.\", # Example: Pure text query, principle\n",
    "        ]\n",
    "\n",
    "        # Collect documents with valid images to build image and multimodal query examples.\n",
    "        image_docs_available_for_queries: List[Dict[str, Any]] = []\n",
    "        if documents_to_index:\n",
    "            for doc_data_item_source in documents_to_index:\n",
    "                img_path_source = doc_data_item_source.get('image_path')\n",
    "                if img_path_source and os.path.exists(img_path_source) and os.path.isfile(img_path_source):\n",
    "                    # Only add documents with valid image paths.\n",
    "                    image_docs_available_for_queries.append({\n",
    "                        'id': doc_data_item_source.get('id'),\n",
    "                        'image_path': img_path_source,\n",
    "                        'text': doc_data_item_source.get('text', '')\n",
    "                    })\n",
    "\n",
    "        image_queries_examples_data: List[Dict[str, Any]] = []\n",
    "        multimodal_queries_examples_data: List[Dict[str, Any]] = []\n",
    "\n",
    "        if image_docs_available_for_queries:\n",
    "            # Randomly select a small number from available image docs for query examples.\n",
    "            num_image_query_samples = min(1, len(image_docs_available_for_queries)) # Reduced to only 1 sample for image/multimodal query examples\n",
    "            logger.info(f\"Found {len(image_docs_available_for_queries)} documents with valid images. Randomly selecting {num_image_query_samples} for image/multimodal query examples.\") # English info\n",
    "            selected_image_docs_for_queries = random.sample(image_docs_available_for_queries, num_image_query_samples)\n",
    "\n",
    "            for selected_doc_info in selected_image_docs_for_queries:\n",
    "                doc_id_for_query = selected_doc_info['id']\n",
    "                img_path_for_query = selected_doc_info['image_path']\n",
    "                img_filename_for_query = os.path.basename(img_path_for_query)\n",
    "\n",
    "                # Build pure image query example.\n",
    "                image_queries_examples_data.append({\n",
    "                    'query_input': {'image_path': img_path_for_query},\n",
    "                    # Explicitly mention image info and requirements in the text question for Generator, guiding it based on text description.\n",
    "                    'query_for_generator': f\"What circuit structure or key concept does this image (filename: {img_filename_for_query}) primarily show? Please explain in detail based on the text description in the associated document.\", # English query\n",
    "                    'description': f\"PureImageQuery_About_{img_filename_for_query}\" # Description for logs and filenames (English)\n",
    "                })\n",
    "\n",
    "                # Build multimodal query example.\n",
    "                multimodal_queries_examples_data.append({\n",
    "                    'query_input': {\n",
    "                        'text': f\"Combining the document content and this image (filename: {img_filename_for_query}), please explain the working principle, key features, or design considerations of the circuit shown.\", # English text query part\n",
    "                        'image_path': img_path_for_query\n",
    "                    },\n",
    "                     # Text question for Generator can be similar to the text part of query_input, or adjusted as needed.\n",
    "                    'query_for_generator': f\"Combining the document content and this image (filename: {img_filename_for_query}), please explain the working principle, key features, or design considerations of the circuit shown.\", # English query for generator\n",
    "                    'description': f\"MultimodalQuery_ExplainImage_{img_filename_for_query}\" # Description for logs and filenames (English)\n",
    "                })\n",
    "        else:\n",
    "             logger.warning(\"WARNING: No valid and existing image files found in the loaded data.\") # English warning\n",
    "             logger.warning(\"      Therefore, examples for pure image queries and multimodal queries will be skipped.\") # English warning\n",
    "\n",
    "\n",
    "        # Group all example queries by type.\n",
    "        all_example_queries_groups: List[Tuple[str, List[Any]]] = [ # Adjusted type for List[Any]\n",
    "            (\"Pure Text Query\", text_queries_examples), # English type name\n",
    "            (\"Pure Image Query\", image_queries_examples_data), # English type name\n",
    "            (\"Multimodal Query\", multimodal_queries_examples_data) # English type name\n",
    "        ]\n",
    "\n",
    "        overall_query_counter = 0 # Track total query count for unique output directories.\n",
    "        # Iterate through each query group and each query within the group.\n",
    "        for query_group_name, queries_in_group in all_example_queries_groups:\n",
    "            logger.info(f\"\\n{'#'*70}\\n>>> Starting Example Queries, Type: [{query_group_name}] (Total in this group: {len(queries_in_group)}) <<<\\n{'#'*70}\\n\") # English header\n",
    "\n",
    "            if not queries_in_group:\n",
    "                logger.info(f\"    (Skipping query examples of type [{query_group_name}] as no query data is available.)\") # English skip message\n",
    "                continue\n",
    "\n",
    "            for query_index_in_group, query_data_item in enumerate(queries_in_group):\n",
    "                overall_query_counter += 1\n",
    "\n",
    "                # Prepare query input format for Retriever and text question for Generator.\n",
    "                query_input_for_retriever: Union[str, Dict[str, str], None] = None\n",
    "                query_text_for_generator: Optional[str] = None\n",
    "                query_description_for_logging: Optional[str] = None     # Query description for logging and reports.\n",
    "                query_file_prefix_for_saving: Optional[str] = None     # Directory name prefix for saving results.\n",
    "\n",
    "                # Extract specific query data based on the group type.\n",
    "                if query_group_name == \"Pure Text Query\":\n",
    "                    query_input_for_retriever = str(query_data_item) # Pure text input is the string itself.\n",
    "                    query_text_for_generator = str(query_data_item) # Generator also uses this string directly.\n",
    "                    query_description_for_logging = str(query_data_item) # Log description.\n",
    "                    query_file_prefix_for_saving = sanitize_filename(f\"TextQuery_{query_data_item}\", max_length=60) # Generate directory prefix.\n",
    "                else: # Pure Image or Multimodal query, data item is a dictionary.\n",
    "                    if isinstance(query_data_item, dict): # Add check to ensure it's a dict\n",
    "                        query_input_for_retriever = query_data_item.get('query_input')\n",
    "                        query_text_for_generator = query_data_item.get('query_for_generator')\n",
    "                        # Use the predefined description, which should be a string.\n",
    "                        query_description_for_logging = query_data_item.get('description')\n",
    "                        if isinstance(query_description_for_logging, str):\n",
    "                            query_file_prefix_for_saving = sanitize_filename(query_description_for_logging, max_length=60)\n",
    "                        else:\n",
    "                             query_file_prefix_for_saving = sanitize_filename(f\"{query_group_name}_query_{query_index_in_group+1}\", max_length=60) # Fallback prefix\n",
    "                             logger.warning(f\"Query #{overall_query_counter} description is not a string, using fallback filename prefix.\")\n",
    "                    else:\n",
    "                        logger.error(f\"Error processing query #{overall_query_counter}: Expected dictionary for {query_group_name}, but got {type(query_data_item)}. Skipping query.\")\n",
    "                        continue # Skip this invalid query item\n",
    "\n",
    "\n",
    "                # Log information about the query currently being processed.\n",
    "                logger.info(f\"\\n--- Processing Query #{overall_query_counter} (Type: {query_group_name} - Index in group: {query_index_in_group+1}/{len(queries_in_group)}) ---\") # English status\n",
    "                log_desc_str = str(query_description_for_logging) if query_description_for_logging else \"N/A\"\n",
    "                logger.info(f\"Query Description: {log_desc_str[:120]}{'...' if len(log_desc_str)>120 else ''}\") # English label\n",
    "                # Log the input being passed to the Retriever in detail.\n",
    "                if isinstance(query_input_for_retriever, dict):\n",
    "                    retriever_input_text = query_input_for_retriever.get('text')\n",
    "                    retriever_input_image = query_input_for_retriever.get('image_path')\n",
    "                    if retriever_input_text:\n",
    "                        logger.info(f\"  -> Input Text for Retriever: '{str(retriever_input_text)[:80]}{'...' if len(str(retriever_input_text)) > 80 else ''}'\") # English label\n",
    "                    if retriever_input_image:\n",
    "                         logger.info(f\"  -> Input Image for Retriever: '{os.path.basename(str(retriever_input_image))}'\") # English label\n",
    "                elif isinstance(query_input_for_retriever, str):\n",
    "                     logger.info(f\"  -> Input Text for Retriever: '{query_input_for_retriever[:80]}{'...' if len(query_input_for_retriever) > 80 else ''}'\") # English label\n",
    "                else:\n",
    "                     logger.info(\"  -> Input for Retriever: (None or invalid format)\")\n",
    "\n",
    "                # Log the text question being passed to the Generator.\n",
    "                if query_text_for_generator:\n",
    "                     logger.info(f\"  -> Question Text for Generator: '{str(query_text_for_generator)[:100]}{'...' if len(str(query_text_for_generator)) > 100 else ''}'\") # English label\n",
    "                else:\n",
    "                     logger.info(\"  -> Question Text for Generator: (None)\") # English label\n",
    "                logger.info(\"-\" * 30)\n",
    "\n",
    "                # Create a unique output directory for the current query to save detailed results.\n",
    "                # Ensure query_file_prefix_for_saving is set\n",
    "                if not query_file_prefix_for_saving:\n",
    "                    query_file_prefix_for_saving = sanitize_filename(f\"query_{overall_query_counter}_fallback\", max_length=60)\n",
    "                    logger.warning(f\"Query #{overall_query_counter} had no valid file prefix, using fallback: {query_file_prefix_for_saving}\")\n",
    "\n",
    "                current_query_specific_output_dir = os.path.join(QUERY_RESULTS_DIR, f\"query_{overall_query_counter:03d}_{query_file_prefix_for_saving}\")\n",
    "                try:\n",
    "                    os.makedirs(current_query_specific_output_dir, exist_ok=True)\n",
    "                    logger.info(f\"  Detailed results for this query will be saved to: {current_query_specific_output_dir}\") # English info\n",
    "                except OSError as e_mkdir:\n",
    "                     logger.error(f\"Error creating output directory for query #{overall_query_counter}: {e_mkdir}. Skipping saving results for this query.\")\n",
    "                     continue # Skip to next query if directory cannot be created\n",
    "\n",
    "\n",
    "                retrieved_context_docs_list: List[Dict[str, Any]] = [] # Initialize retrieval results list.\n",
    "                final_generated_response_text: str = \"LLM generation step was not executed or failed due to an error.\" # Default generator failure message (English)\n",
    "\n",
    "                try:\n",
    "                    # Save the original query input passed to the Retriever.\n",
    "                    query_input_filename = \"input_for_retriever.json\" if isinstance(query_input_for_retriever, dict) else \"input_for_retriever.txt\" # English filename\n",
    "                    query_input_save_path = os.path.join(current_query_specific_output_dir, query_input_filename)\n",
    "                    try:\n",
    "                        with open(query_input_save_path, 'w', encoding='utf-8') as f_query_in:\n",
    "                            if isinstance(query_input_for_retriever, dict):\n",
    "                                json.dump(query_input_for_retriever, f_query_in, ensure_ascii=False, indent=4)\n",
    "                            else:\n",
    "                                f_query_in.write(str(query_input_for_retriever) if query_input_for_retriever is not None else \"\")\n",
    "                        logger.debug(f\"  Query input saved to: {query_input_save_path}\")\n",
    "                    except Exception as e_save_input:\n",
    "                         logger.error(f\"Error saving query input to {query_input_save_path}: {e_save_input}\")\n",
    "\n",
    "\n",
    "                    logger.info(\"  [Retrieval Stage] Calling Retriever.retrieve() method...\") # English stage label\n",
    "                    # Ensure query_input_for_retriever is not None before calling retrieve\n",
    "                    if query_input_for_retriever is not None:\n",
    "                        # Call Retriever to perform retrieval. Use k=2 for Top-2 results for quicker demo.\n",
    "                        retrieved_context_docs_list = retriever_instance.retrieve(query_input_for_retriever, k=2)\n",
    "                        # Print summary of retrieved documents.\n",
    "                        log_retrieved_docs_summary_for_main_process(retrieved_context_docs_list, query_log_prefix=\"    \")\n",
    "                    else:\n",
    "                        logger.warning(\"  [Retrieval Stage] Skipped: Query input for retriever was None or invalid.\")\n",
    "                        retrieved_context_docs_list = [] # Ensure it's empty\n",
    "\n",
    "\n",
    "                    # Save the full list of retrieved documents (context).\n",
    "                    retrieved_context_save_path = os.path.join(current_query_specific_output_dir, \"retrieved_context_documents.json\") # English filename\n",
    "                    try:\n",
    "                        with open(retrieved_context_save_path, 'w', encoding='utf-8') as f_retrieved_ctx:\n",
    "                            json.dump(retrieved_context_docs_list, f_retrieved_ctx, ensure_ascii=False, indent=4)\n",
    "                        logger.debug(f\"  Full retrieved context documents saved to: {retrieved_context_save_path}\")\n",
    "                    except Exception as e_save_context:\n",
    "                         logger.error(f\"Error saving retrieved context to {retrieved_context_save_path}: {e_save_context}\")\n",
    "\n",
    "\n",
    "                    # Proceed to generation stage only if at least one document was retrieved.\n",
    "                    if retrieved_context_docs_list:\n",
    "                        logger.info(\"  [Generation Stage] Calling Generator.generate() method (using retrieved context)...\") # English stage label\n",
    "                        if query_text_for_generator:\n",
    "                            # Call Generator to produce the final answer.\n",
    "                            final_generated_response_text = generator_instance.generate(query_text_for_generator, retrieved_context_docs_list)\n",
    "\n",
    "                            # Print the final response generated by the LLM.\n",
    "                            logger.info(f\"\\n  <<< Final Response Generated by LLM for Query #{overall_query_counter} >>>\") # English header\n",
    "                            logger.info(\"-\" * 35)\n",
    "                            logger.info(final_generated_response_text)\n",
    "                            logger.info(\"-\" * 35)\n",
    "                        else:\n",
    "                            logger.error(\"  [Generation Stage] ERROR: Question text for Generator is None. Cannot generate response.\") # English error\n",
    "                            final_generated_response_text = \"Error: Question text for the generator was empty.\" # English error message\n",
    "                    else:\n",
    "                         # If no context was retrieved, skip generation and log it.\n",
    "                         logger.info(\"  [Generation Stage] Skipped: Retriever did not find any relevant context documents, so LLM generation is not performed.\") # English skip message\n",
    "                         final_generated_response_text = \"No relevant context found by Retriever, LLM generation skipped.\" # English message\n",
    "\n",
    "                    # Save the final response generated by the LLM.\n",
    "                    llm_response_save_path = os.path.join(current_query_specific_output_dir, \"llm_generated_final_response.txt\") # English filename\n",
    "                    try:\n",
    "                        with open(llm_response_save_path, 'w', encoding='utf-8') as f_llm_resp:\n",
    "                            f_llm_resp.write(final_generated_response_text)\n",
    "                        logger.debug(f\"  LLM generated response saved to: {llm_response_save_path}\")\n",
    "                    except Exception as e_save_response:\n",
    "                         logger.error(f\"Error saving LLM response to {llm_response_save_path}: {e_save_response}\")\n",
    "\n",
    "\n",
    "                except Exception as e_query_processing:\n",
    "                     # Catch any exceptions during the processing of a single query, log as critical, but don't stop the whole program.\n",
    "                     log_desc_str = str(query_description_for_logging) if query_description_for_logging else f\"Query_{overall_query_counter}\"\n",
    "                     logger.critical(f\"CRITICAL ERROR occurred while processing query '{log_desc_str}' (Query #{overall_query_counter}): {e_query_processing}\", exc_info=True) # English error\n",
    "                     # Try to save the error message to a file as well.\n",
    "                     try:\n",
    "                        error_info_path = os.path.join(current_query_specific_output_dir, \"processing_error_info.txt\") # English filename\n",
    "                        with open(error_info_path, 'w', encoding='utf-8') as f_proc_err:\n",
    "                            f_proc_err.write(f\"A critical error occurred while processing this query: {e_query_processing}\\nCheck the main log file for the detailed stack trace.\\n\\nOriginal query description: {log_desc_str}\\n\")\n",
    "                     except Exception as e_save_err:\n",
    "                        logger.error(f\"Additional Error: Failed to save query processing error information to file: {e_save_err}\") # English error\n",
    "\n",
    "                logger.info(f\"--- Query #{overall_query_counter} Processing Complete ---\") # English status\n",
    "                # Pause briefly before processing the next query in the same group, for easier log observation.\n",
    "                if query_index_in_group < len(queries_in_group) - 1:\n",
    "                    delay_seconds = 0.5 # Reduced delay for faster testing\n",
    "                    logger.info(f\"\\n...Pausing for {delay_seconds} seconds before next query in this group...\\n\" + \"-\"*70 + \"\\n\") # English pause message\n",
    "                    time.sleep(delay_seconds)\n",
    "\n",
    "            logger.info(f\"\\n{'#'*70}\\n>>> All Example Queries of Type [{query_group_name}] Processed <<<\\n{'#'*70}\\n\") # English group completion message\n",
    "            time.sleep(0.5) # Slightly longer pause between groups.\n",
    "\n",
    "    else:\n",
    "        # If Retriever or Generator failed to initialize, log a critical error and explain why.\n",
    "        logger.critical(\"\\nCRITICAL SYSTEM ISSUE: RAG query example flow cannot execute because one or more core components failed to initialize.\") # English critical issue\n",
    "        if not retriever_instance:\n",
    "            logger.critical(\"  - Reason: Retriever initialization failed.\") # English reason\n",
    "            logger.critical(\"    Please carefully review the logs for Step 2 (Indexer initialization) and Step 3 (Retriever initialization) to identify the root cause.\") # English instruction\n",
    "        if not generator_instance:\n",
    "            logger.critical(\"  - Reason: Generator initialization failed.\") # English reason\n",
    "            logger.critical(\"    Please carefully review the logs for Step 4 (Generator initialization), especially regarding the ZHIPUAI_API_KEY check and ZhipuAI client initialization status.\") # English instruction\n",
    "        logger.critical(\"Please resolve the initialization issues and retry.\") # English instruction\n",
    "\n",
    "    logger.info(\"--- [Main Flow] Step 5 (RAG Query Examples) Complete. ---\\n\")\n",
    "\n",
    "    # =============================================================================================\n",
    "    # Step 6: Cleanup and Close Resources\n",
    "    # Mandatory step before program exit to ensure all resources are properly released or saved.\n",
    "    # =============================================================================================\n",
    "    logger.info(\"--- [Main Flow] Step 6: Cleanup and Close System Resources ---\")\n",
    "    # Close component instances sequentially, if they were successfully initialized.\n",
    "    if retriever_instance:\n",
    "        try:\n",
    "            retriever_instance.close()\n",
    "        except Exception as e_close_retriever:\n",
    "             logger.error(f\"Error during Retriever closing: {e_close_retriever}\", exc_info=True)\n",
    "    else:\n",
    "        logger.info(\"  Retriever was not initialized or failed, no cleanup needed.\") # English info\n",
    "\n",
    "    if generator_instance:\n",
    "        try:\n",
    "            generator_instance.close()\n",
    "        except Exception as e_close_generator:\n",
    "            logger.error(f\"Error during Generator closing: {e_close_generator}\", exc_info=True)\n",
    "    else:\n",
    "        logger.info(\"  Generator was not initialized or failed, no cleanup needed.\") # English info\n",
    "\n",
    "    if indexer_instance:\n",
    "        try:\n",
    "            indexer_instance.close() # Indexer's close method handles saving Faiss indices.\n",
    "        except Exception as e_close_indexer:\n",
    "             logger.error(f\"Error during Indexer closing (Faiss indices might not be saved): {e_close_indexer}\", exc_info=True)\n",
    "    else:\n",
    "        logger.info(\"  Indexer was not initialized or failed, no cleanup needed (Faiss indices may not have been saved).\") # English info\n",
    "\n",
    "    logger.info(\"--- [Main Flow] System resource cleanup and closing process complete. ---\\n\")\n",
    "\n",
    "    logger.info(\"\\n\" + \"=\"*80)\n",
    "    logger.info(\"========= Multimodal RAG System Example Program Execution Finished =========\") # English finish message\n",
    "    logger.info(f\"All output (logs, database, indices, query results) saved to the fixed top-level directory:\") # English summary\n",
    "    logger.info(f\"  {os.path.abspath(OUTPUT_BASE_DIR)}\")\n",
    "    logger.info(\"Key subdirectory overview:\") # English overview\n",
    "    logger.info(f\"  - {os.path.join(OUTPUT_BASE_DIR, 'logs/')}\") # Updated path\n",
    "    logger.info(f\"  - {os.path.join(OUTPUT_BASE_DIR, 'data_storage', 'database/')}\") # Updated path\n",
    "    logger.info(f\"  - {os.path.join(OUTPUT_BASE_DIR, 'data_storage', 'vector_indices/')}\") # Updated path\n",
    "    logger.info(f\"  - {os.path.join(OUTPUT_BASE_DIR, 'query_session_results/')}\") # Updated path\n",
    "    logger.info(f\"    (Under {os.path.basename(QUERY_RESULTS_DIR)}/, each 'query_XXX_...' subdirectory contains detailed input/output for a single query)\") # English explanation\n",
    "    logger.warning(f\"REMINDER: Since the top-level directory '{OUTPUT_BASE_DIR}' is fixed, running the script again with the same identifier will OVERWRITE its contents.\") # English reminder\n",
    "    logger.info(\"=\"*80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
