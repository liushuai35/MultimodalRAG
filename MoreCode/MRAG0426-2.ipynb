{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8c28b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\mrag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== 选项 2: 图片转摘要+文本 RAG 示例 ==========\n",
      "\n",
      "--- 清理旧文件 ---\n",
      "--- 清理完成 ---\n",
      "\n",
      "--- 步骤 1: 加载数据 ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 510\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# --- 1. 加载数据 ---\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 步骤 1: 加载数据 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m documents = load_data_from_json_and_associate_images(\u001b[43mjson_path\u001b[49m, image_directory_path)\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m documents: exit(\u001b[33m\"\u001b[39m\u001b[33m错误: 未加载到文档。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    512\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- 步骤 1 完成 ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'json_path' is not defined"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 选项 2: 图片转文本摘要，LLM 仅处理文本 RAG 实现示例\n",
    "# 使用 MLLM (占位符) 生成图片摘要，使用文本嵌入模型索引原文+摘要，使用纯文本 LLM 生成\n",
    "\n",
    "import sqlite3\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel # 用于文本嵌入\n",
    "from PIL import Image\n",
    "import torch\n",
    "import zhipuai # 用于文本生成 LLM\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# --- 数据加载与图片关联函数 (与原示例相同) ---\n",
    "def load_data_from_json_and_associate_images(json_path: str, image_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    # 从 JSON 文件加载文档数据，并尝试在指定的图像目录中关联对应的图片文件。\n",
    "    # (与原示例代码相同，此处省略详细注释)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"错误: 未找到 JSON 文件 '{json_path}'。\")\n",
    "        return []\n",
    "    documents = []\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff']\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            json_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 读取或解析 JSON 文件 '{json_path}' 失败: {e}\")\n",
    "        return []\n",
    "    for item in json_data:\n",
    "        doc_id = item.get('name')\n",
    "        text_content = item.get('description')\n",
    "        if not doc_id or not text_content:\n",
    "            continue\n",
    "        image_path = None\n",
    "        if image_dir and os.path.exists(image_dir):\n",
    "             for ext in image_extensions:\n",
    "                 potential_image_path = os.path.join(image_dir, str(doc_id) + ext)\n",
    "                 if os.path.exists(potential_image_path):\n",
    "                     image_path = potential_image_path\n",
    "                     break\n",
    "        documents.append({\n",
    "            'id': str(doc_id),\n",
    "            'text': str(text_content) if text_content is not None else None,\n",
    "            'image_path': image_path\n",
    "        })\n",
    "    print(f\"已加载 {len(json_data)} 条记录，成功准备 {len(documents)} 个文档。\")\n",
    "    return documents\n",
    "\n",
    "# --- 新增: 图片摘要生成器 (占位符) ---\n",
    "class ImageSummarizer_Placeholder:\n",
    "    \"\"\"\n",
    "    # 图片摘要生成器 (占位符):\n",
    "    # 负责为给定的图像文件生成文本摘要。\n",
    "    # !!! 注意: 这只是一个占位符实现 !!!\n",
    "    # !!! 实际应用中需要调用真正的 MLLM (如 LLaVA, BLIP-2, GPT-4V API) 来生成摘要 !!!\n",
    "    \"\"\"\n",
    "    def __init__(self, model_identifier=\"PlaceholderSummarizer\"):\n",
    "        print(f\"ImageSummarizer (占位符) 初始化。模型标识: {model_identifier}\")\n",
    "        # 在实际实现中，这里会加载 MLLM 模型或初始化 API 客户端\n",
    "\n",
    "    def summarize(self, image_path: str) -> Optional[str]:\n",
    "        \"\"\"生成图像摘要 (占位符实现)\"\"\"\n",
    "        if not image_path or not os.path.exists(image_path):\n",
    "            print(f\"  - [摘要器占位符] 图像路径无效或文件不存在: {image_path}\")\n",
    "            return None\n",
    "        try:\n",
    "            # --- !!! 在此替换为调用 MLLM 生成摘要的逻辑 !!! ---\n",
    "            # response = call_llava_or_gpt4v(image_path, prompt=\"详细描述这张电路图的关键元件和功能。\")\n",
    "            # summary = response.text\n",
    "\n",
    "            # 占位符逻辑: 返回包含文件名的简单描述\n",
    "            filename = os.path.basename(image_path)\n",
    "            summary = f\"[图像摘要占位符] 这是对图像文件 '{filename}' 的自动生成描述。实际摘要应包含电路类型、关键元件等信息。\"\n",
    "            print(f\"  - [摘要器占位符] 为 '{filename}' 生成了摘要。\")\n",
    "            return summary\n",
    "        except Exception as e:\n",
    "            print(f\"  - [摘要器占位符] 为图像 {image_path} 生成摘要时出错: {e}\")\n",
    "            return None\n",
    "\n",
    "# --- 1. 文本编码器 (TextEncoder - 使用 SentenceTransformer 或类似模型) ---\n",
    "class TextEncoder:\n",
    "    \"\"\"\n",
    "    # 使用 Hugging Face Transformers 加载文本嵌入模型 (例如 Sentence-BERT, BGE)。\n",
    "    # 负责将文本字符串转换为向量。\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\", device: Optional[str] = None):\n",
    "        # 常用模型:\n",
    "        # sentence-transformers/all-MiniLM-L6-v2 (英文, 384维)\n",
    "        # sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 (多语言, 384维)\n",
    "        # BAAI/bge-large-en-v1.5 (英文, 1024维, 效果好)\n",
    "        # BAAI/bge-large-zh-v1.5 (中文, 1024维)\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "            self.vector_dimension = self.model.config.hidden_size\n",
    "\n",
    "            if device:\n",
    "                self.device = torch.device(device)\n",
    "            elif torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            print(f\"TextEncoder 初始化成功，模型: {model_name}, 维度: {self.vector_dimension}, 设备: {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载文本嵌入模型 {model_name} 失败: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def encode(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"编码单个文本字符串\"\"\"\n",
    "        if not text: return None\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Tokenize input texts\n",
    "                inputs = self.tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
    "                # Get hidden states\n",
    "                outputs = self.model(**inputs)\n",
    "                # Mean Pooling: Take the mean of the last hidden state across the sequence length dimension\n",
    "                # Masking is important to ignore padding tokens\n",
    "                attention_mask = inputs['attention_mask']\n",
    "                last_hidden_state = outputs.last_hidden_state\n",
    "                masked_hidden_state = last_hidden_state * attention_mask.unsqueeze(-1)\n",
    "                sum_hidden_state = torch.sum(masked_hidden_state, dim=1)\n",
    "                sum_mask = torch.sum(attention_mask, dim=1, keepdim=True)\n",
    "                mean_pooled_vector = sum_hidden_state / torch.clamp(sum_mask, min=1e-9) # Avoid division by zero\n",
    "\n",
    "                vector = mean_pooled_vector.squeeze().cpu().numpy().astype('float32')\n",
    "                norm = np.linalg.norm(vector)\n",
    "                return vector / norm if norm > 1e-6 else np.zeros_like(vector)\n",
    "        except Exception as e:\n",
    "            print(f\"编码文本 '{text[:30]}...' 出错: {e}\")\n",
    "            return None\n",
    "\n",
    "# --- 2. 索引器 (Indexer) - 选项 2 ---\n",
    "# 为图像生成摘要，使用文本编码器索引原文+摘要，存储原文+摘要文本\n",
    "class Indexer_Option2:\n",
    "    \"\"\"\n",
    "    # 索引器 (选项2):\n",
    "    # - 使用 ImageSummarizer 为图像生成文本摘要。\n",
    "    # - 使用 TextEncoder 分别编码原始文本块和生成的图像摘要。\n",
    "    # - 将所有这些 *文本向量* 存储到 Faiss 索引中。\n",
    "    # - 将原始文本块和生成的 *图像摘要文本* 存储到 SQLite 数据库中。\n",
    "    \"\"\"\n",
    "    def __init__(self, db_path: str, faiss_index_path: str, text_embed_model: str, image_summarizer: ImageSummarizer_Placeholder):\n",
    "        self.db_path = db_path\n",
    "        self.faiss_index_path = faiss_index_path\n",
    "        self.encoder = TextEncoder(text_embed_model) # 使用纯文本编码器\n",
    "        self.summarizer = image_summarizer # 图片摘要器\n",
    "        self.vector_dimension = self.encoder.vector_dimension\n",
    "\n",
    "        self._init_db()\n",
    "        self._load_or_create_faiss_index()\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"初始化 SQLite 数据库和表 (只存储文本信息)\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                # vector_index_id: Faiss ID, 主键\n",
    "                # doc_id: 原始文档 ID\n",
    "                # content_source: 'original_text' 或 'image_summary'\n",
    "                # text_content: 存储原始文本或图像摘要文本\n",
    "                # original_image_path: 存储生成摘要的原始图片路径 (可选，用于追溯)\n",
    "                cursor.execute('''\n",
    "                    CREATE TABLE IF NOT EXISTS text_items (\n",
    "                        vector_index_id INTEGER PRIMARY KEY,\n",
    "                        doc_id TEXT NOT NULL,\n",
    "                        content_source TEXT NOT NULL CHECK(content_source IN ('original_text', 'image_summary')),\n",
    "                        text_content TEXT,\n",
    "                        original_image_path TEXT\n",
    "                    )\n",
    "                ''')\n",
    "                cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_doc_id_source ON text_items (doc_id, content_source)\")\n",
    "                conn.commit()\n",
    "                print(f\"数据库已初始化或连接成功: {self.db_path}\")\n",
    "        except Exception as e: print(f\"初始化数据库失败: {e}\"); raise e\n",
    "\n",
    "    def _load_or_create_faiss_index(self):\n",
    "        \"\"\"加载或创建 Faiss 向量索引 (只存文本向量)\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.faiss_index_path):\n",
    "                self.index = faiss.read_index(self.faiss_index_path)\n",
    "                print(f\"成功加载 Faiss 索引: {self.faiss_index_path}, 包含 {self.index.ntotal} 个向量。\")\n",
    "            else:\n",
    "                print(\"未找到 Faiss 索引文件，创建新的空索引。\")\n",
    "                quantizer = faiss.IndexFlatIP(self.vector_dimension) # 内积\n",
    "                self.index = faiss.IndexIDMap2(quantizer)\n",
    "                print(f\"创建了新的 Faiss IndexIDMap2 (内积) 索引 (维度: {self.vector_dimension})。\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载或创建 Faiss 索引失败: {e}。将创建一个新的空索引。\")\n",
    "            quantizer = faiss.IndexFlatIP(self.vector_dimension)\n",
    "            self.index = faiss.IndexIDMap2(quantizer)\n",
    "\n",
    "    def index_documents(self, documents: List[Dict]):\n",
    "        \"\"\"\n",
    "        # 对文档列表进行索引。\n",
    "        # 先处理原始文本，再为图像生成摘要并处理摘要文本。\n",
    "        \"\"\"\n",
    "        if not documents: return\n",
    "        print(f\"开始处理 {len(documents)} 个文档进行索引 (选项 2: 原文+图片摘要)...\")\n",
    "\n",
    "        vectors_to_add = []\n",
    "        vector_ids_for_batch = []\n",
    "        data_to_store = [] # (vec_id, doc_id, source, text, orig_img_path)\n",
    "\n",
    "        start_vector_index_id = self.index.ntotal\n",
    "        next_vector_index_id = start_vector_index_id\n",
    "\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        processed_count = 0\n",
    "        skipped_duplicates = 0\n",
    "        skipped_encoding_errors = 0\n",
    "        summary_errors = 0\n",
    "\n",
    "        for doc in documents:\n",
    "            doc_id = doc.get('id')\n",
    "            text = doc.get('text')\n",
    "            image_path = doc.get('image_path')\n",
    "\n",
    "            if not doc_id: continue\n",
    "\n",
    "            # --- 1. 索引原始文本部分 ---\n",
    "            if text:\n",
    "                cursor.execute(\"SELECT 1 FROM text_items WHERE doc_id = ? AND content_source = 'original_text'\", (doc_id,))\n",
    "                if cursor.fetchone():\n",
    "                    skipped_duplicates += 1\n",
    "                else:\n",
    "                    text_vector = self.encoder.encode(text)\n",
    "                    if text_vector is not None:\n",
    "                        vectors_to_add.append(text_vector)\n",
    "                        vector_ids_for_batch.append(next_vector_index_id)\n",
    "                        data_to_store.append((next_vector_index_id, doc_id, 'original_text', text, None))\n",
    "                        next_vector_index_id += 1\n",
    "                    else:\n",
    "                        skipped_encoding_errors += 1\n",
    "\n",
    "            # --- 2. 索引图像摘要部分 ---\n",
    "            if image_path and os.path.exists(image_path):\n",
    "                cursor.execute(\"SELECT 1 FROM text_items WHERE doc_id = ? AND content_source = 'image_summary'\", (doc_id,))\n",
    "                if cursor.fetchone():\n",
    "                    skipped_duplicates += 1\n",
    "                else:\n",
    "                    # 生成摘要\n",
    "                    summary_text = self.summarizer.summarize(image_path)\n",
    "                    if summary_text:\n",
    "                        # 编码摘要文本\n",
    "                        summary_vector = self.encoder.encode(summary_text)\n",
    "                        if summary_vector is not None:\n",
    "                            vectors_to_add.append(summary_vector)\n",
    "                            vector_ids_for_batch.append(next_vector_index_id)\n",
    "                            # 存储摘要文本和原始图片路径\n",
    "                            data_to_store.append((next_vector_index_id, doc_id, 'image_summary', summary_text, image_path))\n",
    "                            next_vector_index_id += 1\n",
    "                        else:\n",
    "                            skipped_encoding_errors += 1\n",
    "                    else:\n",
    "                        summary_errors += 1\n",
    "            elif image_path:\n",
    "                # print(f\"  警告: 图像文件不存在 {image_path}\")\n",
    "                pass\n",
    "\n",
    "            processed_count += 1\n",
    "            if processed_count % 50 == 0:\n",
    "                 print(f\"  已处理 {processed_count}/{len(documents)} 个原始文档...\")\n",
    "\n",
    "        # --- 批量添加到 Faiss 和 SQLite ---\n",
    "        if vectors_to_add:\n",
    "            try:\n",
    "                vectors_np = np.array(vectors_to_add, dtype='float32')\n",
    "                ids_np = np.array(vector_ids_for_batch, dtype='int64')\n",
    "                self.index.add_with_ids(vectors_np, ids_np)\n",
    "                print(f\"成功向 Faiss 添加 {len(vectors_np)} 个新文本向量。当前总数: {self.index.ntotal}\")\n",
    "\n",
    "                cursor.executemany(\n",
    "                    \"INSERT INTO text_items (vector_index_id, doc_id, content_source, text_content, original_image_path) VALUES (?, ?, ?, ?, ?)\",\n",
    "                    data_to_store\n",
    "                )\n",
    "                conn.commit()\n",
    "                print(f\"成功向数据库存储 {len(data_to_store)} 条新文本记录。\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"批量添加或存储时出错: {e}\")\n",
    "                conn.rollback()\n",
    "        else:\n",
    "            print(\"没有新的有效向量或数据需要添加。\")\n",
    "\n",
    "        conn.close()\n",
    "        print(f\"索引过程完成。总处理文档: {processed_count}, 跳过重复: {skipped_duplicates}, 跳过编码错误: {skipped_encoding_errors}, 图片摘要失败: {summary_errors}\")\n",
    "\n",
    "\n",
    "    def get_text_item_by_vector_index_id(self, vector_index_id: int) -> Optional[Dict]:\n",
    "        \"\"\"根据 vector_index_id 从数据库获取文本条目信息\"\"\"\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(\n",
    "                    \"SELECT doc_id, content_source, text_content, original_image_path FROM text_items WHERE vector_index_id = ?\",\n",
    "                    (vector_index_id,)\n",
    "                )\n",
    "                row = cursor.fetchone()\n",
    "                if row:\n",
    "                    doc_id, source, text, img_path = row\n",
    "                    return {\n",
    "                        'vector_index_id': vector_index_id,\n",
    "                        'doc_id': doc_id,\n",
    "                        'source': source, # 'original_text' or 'image_summary'\n",
    "                        'text': text, # 原始文本或摘要文本\n",
    "                        'original_image_path': img_path # 如果是摘要，这里有原图路径\n",
    "                    }\n",
    "                return None\n",
    "        except Exception as e:\n",
    "             print(f\"从数据库根据 vector_index_id {vector_index_id} 获取文本条目出错: {e}\")\n",
    "             return None\n",
    "\n",
    "    def get_index_count(self) -> int: return self.index.ntotal if hasattr(self, 'index') else 0\n",
    "    def get_db_item_count(self) -> int:\n",
    "         try:\n",
    "             with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor(); cursor.execute(\"SELECT COUNT(*) FROM text_items\"); return cursor.fetchone()[0]\n",
    "         except Exception: return 0\n",
    "    def save_index(self):\n",
    "        if hasattr(self, 'index') and self.index.ntotal > 0:\n",
    "            try: faiss.write_index(self.index, self.faiss_index_path); print(f\"Faiss 索引已保存: {self.faiss_index_path}\")\n",
    "            except Exception as e: print(f\"保存 Faiss 索引失败: {e}\")\n",
    "        elif hasattr(self, 'index'): print(\"索引为空，跳过保存。\")\n",
    "        else: print(\"索引未初始化，跳过保存。\")\n",
    "    def close(self): self.save_index()\n",
    "\n",
    "# --- 3. 检索器 (Retriever) - 选项 2 ---\n",
    "# 使用文本编码器对文本查询编码，在 Faiss 中搜索文本向量，获取原文或摘要文本\n",
    "class Retriever_Option2:\n",
    "    \"\"\"\n",
    "    # 检索器 (选项2):\n",
    "    # - 使用 Indexer 的 TextEncoder 对用户的 *纯文本查询* 进行编码。\n",
    "    # - 在 Faiss 索引（只包含文本向量）中搜索最相似的文本向量。\n",
    "    # - 根据返回的 vector_index_id 从数据库获取对应的原始文本块或图像摘要文本。\n",
    "    \"\"\"\n",
    "    def __init__(self, indexer: Indexer_Option2):\n",
    "        if not isinstance(indexer, Indexer_Option2): raise ValueError(\"必须提供有效的 Indexer_Option2 实例。\")\n",
    "        self.indexer = indexer\n",
    "        self.encoder = indexer.encoder # TextEncoder\n",
    "        self.index = indexer.index\n",
    "        if self.index is None or self.index.ntotal == 0: print(\"警告: 检索器初始化时发现索引为空。\")\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        # 执行检索流程 (仅限文本查询)。\n",
    "\n",
    "        Args:\n",
    "            query: 用户纯文本查询。\n",
    "            k: 检索数量。\n",
    "\n",
    "        Returns:\n",
    "            包含检索到的文本内容（原文或摘要）和得分的字典列表。\n",
    "            [{'doc_id': ..., 'source': ..., 'text': ..., 'original_image_path': ..., 'score': ...}, ...]\n",
    "        \"\"\"\n",
    "        if self.index is None or self.index.ntotal == 0: print(\"错误: 向量索引不可用或为空。\"); return []\n",
    "        if not isinstance(query, str) or not query: print(\"错误: 查询必须是非空文本字符串。\"); return []\n",
    "\n",
    "        # 1. 查询编码 (纯文本)\n",
    "        query_vector = self.encoder.encode(query)\n",
    "        if query_vector is None: print(\"错误: 查询编码失败。\"); return []\n",
    "        query_vector = query_vector.reshape(1, self.indexer.vector_dimension)\n",
    "\n",
    "        # 2. Faiss 搜索 (文本向量)\n",
    "        try:\n",
    "            print(f\"  - 正在 Faiss 索引中搜索 Top {k} 文本向量...\")\n",
    "            scores, vector_index_ids = self.index.search(query_vector, k)\n",
    "            print(f\"  - Faiss 搜索完成，找到 {len(vector_index_ids[0])} 个结果。\")\n",
    "        except Exception as e: print(f\"  - Faiss 搜索失败: {e}\"); return []\n",
    "\n",
    "        # 3. 获取文本数据 (原文或摘要)\n",
    "        retrieved_texts = []\n",
    "        print(\"  - 正在从数据库获取文本条目信息...\")\n",
    "        for i, vec_id in enumerate(vector_index_ids[0]):\n",
    "            if vec_id == -1: continue\n",
    "            text_item_data = self.indexer.get_text_item_by_vector_index_id(int(vec_id))\n",
    "            if text_item_data:\n",
    "                text_item_data['score'] = float(scores[0][i])\n",
    "                retrieved_texts.append(text_item_data)\n",
    "            else: print(f\"  - 警告: 未找到 vector_index_id {vec_id} 对应的数据库记录。\")\n",
    "        print(f\"  - 成功获取 {len(retrieved_texts)} 个文本条目的数据。\")\n",
    "        return retrieved_texts\n",
    "\n",
    "    def close(self): pass\n",
    "\n",
    "# --- 4. 生成器 (Generator) - 选项 2 (使用纯文本 LLM) ---\n",
    "# 与原示例的 Generator 基本相同，接收纯文本上下文\n",
    "class Generator_Option2:\n",
    "    \"\"\"\n",
    "    # 生成器 (选项2):\n",
    "    # - 使用标准的纯文本大语言模型 (如 ZhipuAI GLM-4-flash)。\n",
    "    # - 接收检索到的纯文本上下文（原文片段 + 图片摘要文本）。\n",
    "    # - 构建适合文本 LLM 的 Prompt。\n",
    "    # - 调用文本 LLM API 生成答案。\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key: Optional[str] = None, model_name: str = \"glm-4-flash\"):\n",
    "        final_api_key = api_key if api_key else os.getenv(\"ZHIPUAI_API_KEY\")\n",
    "        if not final_api_key: raise ValueError(\"未提供 ZHIPUAI_API_KEY。\")\n",
    "        try:\n",
    "            self.client = zhipuai.ZhipuAI(api_key=final_api_key)\n",
    "            self.model_name = model_name\n",
    "            print(f\"Generator (选项 2 - 纯文本 LLM) 初始化成功。模型: {self.model_name}\")\n",
    "        except Exception as e: print(f\"初始化智谱 AI 客户端失败: {e}\"); raise e\n",
    "\n",
    "    def generate(self, query: str, context: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        # 接收文本查询和纯文本上下文，生成答案。\n",
    "\n",
    "        Args:\n",
    "            query: 原始用户文本查询。\n",
    "            context: 从 Retriever_Option2 获取的列表，每个元素包含文本内容。\n",
    "                     [{'doc_id': ..., 'source': ..., 'text': ..., 'score': ...}, ...]\n",
    "\n",
    "        Returns:\n",
    "            LLM 生成的文本响应。\n",
    "        \"\"\"\n",
    "        print(f\"正在使用 {len(context)} 个检索到的文本条目为查询生成响应...\")\n",
    "\n",
    "        # 1. 构建 Prompt (与原示例类似，但上下文包含摘要)\n",
    "        messages = self._build_messages(query, context)\n",
    "        print(\"  - Prompt 构建完成。\")\n",
    "\n",
    "        # 2. 调用文本 LLM API\n",
    "        print(f\"  - 正在调用智谱 AI API (模型: {self.model_name})...\")\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name, messages=messages, temperature=0.7, max_tokens=1024\n",
    "            )\n",
    "            llm_response = response.choices[0].message.content\n",
    "            print(\"  - 智谱 AI API 调用成功。\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - 调用 LLM 时出错: {e}\")\n",
    "            return f\"调用 LLM 出错: {e}\"\n",
    "\n",
    "        # 3. 后处理\n",
    "        processed_response = llm_response.strip()\n",
    "        print(\"  - 响应生成和后处理完成。\")\n",
    "        return processed_response\n",
    "\n",
    "    def _build_messages(self, query: str, context: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"构建发送给纯文本 LLM 的消息列表\"\"\"\n",
    "        system_message_content = \"\"\"\n",
    "        你是一个专业的文档问答助手。请严格根据以下提供的\"参考信息\"来回答用户的查询。\n",
    "        # 重要规则:\n",
    "        - **严格性:** 回答必须完全基于下面的 \"参考信息\"。不要使用外部知识。\n",
    "        - **信息来源:** 参考信息可能来自原始文档文本，也可能来自对相关图片的文本描述(标记为 [图片摘要])。\n",
    "        - **信息不足:** 如果信息不足以回答，请说明\"根据提供的文档，我无法回答这个问题\"。\n",
    "        - **关于图片:** 你看到的是图片的文本描述，而不是图片本身。回答时请基于这些描述。\n",
    "\n",
    "        # 参考信息:\n",
    "        --- 开始参考信息 ---\n",
    "        \"\"\".strip()\n",
    "\n",
    "        context_text_parts = []\n",
    "        if not context:\n",
    "            context_text_parts.append(\"未找到相关信息。\")\n",
    "        else:\n",
    "            for i, item in enumerate(context):\n",
    "                doc_id = item.get('doc_id', 'N/A')\n",
    "                score = item.get('score', 'N/A')\n",
    "                text_content = item.get('text', '无内容')\n",
    "                source_type = item.get('source', '未知来源')\n",
    "                source_tag = \"[图片摘要]\" if source_type == 'image_summary' else \"[原文文本]\"\n",
    "\n",
    "                # 格式化每个条目\n",
    "                context_text_parts.append(f\"信息 {i+1} (来源 Doc ID: {doc_id}, 类型: {source_tag}, 得分: {score:.4f}):\")\n",
    "                truncated_text = text_content[:500] + ('...' if len(text_content) > 500 else '')\n",
    "                context_text_parts.append(truncated_text)\n",
    "                context_text_parts.append(\"-\" * 10)\n",
    "\n",
    "            if context_text_parts and context_text_parts[-1] == \"-\" * 10: context_text_parts.pop()\n",
    "            context_text_parts.append(\"--- 结束参考信息 ---\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content + \"\\n\" + \"\\n\".join(context_text_parts)},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "        return messages\n",
    "\n",
    "# --- 选项 2 示例使用流程 ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*10 + \" 选项 2: 图片转摘要+文本 RAG 示例 \" + \"=\"*10 + \"\\n\")\n",
    "\n",
    "    # --- 配置 ---\n",
    "    json_data_path = 'data.json'\n",
    "    image_directory_path = 'images'\n",
    "    db_file_opt2 = 'rag_option2.db'\n",
    "    faiss_index_file_opt2 = 'rag_option2.faiss'\n",
    "    TEXT_EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" # 选择文本嵌入模型\n",
    "    TEXT_LLM_MODEL = \"glm-4-flash\" # 选择纯文本生成模型\n",
    "\n",
    "    # --- 清理 ---\n",
    "    print(\"--- 清理旧文件 ---\")\n",
    "    if os.path.exists(db_file_opt2): os.remove(db_file_opt2)\n",
    "    if os.path.exists(faiss_index_file_opt2): os.remove(faiss_index_file_opt2)\n",
    "    print(\"--- 清理完成 ---\")\n",
    "\n",
    "    # --- 1. 加载数据 ---\n",
    "    print(\"\\n--- 步骤 1: 加载数据 ---\")\n",
    "    documents = load_data_from_json_and_associate_images(json_path, image_directory_path)\n",
    "    if not documents: exit(\"错误: 未加载到文档。\")\n",
    "    print(\"--- 步骤 1 完成 ---\")\n",
    "\n",
    "    # --- 1.5 初始化图片摘要器 (占位符) ---\n",
    "    print(\"\\n--- 步骤 1.5: 初始化图片摘要器 (占位符) ---\")\n",
    "    image_summarizer = ImageSummarizer_Placeholder()\n",
    "    print(\"--- 步骤 1.5 完成 ---\")\n",
    "\n",
    "    # --- 2. 初始化 Indexer 并索引 ---\n",
    "    print(\"\\n--- 步骤 2: 初始化 Indexer (选项 2) 并索引 ---\")\n",
    "    indexer_opt2 = None\n",
    "    try:\n",
    "        indexer_opt2 = Indexer_Option2(\n",
    "            db_path=db_file_opt2,\n",
    "            faiss_index_path=faiss_index_file_opt2,\n",
    "            text_embed_model=TEXT_EMBED_MODEL,\n",
    "            image_summarizer=image_summarizer # 传入摘要器实例\n",
    "        )\n",
    "        indexer_opt2.index_documents(documents)\n",
    "        print(f\"索引完成。向量总数: {indexer_opt2.get_index_count()}, 数据库条目数: {indexer_opt2.get_db_item_count()}\")\n",
    "        if indexer_opt2.get_index_count() == 0: print(\"错误: 索引为空！\"); indexer_opt2 = None\n",
    "    except Exception as e:\n",
    "        print(f\"Indexer 初始化或索引失败: {e}\")\n",
    "        indexer_opt2 = None\n",
    "    print(\"--- 步骤 2 完成 ---\")\n",
    "\n",
    "    # --- 3. 初始化 Retriever ---\n",
    "    print(\"\\n--- 步骤 3: 初始化 Retriever (选项 2) ---\")\n",
    "    retriever_opt2 = None\n",
    "    if indexer_opt2:\n",
    "        try:\n",
    "            retriever_opt2 = Retriever_Option2(indexer=indexer_opt2)\n",
    "            print(\"Retriever 初始化成功。\")\n",
    "        except Exception as e: print(f\"Retriever 初始化失败: {e}\")\n",
    "    else: print(\"Indexer 不可用，跳过 Retriever 初始化。\")\n",
    "    print(\"--- 步骤 3 完成 ---\")\n",
    "\n",
    "    # --- 4. 初始化 Generator ---\n",
    "    print(\"\\n--- 步骤 4: 初始化 Generator (选项 2 - 纯文本 LLM) ---\")\n",
    "    generator_opt2 = None\n",
    "    if os.getenv(\"ZHIPUAI_API_KEY\"):\n",
    "        try:\n",
    "            generator_opt2 = Generator_Option2(model_name=TEXT_LLM_MODEL)\n",
    "            print(\"Generator 初始化成功。\")\n",
    "        except Exception as e: print(f\"Generator 初始化失败: {e}\")\n",
    "    else: print(\"ZHIPUAI_API_KEY 未设置，跳过 Generator 初始化。\")\n",
    "    print(\"--- 步骤 4 完成 ---\")\n",
    "\n",
    "    # --- 5. 执行查询 (仅文本) ---\n",
    "    print(\"\\n--- 步骤 5: 执行查询示例 (仅文本) ---\")\n",
    "    if retriever_opt2 and generator_opt2:\n",
    "        # 辅助函数打印结果\n",
    "        def print_retrieved_items_opt2(items: List[Dict]):\n",
    "            if not items: print(\"    (未检索到条目)\")\n",
    "            for i, item in enumerate(items):\n",
    "                score_str = f\"得分: {item.get('score', 'N/A'):.4f}\"\n",
    "                source_tag = \"[图片摘要]\" if item['source'] == 'image_summary' else \"[原文文本]\"\n",
    "                print(f\"    {i+1}. {source_tag} ID: {item['doc_id']}, {score_str}\")\n",
    "                text_preview = item['text'][:150] + ('...' if len(item['text']) > 150 else '')\n",
    "                print(f\"       文本: {text_preview}\")\n",
    "                if item.get('original_image_path'):\n",
    "                     print(f\"       (来自图像: {os.path.basename(item['original_image_path'])})\") # 显示来源图片\n",
    "                print(\"    \" + \"-\" * 15)\n",
    "\n",
    "        # 查询示例 (纯文本)\n",
    "        queries_opt2 = [\n",
    "            \"带隙基准电路的核心原理是什么？\",\n",
    "            \"请根据图片摘要描述基于 LM317 的电路。\", # 尝试通过摘要提问\n",
    "            \"CTAT 电压是如何补偿温度变化的？\",\n",
    "        ]\n",
    "\n",
    "        for i, query in enumerate(queries_opt2):\n",
    "            print(f\"\\n--- 文本查询 {i+1}/{len(queries_opt2)} ---\")\n",
    "            print(f\"查询: {query}\")\n",
    "\n",
    "            try:\n",
    "                retrieved_context = retriever_opt2.retrieve(query, k=5)\n",
    "                print(f\"  检索到 {len(retrieved_context)} 个文本条目:\")\n",
    "                print_retrieved_items_opt2(retrieved_context)\n",
    "\n",
    "                if retrieved_context:\n",
    "                    print(\"\\n  生成响应 (使用纯文本 LLM):\")\n",
    "                    response = generator_opt2.generate(query, retrieved_context)\n",
    "                    print(response)\n",
    "                else:\n",
    "                    print(\"\\n  未检索到上下文，无法生成响应。\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n  执行查询或生成时出错: {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    else:\n",
    "        print(\"Retriever 或 Generator 未初始化，跳过查询执行。\")\n",
    "    print(\"--- 步骤 5 完成 ---\")\n",
    "\n",
    "    # --- 6. 清理 ---\n",
    "    print(\"\\n--- 步骤 6: 清理资源 ---\")\n",
    "    if indexer_opt2: indexer_opt2.close()\n",
    "    # Retriever 和 Generator 无需关闭\n",
    "    print(\"--- 清理完成 ---\")\n",
    "    print(\"\\n\" + \"=\"*10 + \" 选项 2 示例结束 \" + \"=\"*10 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
